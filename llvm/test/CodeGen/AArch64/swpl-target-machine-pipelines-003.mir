#RUN: llc %s -mcpu=a64fx -ffj-swp -O1 -swpl-debug-dump-resource-filter=".*" -start-before=aarch64-swpipeliner -o /dev/null 2>&1 | FileCheck %s

#CHECK:DBG(AArch64SwplTargetMachine::getPipelines): MI: %251:gpr64 = UBFMXri %246:gpr64, 32, 31
#CHECK-NEXT:  ResourceID: INT_OP+6
#CHECK-NEXT:  latency: 2
#CHECK-NEXT:  seqDecode: false
#CHECK-NEXT:  stage/resource(): 0/EXA
#CHECK-NEXT:  stage/resource(): 0/EXB

#CHECK:DBG(AArch64SwplTargetMachine::getPipelines): MI: %252:gpr64common = SBFMXri %246:gpr64, 0, 31
#CHECK-NEXT:  ResourceID: INT_OP+2
#CHECK-NEXT:  latency: 1
#CHECK-NEXT:  seqDecode: false
#CHECK-NEXT:  stage/resource(): 0/EXA
#CHECK-NEXT:  stage/resource(): 0/EXB

--- |
  ; ModuleID = '/TSVC_2/src_sep_optnone/s125.c'
  source_filename = "/TSVC_2/src_sep_optnone/s125.c"
  target datalayout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128"
  target triple = "aarch64-unknown-linux-gnu"
  
  %struct.args_t = type { %struct.timeval, %struct.timeval, ptr }
  %struct.timeval = type { i64, i64 }
  
  @__func__.s125 = private unnamed_addr constant [5 x i8] c"s125\00", align 1
  @aa = external global [256 x [256 x double]], align 64
  @bb = external global [256 x [256 x double]], align 64
  @cc = external global [256 x [256 x double]], align 64
  @flat_2d_array = external local_unnamed_addr global [65536 x double], align 64
  @a = external global [32000 x double], align 64
  @b = external global [32000 x double], align 64
  @c = external global [32000 x double], align 64
  @d = external global [32000 x double], align 64
  @e = external global [32000 x double], align 64
  
  ; Function Attrs: nounwind uwtable vscale_range(4,4)
  define dso_local double @s125(ptr nocapture noundef %func_args) local_unnamed_addr #0 {
  entry:
    %call = tail call i32 @initialise_arrays(ptr noundef nonnull @__func__.s125) #5
    %call1 = tail call i32 @gettimeofday(ptr noundef %func_args, ptr noundef null) #5
    br label %for.cond2.preheader
  
  for.cond2.preheader:                              ; preds = %entry, %for.cond.cleanup4
    %nl.047 = phi i32 [ 0, %entry ], [ %inc28, %for.cond.cleanup4 ]
    %0 = call i64 @llvm.start.loop.iterations.i64(i64 256)
    br label %for.cond6.preheader
  
  for.cond.cleanup:                                 ; preds = %for.cond.cleanup4
    %t2 = getelementptr inbounds %struct.args_t, ptr %func_args, i64 0, i32 1
    %call30 = tail call i32 @gettimeofday(ptr noundef nonnull %t2, ptr noundef null) #5
    %call31 = tail call fast double @calc_checksum(ptr noundef nonnull @__func__.s125) #5
    ret double %call31
  
  for.cond6.preheader:                              ; preds = %for.cond2.preheader, %for.cond6.preheader
    %lsr.iv = phi i64 [ 0, %for.cond2.preheader ], [ %lsr.iv.next, %for.cond6.preheader ]
    %k.045 = phi i64 [ -1, %for.cond2.preheader ], [ %ind.end, %for.cond6.preheader ]
    %1 = phi i64 [ %0, %for.cond2.preheader ], [ %92, %for.cond6.preheader ]
    %uglygep570 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep571 = getelementptr i8, ptr %uglygep570, i64 3840
    %uglygep492 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep493 = getelementptr i8, ptr %uglygep492, i64 3840
    %uglygep415 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep416 = getelementptr i8, ptr %uglygep415, i64 3840
    %uglygep572 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep573 = getelementptr i8, ptr %uglygep572, i64 3584
    %uglygep494 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep495 = getelementptr i8, ptr %uglygep494, i64 3584
    %uglygep417 = getelementptr i8, ptr %uglygep415, i64 3584
    %uglygep568 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep569 = getelementptr i8, ptr %uglygep568, i64 3328
    %uglygep490 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep491 = getelementptr i8, ptr %uglygep490, i64 3328
    %uglygep413 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep414 = getelementptr i8, ptr %uglygep413, i64 3328
    %uglygep566 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep567 = getelementptr i8, ptr %uglygep566, i64 3072
    %uglygep488 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep489 = getelementptr i8, ptr %uglygep488, i64 3072
    %uglygep411 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep412 = getelementptr i8, ptr %uglygep411, i64 3072
    %uglygep564 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep565 = getelementptr i8, ptr %uglygep564, i64 2816
    %uglygep486 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep487 = getelementptr i8, ptr %uglygep486, i64 2816
    %uglygep409 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep410 = getelementptr i8, ptr %uglygep409, i64 2816
    %uglygep562 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep563 = getelementptr i8, ptr %uglygep562, i64 2560
    %uglygep484 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep485 = getelementptr i8, ptr %uglygep484, i64 2560
    %uglygep407 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep408 = getelementptr i8, ptr %uglygep407, i64 2560
    %uglygep560 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep561 = getelementptr i8, ptr %uglygep560, i64 2304
    %uglygep482 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep483 = getelementptr i8, ptr %uglygep482, i64 2304
    %uglygep405 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep406 = getelementptr i8, ptr %uglygep405, i64 2304
    %uglygep558 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep559 = getelementptr i8, ptr %uglygep558, i64 2048
    %uglygep480 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep481 = getelementptr i8, ptr %uglygep480, i64 2048
    %uglygep403 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep404 = getelementptr i8, ptr %uglygep403, i64 2048
    %sext = shl i64 %k.045, 32
    %2 = ashr exact i64 %sext, 32
    %ind.end = add nsw i64 %2, 256
    call void @llvm.prefetch.p0(ptr %uglygep404, i32 0, i32 3, i32 1)
    %wide.load = load <vscale x 2 x double>, ptr %uglygep403, align 64, !tbaa !6
    %uglygep401 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep402 = getelementptr i8, ptr %uglygep401, i64 64
    %wide.load59 = load <vscale x 2 x double>, ptr %uglygep402, align 64, !tbaa !6
    %uglygep399 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep400 = getelementptr i8, ptr %uglygep399, i64 128
    %wide.load60 = load <vscale x 2 x double>, ptr %uglygep400, align 64, !tbaa !6
    %uglygep397 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep398 = getelementptr i8, ptr %uglygep397, i64 192
    %wide.load61 = load <vscale x 2 x double>, ptr %uglygep398, align 64, !tbaa !6
    %uglygep575 = getelementptr i8, ptr @bb, i64 %lsr.iv
    call void @llvm.prefetch.p0(ptr %uglygep481, i32 0, i32 3, i32 1)
    %wide.load62 = load <vscale x 2 x double>, ptr %uglygep575, align 64, !tbaa !6
    %uglygep478 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep479 = getelementptr i8, ptr %uglygep478, i64 64
    %wide.load63 = load <vscale x 2 x double>, ptr %uglygep479, align 64, !tbaa !6
    %uglygep476 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep477 = getelementptr i8, ptr %uglygep476, i64 128
    %wide.load64 = load <vscale x 2 x double>, ptr %uglygep477, align 64, !tbaa !6
    %uglygep474 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep475 = getelementptr i8, ptr %uglygep474, i64 192
    %wide.load65 = load <vscale x 2 x double>, ptr %uglygep475, align 64, !tbaa !6
    %uglygep574 = getelementptr i8, ptr @cc, i64 %lsr.iv
    call void @llvm.prefetch.p0(ptr %uglygep559, i32 0, i32 3, i32 1)
    %wide.load66 = load <vscale x 2 x double>, ptr %uglygep574, align 64, !tbaa !6
    %uglygep556 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep557 = getelementptr i8, ptr %uglygep556, i64 64
    %wide.load67 = load <vscale x 2 x double>, ptr %uglygep557, align 64, !tbaa !6
    %uglygep554 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep555 = getelementptr i8, ptr %uglygep554, i64 128
    %wide.load68 = load <vscale x 2 x double>, ptr %uglygep555, align 64, !tbaa !6
    %uglygep552 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep553 = getelementptr i8, ptr %uglygep552, i64 192
    %wide.load69 = load <vscale x 2 x double>, ptr %uglygep553, align 64, !tbaa !6
    %3 = fmul fast <vscale x 2 x double> %wide.load66, %wide.load62
    %4 = fmul fast <vscale x 2 x double> %wide.load67, %wide.load63
    %5 = fmul fast <vscale x 2 x double> %wide.load68, %wide.load64
    %6 = fmul fast <vscale x 2 x double> %wide.load69, %wide.load65
    %7 = fadd fast <vscale x 2 x double> %3, %wide.load
    %8 = fadd fast <vscale x 2 x double> %4, %wide.load59
    %9 = fadd fast <vscale x 2 x double> %5, %wide.load60
    %10 = fadd fast <vscale x 2 x double> %6, %wide.load61
    %11 = shl i64 %2, 3
    %uglygep158 = getelementptr i8, ptr @flat_2d_array, i64 %11
    %uglygep159 = getelementptr i8, ptr %uglygep158, i64 8
    store <vscale x 2 x double> %7, ptr %uglygep159, align 8, !tbaa !6
    %12 = getelementptr inbounds double, ptr %uglygep159, i64 8
    store <vscale x 2 x double> %8, ptr %12, align 8, !tbaa !6
    %13 = getelementptr inbounds double, ptr %uglygep159, i64 16
    store <vscale x 2 x double> %9, ptr %13, align 8, !tbaa !6
    %14 = getelementptr inbounds double, ptr %uglygep159, i64 24
    store <vscale x 2 x double> %10, ptr %14, align 8, !tbaa !6
    %uglygep395 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep396 = getelementptr i8, ptr %uglygep395, i64 256
    call void @llvm.prefetch.p0(ptr %uglygep406, i32 0, i32 3, i32 1)
    %wide.load.1 = load <vscale x 2 x double>, ptr %uglygep396, align 64, !tbaa !6
    %uglygep393 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep394 = getelementptr i8, ptr %uglygep393, i64 320
    %wide.load59.1 = load <vscale x 2 x double>, ptr %uglygep394, align 64, !tbaa !6
    %uglygep391 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep392 = getelementptr i8, ptr %uglygep391, i64 384
    %wide.load60.1 = load <vscale x 2 x double>, ptr %uglygep392, align 64, !tbaa !6
    %uglygep389 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep390 = getelementptr i8, ptr %uglygep389, i64 448
    %wide.load61.1 = load <vscale x 2 x double>, ptr %uglygep390, align 64, !tbaa !6
    %uglygep472 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep473 = getelementptr i8, ptr %uglygep472, i64 256
    call void @llvm.prefetch.p0(ptr %uglygep483, i32 0, i32 3, i32 1)
    %wide.load62.1 = load <vscale x 2 x double>, ptr %uglygep473, align 64, !tbaa !6
    %uglygep470 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep471 = getelementptr i8, ptr %uglygep470, i64 320
    %wide.load63.1 = load <vscale x 2 x double>, ptr %uglygep471, align 64, !tbaa !6
    %uglygep468 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep469 = getelementptr i8, ptr %uglygep468, i64 384
    %wide.load64.1 = load <vscale x 2 x double>, ptr %uglygep469, align 64, !tbaa !6
    %uglygep466 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep467 = getelementptr i8, ptr %uglygep466, i64 448
    %wide.load65.1 = load <vscale x 2 x double>, ptr %uglygep467, align 64, !tbaa !6
    %uglygep550 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep551 = getelementptr i8, ptr %uglygep550, i64 256
    call void @llvm.prefetch.p0(ptr %uglygep561, i32 0, i32 3, i32 1)
    %wide.load66.1 = load <vscale x 2 x double>, ptr %uglygep551, align 64, !tbaa !6
    %uglygep548 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep549 = getelementptr i8, ptr %uglygep548, i64 320
    %wide.load67.1 = load <vscale x 2 x double>, ptr %uglygep549, align 64, !tbaa !6
    %uglygep546 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep547 = getelementptr i8, ptr %uglygep546, i64 384
    %wide.load68.1 = load <vscale x 2 x double>, ptr %uglygep547, align 64, !tbaa !6
    %uglygep544 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep545 = getelementptr i8, ptr %uglygep544, i64 448
    %wide.load69.1 = load <vscale x 2 x double>, ptr %uglygep545, align 64, !tbaa !6
    %15 = fmul fast <vscale x 2 x double> %wide.load66.1, %wide.load62.1
    %16 = fmul fast <vscale x 2 x double> %wide.load67.1, %wide.load63.1
    %17 = fmul fast <vscale x 2 x double> %wide.load68.1, %wide.load64.1
    %18 = fmul fast <vscale x 2 x double> %wide.load69.1, %wide.load65.1
    %19 = fadd fast <vscale x 2 x double> %15, %wide.load.1
    %20 = fadd fast <vscale x 2 x double> %16, %wide.load59.1
    %21 = fadd fast <vscale x 2 x double> %17, %wide.load60.1
    %22 = fadd fast <vscale x 2 x double> %18, %wide.load61.1
    %uglygep185 = getelementptr i8, ptr %uglygep158, i64 264
    store <vscale x 2 x double> %19, ptr %uglygep185, align 8, !tbaa !6
    %23 = getelementptr inbounds double, ptr %uglygep185, i64 8
    store <vscale x 2 x double> %20, ptr %23, align 8, !tbaa !6
    %24 = getelementptr inbounds double, ptr %uglygep185, i64 16
    store <vscale x 2 x double> %21, ptr %24, align 8, !tbaa !6
    %25 = getelementptr inbounds double, ptr %uglygep185, i64 24
    store <vscale x 2 x double> %22, ptr %25, align 8, !tbaa !6
    %uglygep387 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep388 = getelementptr i8, ptr %uglygep387, i64 512
    call void @llvm.prefetch.p0(ptr %uglygep408, i32 0, i32 3, i32 1)
    %wide.load.2 = load <vscale x 2 x double>, ptr %uglygep388, align 64, !tbaa !6
    %uglygep385 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep386 = getelementptr i8, ptr %uglygep385, i64 576
    %wide.load59.2 = load <vscale x 2 x double>, ptr %uglygep386, align 64, !tbaa !6
    %uglygep383 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep384 = getelementptr i8, ptr %uglygep383, i64 640
    %wide.load60.2 = load <vscale x 2 x double>, ptr %uglygep384, align 64, !tbaa !6
    %uglygep381 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep382 = getelementptr i8, ptr %uglygep381, i64 704
    %wide.load61.2 = load <vscale x 2 x double>, ptr %uglygep382, align 64, !tbaa !6
    %uglygep464 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep465 = getelementptr i8, ptr %uglygep464, i64 512
    call void @llvm.prefetch.p0(ptr %uglygep485, i32 0, i32 3, i32 1)
    %wide.load62.2 = load <vscale x 2 x double>, ptr %uglygep465, align 64, !tbaa !6
    %uglygep462 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep463 = getelementptr i8, ptr %uglygep462, i64 576
    %wide.load63.2 = load <vscale x 2 x double>, ptr %uglygep463, align 64, !tbaa !6
    %uglygep460 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep461 = getelementptr i8, ptr %uglygep460, i64 640
    %wide.load64.2 = load <vscale x 2 x double>, ptr %uglygep461, align 64, !tbaa !6
    %uglygep458 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep459 = getelementptr i8, ptr %uglygep458, i64 704
    %wide.load65.2 = load <vscale x 2 x double>, ptr %uglygep459, align 64, !tbaa !6
    %uglygep542 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep543 = getelementptr i8, ptr %uglygep542, i64 512
    call void @llvm.prefetch.p0(ptr %uglygep563, i32 0, i32 3, i32 1)
    %wide.load66.2 = load <vscale x 2 x double>, ptr %uglygep543, align 64, !tbaa !6
    %uglygep540 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep541 = getelementptr i8, ptr %uglygep540, i64 576
    %wide.load67.2 = load <vscale x 2 x double>, ptr %uglygep541, align 64, !tbaa !6
    %uglygep538 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep539 = getelementptr i8, ptr %uglygep538, i64 640
    %wide.load68.2 = load <vscale x 2 x double>, ptr %uglygep539, align 64, !tbaa !6
    %uglygep536 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep537 = getelementptr i8, ptr %uglygep536, i64 704
    %wide.load69.2 = load <vscale x 2 x double>, ptr %uglygep537, align 64, !tbaa !6
    %26 = fmul fast <vscale x 2 x double> %wide.load66.2, %wide.load62.2
    %27 = fmul fast <vscale x 2 x double> %wide.load67.2, %wide.load63.2
    %28 = fmul fast <vscale x 2 x double> %wide.load68.2, %wide.load64.2
    %29 = fmul fast <vscale x 2 x double> %wide.load69.2, %wide.load65.2
    %30 = fadd fast <vscale x 2 x double> %26, %wide.load.2
    %31 = fadd fast <vscale x 2 x double> %27, %wide.load59.2
    %32 = fadd fast <vscale x 2 x double> %28, %wide.load60.2
    %33 = fadd fast <vscale x 2 x double> %29, %wide.load61.2
    %uglygep211 = getelementptr i8, ptr %uglygep158, i64 520
    store <vscale x 2 x double> %30, ptr %uglygep211, align 8, !tbaa !6
    %34 = getelementptr inbounds double, ptr %uglygep211, i64 8
    store <vscale x 2 x double> %31, ptr %34, align 8, !tbaa !6
    %35 = getelementptr inbounds double, ptr %uglygep211, i64 16
    store <vscale x 2 x double> %32, ptr %35, align 8, !tbaa !6
    %36 = getelementptr inbounds double, ptr %uglygep211, i64 24
    store <vscale x 2 x double> %33, ptr %36, align 8, !tbaa !6
    %uglygep379 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep380 = getelementptr i8, ptr %uglygep379, i64 768
    call void @llvm.prefetch.p0(ptr %uglygep410, i32 0, i32 3, i32 1)
    %wide.load.3 = load <vscale x 2 x double>, ptr %uglygep380, align 64, !tbaa !6
    %uglygep377 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep378 = getelementptr i8, ptr %uglygep377, i64 832
    %wide.load59.3 = load <vscale x 2 x double>, ptr %uglygep378, align 64, !tbaa !6
    %uglygep375 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep376 = getelementptr i8, ptr %uglygep375, i64 896
    %wide.load60.3 = load <vscale x 2 x double>, ptr %uglygep376, align 64, !tbaa !6
    %uglygep373 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep374 = getelementptr i8, ptr %uglygep373, i64 960
    %wide.load61.3 = load <vscale x 2 x double>, ptr %uglygep374, align 64, !tbaa !6
    %uglygep456 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep457 = getelementptr i8, ptr %uglygep456, i64 768
    call void @llvm.prefetch.p0(ptr %uglygep487, i32 0, i32 3, i32 1)
    %wide.load62.3 = load <vscale x 2 x double>, ptr %uglygep457, align 64, !tbaa !6
    %uglygep454 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep455 = getelementptr i8, ptr %uglygep454, i64 832
    %wide.load63.3 = load <vscale x 2 x double>, ptr %uglygep455, align 64, !tbaa !6
    %uglygep452 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep453 = getelementptr i8, ptr %uglygep452, i64 896
    %wide.load64.3 = load <vscale x 2 x double>, ptr %uglygep453, align 64, !tbaa !6
    %uglygep450 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep451 = getelementptr i8, ptr %uglygep450, i64 960
    %wide.load65.3 = load <vscale x 2 x double>, ptr %uglygep451, align 64, !tbaa !6
    %uglygep534 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep535 = getelementptr i8, ptr %uglygep534, i64 768
    call void @llvm.prefetch.p0(ptr %uglygep565, i32 0, i32 3, i32 1)
    %wide.load66.3 = load <vscale x 2 x double>, ptr %uglygep535, align 64, !tbaa !6
    %uglygep532 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep533 = getelementptr i8, ptr %uglygep532, i64 832
    %wide.load67.3 = load <vscale x 2 x double>, ptr %uglygep533, align 64, !tbaa !6
    %uglygep530 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep531 = getelementptr i8, ptr %uglygep530, i64 896
    %wide.load68.3 = load <vscale x 2 x double>, ptr %uglygep531, align 64, !tbaa !6
    %uglygep528 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep529 = getelementptr i8, ptr %uglygep528, i64 960
    %wide.load69.3 = load <vscale x 2 x double>, ptr %uglygep529, align 64, !tbaa !6
    %37 = fmul fast <vscale x 2 x double> %wide.load66.3, %wide.load62.3
    %38 = fmul fast <vscale x 2 x double> %wide.load67.3, %wide.load63.3
    %39 = fmul fast <vscale x 2 x double> %wide.load68.3, %wide.load64.3
    %40 = fmul fast <vscale x 2 x double> %wide.load69.3, %wide.load65.3
    %41 = fadd fast <vscale x 2 x double> %37, %wide.load.3
    %42 = fadd fast <vscale x 2 x double> %38, %wide.load59.3
    %43 = fadd fast <vscale x 2 x double> %39, %wide.load60.3
    %44 = fadd fast <vscale x 2 x double> %40, %wide.load61.3
    %uglygep237 = getelementptr i8, ptr %uglygep158, i64 776
    store <vscale x 2 x double> %41, ptr %uglygep237, align 8, !tbaa !6
    %45 = getelementptr inbounds double, ptr %uglygep237, i64 8
    store <vscale x 2 x double> %42, ptr %45, align 8, !tbaa !6
    %46 = getelementptr inbounds double, ptr %uglygep237, i64 16
    store <vscale x 2 x double> %43, ptr %46, align 8, !tbaa !6
    %47 = getelementptr inbounds double, ptr %uglygep237, i64 24
    store <vscale x 2 x double> %44, ptr %47, align 8, !tbaa !6
    %uglygep371 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep372 = getelementptr i8, ptr %uglygep371, i64 1024
    call void @llvm.prefetch.p0(ptr %uglygep412, i32 0, i32 3, i32 1)
    %wide.load.4 = load <vscale x 2 x double>, ptr %uglygep372, align 64, !tbaa !6
    %uglygep369 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep370 = getelementptr i8, ptr %uglygep369, i64 1088
    %wide.load59.4 = load <vscale x 2 x double>, ptr %uglygep370, align 64, !tbaa !6
    %uglygep367 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep368 = getelementptr i8, ptr %uglygep367, i64 1152
    %wide.load60.4 = load <vscale x 2 x double>, ptr %uglygep368, align 64, !tbaa !6
    %uglygep365 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep366 = getelementptr i8, ptr %uglygep365, i64 1216
    %wide.load61.4 = load <vscale x 2 x double>, ptr %uglygep366, align 64, !tbaa !6
    %uglygep448 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep449 = getelementptr i8, ptr %uglygep448, i64 1024
    call void @llvm.prefetch.p0(ptr %uglygep489, i32 0, i32 3, i32 1)
    %wide.load62.4 = load <vscale x 2 x double>, ptr %uglygep449, align 64, !tbaa !6
    %uglygep446 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep447 = getelementptr i8, ptr %uglygep446, i64 1088
    %wide.load63.4 = load <vscale x 2 x double>, ptr %uglygep447, align 64, !tbaa !6
    %uglygep444 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep445 = getelementptr i8, ptr %uglygep444, i64 1152
    %wide.load64.4 = load <vscale x 2 x double>, ptr %uglygep445, align 64, !tbaa !6
    %uglygep442 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep443 = getelementptr i8, ptr %uglygep442, i64 1216
    %wide.load65.4 = load <vscale x 2 x double>, ptr %uglygep443, align 64, !tbaa !6
    %uglygep526 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep527 = getelementptr i8, ptr %uglygep526, i64 1024
    call void @llvm.prefetch.p0(ptr %uglygep567, i32 0, i32 3, i32 1)
    %wide.load66.4 = load <vscale x 2 x double>, ptr %uglygep527, align 64, !tbaa !6
    %uglygep524 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep525 = getelementptr i8, ptr %uglygep524, i64 1088
    %wide.load67.4 = load <vscale x 2 x double>, ptr %uglygep525, align 64, !tbaa !6
    %uglygep522 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep523 = getelementptr i8, ptr %uglygep522, i64 1152
    %wide.load68.4 = load <vscale x 2 x double>, ptr %uglygep523, align 64, !tbaa !6
    %uglygep520 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep521 = getelementptr i8, ptr %uglygep520, i64 1216
    %wide.load69.4 = load <vscale x 2 x double>, ptr %uglygep521, align 64, !tbaa !6
    %48 = fmul fast <vscale x 2 x double> %wide.load66.4, %wide.load62.4
    %49 = fmul fast <vscale x 2 x double> %wide.load67.4, %wide.load63.4
    %50 = fmul fast <vscale x 2 x double> %wide.load68.4, %wide.load64.4
    %51 = fmul fast <vscale x 2 x double> %wide.load69.4, %wide.load65.4
    %52 = fadd fast <vscale x 2 x double> %48, %wide.load.4
    %53 = fadd fast <vscale x 2 x double> %49, %wide.load59.4
    %54 = fadd fast <vscale x 2 x double> %50, %wide.load60.4
    %55 = fadd fast <vscale x 2 x double> %51, %wide.load61.4
    %uglygep263 = getelementptr i8, ptr %uglygep158, i64 1032
    store <vscale x 2 x double> %52, ptr %uglygep263, align 8, !tbaa !6
    %56 = getelementptr inbounds double, ptr %uglygep263, i64 8
    store <vscale x 2 x double> %53, ptr %56, align 8, !tbaa !6
    %57 = getelementptr inbounds double, ptr %uglygep263, i64 16
    store <vscale x 2 x double> %54, ptr %57, align 8, !tbaa !6
    %58 = getelementptr inbounds double, ptr %uglygep263, i64 24
    store <vscale x 2 x double> %55, ptr %58, align 8, !tbaa !6
    %uglygep363 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep364 = getelementptr i8, ptr %uglygep363, i64 1280
    call void @llvm.prefetch.p0(ptr %uglygep414, i32 0, i32 3, i32 1)
    %wide.load.5 = load <vscale x 2 x double>, ptr %uglygep364, align 64, !tbaa !6
    %uglygep361 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep362 = getelementptr i8, ptr %uglygep361, i64 1344
    %wide.load59.5 = load <vscale x 2 x double>, ptr %uglygep362, align 64, !tbaa !6
    %uglygep359 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep360 = getelementptr i8, ptr %uglygep359, i64 1408
    %wide.load60.5 = load <vscale x 2 x double>, ptr %uglygep360, align 64, !tbaa !6
    %uglygep357 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep358 = getelementptr i8, ptr %uglygep357, i64 1472
    %wide.load61.5 = load <vscale x 2 x double>, ptr %uglygep358, align 64, !tbaa !6
    %uglygep440 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep441 = getelementptr i8, ptr %uglygep440, i64 1280
    call void @llvm.prefetch.p0(ptr %uglygep491, i32 0, i32 3, i32 1)
    %wide.load62.5 = load <vscale x 2 x double>, ptr %uglygep441, align 64, !tbaa !6
    %uglygep438 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep439 = getelementptr i8, ptr %uglygep438, i64 1344
    %wide.load63.5 = load <vscale x 2 x double>, ptr %uglygep439, align 64, !tbaa !6
    %uglygep436 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep437 = getelementptr i8, ptr %uglygep436, i64 1408
    %wide.load64.5 = load <vscale x 2 x double>, ptr %uglygep437, align 64, !tbaa !6
    %uglygep434 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep435 = getelementptr i8, ptr %uglygep434, i64 1472
    %wide.load65.5 = load <vscale x 2 x double>, ptr %uglygep435, align 64, !tbaa !6
    %uglygep518 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep519 = getelementptr i8, ptr %uglygep518, i64 1280
    call void @llvm.prefetch.p0(ptr %uglygep569, i32 0, i32 3, i32 1)
    %wide.load66.5 = load <vscale x 2 x double>, ptr %uglygep519, align 64, !tbaa !6
    %uglygep516 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep517 = getelementptr i8, ptr %uglygep516, i64 1344
    %wide.load67.5 = load <vscale x 2 x double>, ptr %uglygep517, align 64, !tbaa !6
    %uglygep514 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep515 = getelementptr i8, ptr %uglygep514, i64 1408
    %wide.load68.5 = load <vscale x 2 x double>, ptr %uglygep515, align 64, !tbaa !6
    %uglygep512 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep513 = getelementptr i8, ptr %uglygep512, i64 1472
    %wide.load69.5 = load <vscale x 2 x double>, ptr %uglygep513, align 64, !tbaa !6
    %59 = fmul fast <vscale x 2 x double> %wide.load66.5, %wide.load62.5
    %60 = fmul fast <vscale x 2 x double> %wide.load67.5, %wide.load63.5
    %61 = fmul fast <vscale x 2 x double> %wide.load68.5, %wide.load64.5
    %62 = fmul fast <vscale x 2 x double> %wide.load69.5, %wide.load65.5
    %63 = fadd fast <vscale x 2 x double> %59, %wide.load.5
    %64 = fadd fast <vscale x 2 x double> %60, %wide.load59.5
    %65 = fadd fast <vscale x 2 x double> %61, %wide.load60.5
    %66 = fadd fast <vscale x 2 x double> %62, %wide.load61.5
    %uglygep289 = getelementptr i8, ptr %uglygep158, i64 1288
    store <vscale x 2 x double> %63, ptr %uglygep289, align 8, !tbaa !6
    %67 = getelementptr inbounds double, ptr %uglygep289, i64 8
    store <vscale x 2 x double> %64, ptr %67, align 8, !tbaa !6
    %68 = getelementptr inbounds double, ptr %uglygep289, i64 16
    store <vscale x 2 x double> %65, ptr %68, align 8, !tbaa !6
    %69 = getelementptr inbounds double, ptr %uglygep289, i64 24
    store <vscale x 2 x double> %66, ptr %69, align 8, !tbaa !6
    %uglygep355 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep356 = getelementptr i8, ptr %uglygep355, i64 1536
    call void @llvm.prefetch.p0(ptr %uglygep417, i32 0, i32 3, i32 1)
    %wide.load.6 = load <vscale x 2 x double>, ptr %uglygep356, align 64, !tbaa !6
    %uglygep353 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep354 = getelementptr i8, ptr %uglygep353, i64 1600
    %wide.load59.6 = load <vscale x 2 x double>, ptr %uglygep354, align 64, !tbaa !6
    %uglygep351 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep352 = getelementptr i8, ptr %uglygep351, i64 1664
    %wide.load60.6 = load <vscale x 2 x double>, ptr %uglygep352, align 64, !tbaa !6
    %uglygep349 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep350 = getelementptr i8, ptr %uglygep349, i64 1728
    %wide.load61.6 = load <vscale x 2 x double>, ptr %uglygep350, align 64, !tbaa !6
    %uglygep432 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep433 = getelementptr i8, ptr %uglygep432, i64 1536
    call void @llvm.prefetch.p0(ptr %uglygep495, i32 0, i32 3, i32 1)
    %wide.load62.6 = load <vscale x 2 x double>, ptr %uglygep433, align 64, !tbaa !6
    %uglygep430 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep431 = getelementptr i8, ptr %uglygep430, i64 1600
    %wide.load63.6 = load <vscale x 2 x double>, ptr %uglygep431, align 64, !tbaa !6
    %uglygep428 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep429 = getelementptr i8, ptr %uglygep428, i64 1664
    %wide.load64.6 = load <vscale x 2 x double>, ptr %uglygep429, align 64, !tbaa !6
    %uglygep426 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep427 = getelementptr i8, ptr %uglygep426, i64 1728
    %wide.load65.6 = load <vscale x 2 x double>, ptr %uglygep427, align 64, !tbaa !6
    %uglygep510 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep511 = getelementptr i8, ptr %uglygep510, i64 1536
    call void @llvm.prefetch.p0(ptr %uglygep573, i32 0, i32 3, i32 1)
    %wide.load66.6 = load <vscale x 2 x double>, ptr %uglygep511, align 64, !tbaa !6
    %uglygep508 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep509 = getelementptr i8, ptr %uglygep508, i64 1600
    %wide.load67.6 = load <vscale x 2 x double>, ptr %uglygep509, align 64, !tbaa !6
    %uglygep506 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep507 = getelementptr i8, ptr %uglygep506, i64 1664
    %wide.load68.6 = load <vscale x 2 x double>, ptr %uglygep507, align 64, !tbaa !6
    %uglygep504 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep505 = getelementptr i8, ptr %uglygep504, i64 1728
    %wide.load69.6 = load <vscale x 2 x double>, ptr %uglygep505, align 64, !tbaa !6
    %70 = fmul fast <vscale x 2 x double> %wide.load66.6, %wide.load62.6
    %71 = fmul fast <vscale x 2 x double> %wide.load67.6, %wide.load63.6
    %72 = fmul fast <vscale x 2 x double> %wide.load68.6, %wide.load64.6
    %73 = fmul fast <vscale x 2 x double> %wide.load69.6, %wide.load65.6
    %74 = fadd fast <vscale x 2 x double> %70, %wide.load.6
    %75 = fadd fast <vscale x 2 x double> %71, %wide.load59.6
    %76 = fadd fast <vscale x 2 x double> %72, %wide.load60.6
    %77 = fadd fast <vscale x 2 x double> %73, %wide.load61.6
    %uglygep315 = getelementptr i8, ptr %uglygep158, i64 1544
    store <vscale x 2 x double> %74, ptr %uglygep315, align 8, !tbaa !6
    %78 = getelementptr inbounds double, ptr %uglygep315, i64 8
    store <vscale x 2 x double> %75, ptr %78, align 8, !tbaa !6
    %79 = getelementptr inbounds double, ptr %uglygep315, i64 16
    store <vscale x 2 x double> %76, ptr %79, align 8, !tbaa !6
    %80 = getelementptr inbounds double, ptr %uglygep315, i64 24
    store <vscale x 2 x double> %77, ptr %80, align 8, !tbaa !6
    %uglygep347 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep348 = getelementptr i8, ptr %uglygep347, i64 1792
    call void @llvm.prefetch.p0(ptr %uglygep416, i32 0, i32 3, i32 1)
    %wide.load.7 = load <vscale x 2 x double>, ptr %uglygep348, align 64, !tbaa !6
    %uglygep345 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep346 = getelementptr i8, ptr %uglygep345, i64 1856
    %wide.load59.7 = load <vscale x 2 x double>, ptr %uglygep346, align 64, !tbaa !6
    %uglygep343 = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep344 = getelementptr i8, ptr %uglygep343, i64 1920
    %wide.load60.7 = load <vscale x 2 x double>, ptr %uglygep344, align 64, !tbaa !6
    %uglygep = getelementptr i8, ptr @aa, i64 %lsr.iv
    %uglygep342 = getelementptr i8, ptr %uglygep, i64 1984
    %wide.load61.7 = load <vscale x 2 x double>, ptr %uglygep342, align 64, !tbaa !6
    %uglygep424 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep425 = getelementptr i8, ptr %uglygep424, i64 1792
    call void @llvm.prefetch.p0(ptr %uglygep493, i32 0, i32 3, i32 1)
    %wide.load62.7 = load <vscale x 2 x double>, ptr %uglygep425, align 64, !tbaa !6
    %uglygep422 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep423 = getelementptr i8, ptr %uglygep422, i64 1856
    %wide.load63.7 = load <vscale x 2 x double>, ptr %uglygep423, align 64, !tbaa !6
    %uglygep420 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep421 = getelementptr i8, ptr %uglygep420, i64 1920
    %wide.load64.7 = load <vscale x 2 x double>, ptr %uglygep421, align 64, !tbaa !6
    %uglygep418 = getelementptr i8, ptr @bb, i64 %lsr.iv
    %uglygep419 = getelementptr i8, ptr %uglygep418, i64 1984
    %wide.load65.7 = load <vscale x 2 x double>, ptr %uglygep419, align 64, !tbaa !6
    %uglygep502 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep503 = getelementptr i8, ptr %uglygep502, i64 1792
    call void @llvm.prefetch.p0(ptr %uglygep571, i32 0, i32 3, i32 1)
    %wide.load66.7 = load <vscale x 2 x double>, ptr %uglygep503, align 64, !tbaa !6
    %uglygep500 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep501 = getelementptr i8, ptr %uglygep500, i64 1856
    %wide.load67.7 = load <vscale x 2 x double>, ptr %uglygep501, align 64, !tbaa !6
    %uglygep498 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep499 = getelementptr i8, ptr %uglygep498, i64 1920
    %wide.load68.7 = load <vscale x 2 x double>, ptr %uglygep499, align 64, !tbaa !6
    %uglygep496 = getelementptr i8, ptr @cc, i64 %lsr.iv
    %uglygep497 = getelementptr i8, ptr %uglygep496, i64 1984
    %wide.load69.7 = load <vscale x 2 x double>, ptr %uglygep497, align 64, !tbaa !6
    %81 = fmul fast <vscale x 2 x double> %wide.load66.7, %wide.load62.7
    %82 = fmul fast <vscale x 2 x double> %wide.load67.7, %wide.load63.7
    %83 = fmul fast <vscale x 2 x double> %wide.load68.7, %wide.load64.7
    %84 = fmul fast <vscale x 2 x double> %wide.load69.7, %wide.load65.7
    %85 = fadd fast <vscale x 2 x double> %81, %wide.load.7
    %86 = fadd fast <vscale x 2 x double> %82, %wide.load59.7
    %87 = fadd fast <vscale x 2 x double> %83, %wide.load60.7
    %88 = fadd fast <vscale x 2 x double> %84, %wide.load61.7
    %uglygep341 = getelementptr i8, ptr %uglygep158, i64 1800
    store <vscale x 2 x double> %85, ptr %uglygep341, align 8, !tbaa !6
    %89 = getelementptr inbounds double, ptr %uglygep341, i64 8
    store <vscale x 2 x double> %86, ptr %89, align 8, !tbaa !6
    %90 = getelementptr inbounds double, ptr %uglygep341, i64 16
    store <vscale x 2 x double> %87, ptr %90, align 8, !tbaa !6
    %91 = getelementptr inbounds double, ptr %uglygep341, i64 24
    store <vscale x 2 x double> %88, ptr %91, align 8, !tbaa !6
    %lsr.iv.next = add nuw nsw i64 %lsr.iv, 2048
    %92 = call i64 @llvm.loop.decrement.reg.i64(i64 %1, i64 1)
    %93 = icmp ne i64 %92, 0
    br i1 %93, label %for.cond6.preheader, label %for.cond.cleanup4, !llvm.loop !10
  
  for.cond.cleanup4:                                ; preds = %for.cond6.preheader
    %call26 = tail call i32 @dummy(ptr noundef nonnull @a, ptr noundef nonnull @b, ptr noundef nonnull @c, ptr noundef nonnull @d, ptr noundef nonnull @e, ptr noundef nonnull @aa, ptr noundef nonnull @bb, ptr noundef nonnull @cc, double noundef 0.000000e+00) #5
    %inc28 = add nuw nsw i32 %nl.047, 1
    %exitcond57.not = icmp eq i32 %inc28, 39000
    br i1 %exitcond57.not, label %for.cond.cleanup, label %for.cond2.preheader, !llvm.loop !12
  }
  
  declare i32 @initialise_arrays(ptr noundef) local_unnamed_addr #1
  
  ; Function Attrs: nofree nounwind
  declare noundef i32 @gettimeofday(ptr nocapture noundef, ptr nocapture noundef) local_unnamed_addr #2
  
  declare i32 @dummy(ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, double noundef) local_unnamed_addr #1
  
  declare double @calc_checksum(ptr noundef) local_unnamed_addr #1
  
  ; Function Attrs: inaccessiblemem_or_argmemonly nocallback nofree nosync nounwind willreturn
  declare void @llvm.prefetch.p0(ptr nocapture readonly, i32 immarg, i32 immarg, i32) #3
  
  ; Function Attrs: nocallback noduplicate nofree nosync nounwind willreturn
  declare i64 @llvm.start.loop.iterations.i64(i64) #4
  
  ; Function Attrs: nocallback noduplicate nofree nosync nounwind willreturn
  declare i64 @llvm.loop.decrement.reg.i64(i64, i64) #4
  
  attributes #0 = { nounwind uwtable vscale_range(4,4) "approx-func-fp-math"="true" "frame-pointer"="non-leaf" "min-legal-vector-width"="0" "no-infs-fp-math"="true" "no-nans-fp-math"="true" "no-signed-zeros-fp-math"="true" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="a64fx" "target-features"="+aes,+crc,+crypto,+fp-armv8,+fullfp16,+lse,+neon,+outline-atomics,+ras,+rdm,+sha2,+sve,+v8.2a" "unsafe-fp-math"="true" }
  attributes #1 = { "approx-func-fp-math"="true" "frame-pointer"="non-leaf" "no-infs-fp-math"="true" "no-nans-fp-math"="true" "no-signed-zeros-fp-math"="true" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="a64fx" "target-features"="+aes,+crc,+crypto,+fp-armv8,+fullfp16,+lse,+neon,+outline-atomics,+ras,+rdm,+sha2,+sve,+v8.2a" "unsafe-fp-math"="true" }
  attributes #2 = { nofree nounwind "approx-func-fp-math"="true" "frame-pointer"="non-leaf" "no-infs-fp-math"="true" "no-nans-fp-math"="true" "no-signed-zeros-fp-math"="true" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="a64fx" "target-features"="+aes,+crc,+crypto,+fp-armv8,+fullfp16,+lse,+neon,+outline-atomics,+ras,+rdm,+sha2,+sve,+v8.2a" "unsafe-fp-math"="true" }
  attributes #3 = { inaccessiblemem_or_argmemonly nocallback nofree nosync nounwind willreturn }
  attributes #4 = { nocallback noduplicate nofree nosync nounwind willreturn }
  attributes #5 = { nounwind }
  
  !llvm.module.flags = !{!0, !1, !2, !3, !4}
  
  !0 = !{i32 1, !"wchar_size", i32 4}
  !1 = !{i32 7, !"PIC Level", i32 2}
  !2 = !{i32 7, !"PIE Level", i32 2}
  !3 = !{i32 7, !"uwtable", i32 2}
  !4 = !{i32 7, !"frame-pointer", i32 1}
  !6 = !{!7, !7, i64 0}
  !7 = !{!"double", !8, i64 0}
  !8 = !{!"omnipotent char", !9, i64 0}
  !9 = !{!"Simple C/C++ TBAA"}
  !10 = distinct !{!10, !11}
  !11 = !{!"llvm.loop.mustprogress"}
  !12 = distinct !{!12, !11}

...
---
name:            s125
alignment:       8
exposesReturnsTwice: false
legalized:       false
regBankSelected: false
selected:        false
failedISel:      false
tracksRegLiveness: true
hasWinCFI:       false
callsEHReturn:   false
callsUnwindInit: false
hasEHCatchret:   false
hasEHScopes:     false
hasEHFunclets:   false
failsVerification: false
tracksDebugUserValues: false
registers:
  - { id: 0, class: gpr32sp, preferred-register: '' }
  - { id: 1, class: gpr64all, preferred-register: '' }
  - { id: 2, class: gpr64common, preferred-register: '' }
  - { id: 3, class: gpr64, preferred-register: '' }
  - { id: 4, class: gpr64sp, preferred-register: '' }
  - { id: 5, class: gpr64all, preferred-register: '' }
  - { id: 6, class: gpr64all, preferred-register: '' }
  - { id: 7, class: gpr64all, preferred-register: '' }
  - { id: 8, class: gpr32all, preferred-register: '' }
  - { id: 9, class: gpr64common, preferred-register: '' }
  - { id: 10, class: gpr32all, preferred-register: '' }
  - { id: 11, class: gpr64common, preferred-register: '' }
  - { id: 12, class: gpr32all, preferred-register: '' }
  - { id: 13, class: gpr64all, preferred-register: '' }
  - { id: 14, class: gpr32all, preferred-register: '' }
  - { id: 15, class: gpr32all, preferred-register: '' }
  - { id: 16, class: gpr64all, preferred-register: '' }
  - { id: 17, class: gpr64all, preferred-register: '' }
  - { id: 18, class: gpr64, preferred-register: '' }
  - { id: 19, class: gpr64all, preferred-register: '' }
  - { id: 20, class: gpr32, preferred-register: '' }
  - { id: 21, class: gpr64common, preferred-register: '' }
  - { id: 22, class: gpr64common, preferred-register: '' }
  - { id: 23, class: gpr64common, preferred-register: '' }
  - { id: 24, class: gpr64common, preferred-register: '' }
  - { id: 25, class: gpr64common, preferred-register: '' }
  - { id: 26, class: gpr64common, preferred-register: '' }
  - { id: 27, class: gpr64, preferred-register: '' }
  - { id: 28, class: gpr64common, preferred-register: '' }
  - { id: 29, class: gpr64sp, preferred-register: '' }
  - { id: 30, class: ppr_3b, preferred-register: '' }
  - { id: 31, class: zpr, preferred-register: '' }
  - { id: 32, class: gpr64common, preferred-register: '' }
  - { id: 33, class: ppr_3b, preferred-register: '' }
  - { id: 34, class: zpr, preferred-register: '' }
  - { id: 35, class: gpr64common, preferred-register: '' }
  - { id: 36, class: zpr, preferred-register: '' }
  - { id: 37, class: gpr64common, preferred-register: '' }
  - { id: 38, class: zpr, preferred-register: '' }
  - { id: 39, class: zpr, preferred-register: '' }
  - { id: 40, class: zpr, preferred-register: '' }
  - { id: 41, class: zpr, preferred-register: '' }
  - { id: 42, class: zpr, preferred-register: '' }
  - { id: 43, class: zpr, preferred-register: '' }
  - { id: 44, class: zpr, preferred-register: '' }
  - { id: 45, class: zpr, preferred-register: '' }
  - { id: 46, class: zpr, preferred-register: '' }
  - { id: 47, class: ppr_3b, preferred-register: '' }
  - { id: 48, class: zpr, preferred-register: '' }
  - { id: 49, class: zpr, preferred-register: '' }
  - { id: 50, class: zpr, preferred-register: '' }
  - { id: 51, class: zpr, preferred-register: '' }
  - { id: 52, class: gpr64common, preferred-register: '' }
  - { id: 53, class: gpr64common, preferred-register: '' }
  - { id: 54, class: gpr64common, preferred-register: '' }
  - { id: 55, class: gpr64common, preferred-register: '' }
  - { id: 56, class: gpr64common, preferred-register: '' }
  - { id: 57, class: gpr64common, preferred-register: '' }
  - { id: 58, class: gpr64common, preferred-register: '' }
  - { id: 59, class: zpr, preferred-register: '' }
  - { id: 60, class: gpr64common, preferred-register: '' }
  - { id: 61, class: zpr, preferred-register: '' }
  - { id: 62, class: gpr64common, preferred-register: '' }
  - { id: 63, class: zpr, preferred-register: '' }
  - { id: 64, class: gpr64common, preferred-register: '' }
  - { id: 65, class: zpr, preferred-register: '' }
  - { id: 66, class: zpr, preferred-register: '' }
  - { id: 67, class: zpr, preferred-register: '' }
  - { id: 68, class: zpr, preferred-register: '' }
  - { id: 69, class: zpr, preferred-register: '' }
  - { id: 70, class: zpr, preferred-register: '' }
  - { id: 71, class: zpr, preferred-register: '' }
  - { id: 72, class: zpr, preferred-register: '' }
  - { id: 73, class: zpr, preferred-register: '' }
  - { id: 74, class: zpr, preferred-register: '' }
  - { id: 75, class: zpr, preferred-register: '' }
  - { id: 76, class: zpr, preferred-register: '' }
  - { id: 77, class: zpr, preferred-register: '' }
  - { id: 78, class: gpr64common, preferred-register: '' }
  - { id: 79, class: gpr64common, preferred-register: '' }
  - { id: 80, class: gpr64common, preferred-register: '' }
  - { id: 81, class: gpr64common, preferred-register: '' }
  - { id: 82, class: gpr64common, preferred-register: '' }
  - { id: 83, class: zpr, preferred-register: '' }
  - { id: 84, class: gpr64common, preferred-register: '' }
  - { id: 85, class: zpr, preferred-register: '' }
  - { id: 86, class: gpr64common, preferred-register: '' }
  - { id: 87, class: zpr, preferred-register: '' }
  - { id: 88, class: gpr64common, preferred-register: '' }
  - { id: 89, class: zpr, preferred-register: '' }
  - { id: 90, class: zpr, preferred-register: '' }
  - { id: 91, class: zpr, preferred-register: '' }
  - { id: 92, class: zpr, preferred-register: '' }
  - { id: 93, class: zpr, preferred-register: '' }
  - { id: 94, class: zpr, preferred-register: '' }
  - { id: 95, class: zpr, preferred-register: '' }
  - { id: 96, class: zpr, preferred-register: '' }
  - { id: 97, class: zpr, preferred-register: '' }
  - { id: 98, class: zpr, preferred-register: '' }
  - { id: 99, class: zpr, preferred-register: '' }
  - { id: 100, class: zpr, preferred-register: '' }
  - { id: 101, class: zpr, preferred-register: '' }
  - { id: 102, class: gpr64common, preferred-register: '' }
  - { id: 103, class: gpr64common, preferred-register: '' }
  - { id: 104, class: gpr64common, preferred-register: '' }
  - { id: 105, class: gpr64common, preferred-register: '' }
  - { id: 106, class: gpr64common, preferred-register: '' }
  - { id: 107, class: zpr, preferred-register: '' }
  - { id: 108, class: gpr64common, preferred-register: '' }
  - { id: 109, class: zpr, preferred-register: '' }
  - { id: 110, class: gpr64common, preferred-register: '' }
  - { id: 111, class: zpr, preferred-register: '' }
  - { id: 112, class: gpr64common, preferred-register: '' }
  - { id: 113, class: zpr, preferred-register: '' }
  - { id: 114, class: zpr, preferred-register: '' }
  - { id: 115, class: zpr, preferred-register: '' }
  - { id: 116, class: zpr, preferred-register: '' }
  - { id: 117, class: zpr, preferred-register: '' }
  - { id: 118, class: zpr, preferred-register: '' }
  - { id: 119, class: zpr, preferred-register: '' }
  - { id: 120, class: zpr, preferred-register: '' }
  - { id: 121, class: zpr, preferred-register: '' }
  - { id: 122, class: zpr, preferred-register: '' }
  - { id: 123, class: zpr, preferred-register: '' }
  - { id: 124, class: zpr, preferred-register: '' }
  - { id: 125, class: zpr, preferred-register: '' }
  - { id: 126, class: gpr64common, preferred-register: '' }
  - { id: 127, class: gpr64common, preferred-register: '' }
  - { id: 128, class: gpr64common, preferred-register: '' }
  - { id: 129, class: gpr64common, preferred-register: '' }
  - { id: 130, class: gpr64common, preferred-register: '' }
  - { id: 131, class: zpr, preferred-register: '' }
  - { id: 132, class: gpr64common, preferred-register: '' }
  - { id: 133, class: zpr, preferred-register: '' }
  - { id: 134, class: gpr64common, preferred-register: '' }
  - { id: 135, class: zpr, preferred-register: '' }
  - { id: 136, class: gpr64common, preferred-register: '' }
  - { id: 137, class: zpr, preferred-register: '' }
  - { id: 138, class: zpr, preferred-register: '' }
  - { id: 139, class: zpr, preferred-register: '' }
  - { id: 140, class: zpr, preferred-register: '' }
  - { id: 141, class: zpr, preferred-register: '' }
  - { id: 142, class: zpr, preferred-register: '' }
  - { id: 143, class: zpr, preferred-register: '' }
  - { id: 144, class: zpr, preferred-register: '' }
  - { id: 145, class: zpr, preferred-register: '' }
  - { id: 146, class: zpr, preferred-register: '' }
  - { id: 147, class: zpr, preferred-register: '' }
  - { id: 148, class: zpr, preferred-register: '' }
  - { id: 149, class: zpr, preferred-register: '' }
  - { id: 150, class: gpr64common, preferred-register: '' }
  - { id: 151, class: gpr64common, preferred-register: '' }
  - { id: 152, class: gpr64common, preferred-register: '' }
  - { id: 153, class: gpr64common, preferred-register: '' }
  - { id: 154, class: gpr64common, preferred-register: '' }
  - { id: 155, class: zpr, preferred-register: '' }
  - { id: 156, class: gpr64common, preferred-register: '' }
  - { id: 157, class: zpr, preferred-register: '' }
  - { id: 158, class: gpr64common, preferred-register: '' }
  - { id: 159, class: zpr, preferred-register: '' }
  - { id: 160, class: gpr64common, preferred-register: '' }
  - { id: 161, class: zpr, preferred-register: '' }
  - { id: 162, class: zpr, preferred-register: '' }
  - { id: 163, class: zpr, preferred-register: '' }
  - { id: 164, class: zpr, preferred-register: '' }
  - { id: 165, class: zpr, preferred-register: '' }
  - { id: 166, class: zpr, preferred-register: '' }
  - { id: 167, class: zpr, preferred-register: '' }
  - { id: 168, class: zpr, preferred-register: '' }
  - { id: 169, class: zpr, preferred-register: '' }
  - { id: 170, class: zpr, preferred-register: '' }
  - { id: 171, class: zpr, preferred-register: '' }
  - { id: 172, class: zpr, preferred-register: '' }
  - { id: 173, class: zpr, preferred-register: '' }
  - { id: 174, class: gpr64common, preferred-register: '' }
  - { id: 175, class: gpr64common, preferred-register: '' }
  - { id: 176, class: gpr64common, preferred-register: '' }
  - { id: 177, class: gpr64common, preferred-register: '' }
  - { id: 178, class: gpr64common, preferred-register: '' }
  - { id: 179, class: zpr, preferred-register: '' }
  - { id: 180, class: gpr64common, preferred-register: '' }
  - { id: 181, class: zpr, preferred-register: '' }
  - { id: 182, class: gpr64common, preferred-register: '' }
  - { id: 183, class: zpr, preferred-register: '' }
  - { id: 184, class: gpr64common, preferred-register: '' }
  - { id: 185, class: zpr, preferred-register: '' }
  - { id: 186, class: zpr, preferred-register: '' }
  - { id: 187, class: zpr, preferred-register: '' }
  - { id: 188, class: zpr, preferred-register: '' }
  - { id: 189, class: zpr, preferred-register: '' }
  - { id: 190, class: zpr, preferred-register: '' }
  - { id: 191, class: zpr, preferred-register: '' }
  - { id: 192, class: zpr, preferred-register: '' }
  - { id: 193, class: zpr, preferred-register: '' }
  - { id: 194, class: zpr, preferred-register: '' }
  - { id: 195, class: zpr, preferred-register: '' }
  - { id: 196, class: zpr, preferred-register: '' }
  - { id: 197, class: zpr, preferred-register: '' }
  - { id: 198, class: gpr64common, preferred-register: '' }
  - { id: 199, class: gpr64common, preferred-register: '' }
  - { id: 200, class: gpr64common, preferred-register: '' }
  - { id: 201, class: gpr64common, preferred-register: '' }
  - { id: 202, class: gpr64common, preferred-register: '' }
  - { id: 203, class: zpr, preferred-register: '' }
  - { id: 204, class: gpr64common, preferred-register: '' }
  - { id: 205, class: zpr, preferred-register: '' }
  - { id: 206, class: gpr64common, preferred-register: '' }
  - { id: 207, class: zpr, preferred-register: '' }
  - { id: 208, class: gpr64common, preferred-register: '' }
  - { id: 209, class: zpr, preferred-register: '' }
  - { id: 210, class: zpr, preferred-register: '' }
  - { id: 211, class: zpr, preferred-register: '' }
  - { id: 212, class: zpr, preferred-register: '' }
  - { id: 213, class: zpr, preferred-register: '' }
  - { id: 214, class: zpr, preferred-register: '' }
  - { id: 215, class: zpr, preferred-register: '' }
  - { id: 216, class: zpr, preferred-register: '' }
  - { id: 217, class: zpr, preferred-register: '' }
  - { id: 218, class: zpr, preferred-register: '' }
  - { id: 219, class: zpr, preferred-register: '' }
  - { id: 220, class: zpr, preferred-register: '' }
  - { id: 221, class: zpr, preferred-register: '' }
  - { id: 222, class: gpr64common, preferred-register: '' }
  - { id: 223, class: gpr64common, preferred-register: '' }
  - { id: 224, class: gpr64common, preferred-register: '' }
  - { id: 225, class: gpr64common, preferred-register: '' }
  - { id: 226, class: gpr64sp, preferred-register: '' }
  - { id: 227, class: gpr64, preferred-register: '' }
  - { id: 228, class: gpr64common, preferred-register: '' }
  - { id: 229, class: gpr64common, preferred-register: '' }
  - { id: 230, class: gpr64common, preferred-register: '' }
  - { id: 231, class: gpr64common, preferred-register: '' }
  - { id: 232, class: gpr64common, preferred-register: '' }
  - { id: 233, class: gpr64common, preferred-register: '' }
  - { id: 234, class: gpr64common, preferred-register: '' }
  - { id: 235, class: gpr64common, preferred-register: '' }
  - { id: 236, class: fpr64, preferred-register: '' }
  - { id: 237, class: gpr32all, preferred-register: '' }
  - { id: 238, class: gpr32common, preferred-register: '' }
  - { id: 239, class: gpr32, preferred-register: '' }
  - { id: 240, class: gpr32, preferred-register: '' }
  - { id: 241, class: gpr64sp, preferred-register: '' }
  - { id: 242, class: gpr64all, preferred-register: '' }
  - { id: 243, class: gpr32all, preferred-register: '' }
  - { id: 244, class: gpr64common, preferred-register: '' }
liveins:
  - { reg: '$x0', virtual-reg: '%9' }
frameInfo:
  isFrameAddressTaken: false
  isReturnAddressTaken: false
  hasStackMap:     false
  hasPatchPoint:   false
  stackSize:       0
  offsetAdjustment: 0
  maxAlignment:    1
  adjustsStack:    true
  hasCalls:        true
  stackProtector:  ''
  functionContext: ''
  maxCallFrameSize: 0
  cvBytesOfCalleeSavedRegisters: 0
  hasOpaqueSPAdjustment: false
  hasVAStart:      false
  hasMustTailInVarArgFunc: false
  hasTailCall:     true
  localFrameSize:  0
  savePoint:       ''
  restorePoint:    ''
fixedStack:      []
stack:           []
callSites:       []
debugValueSubstitutions: []
constants:       []
machineFunctionInfo: {}
body:             |
  bb.0.entry:
    successors: %bb.1(0x80000000)
    liveins: $x0
  
    %9:gpr64common = COPY $x0
    ADJCALLSTACKDOWN 0, 0, implicit-def dead $sp, implicit $sp
    %11:gpr64common = MOVaddr target-flags(aarch64-page) @__func__.s125, target-flags(aarch64-pageoff, aarch64-nc) @__func__.s125
    $x0 = COPY %11
    BL @initialise_arrays, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit-def $sp, implicit-def $w0
    ADJCALLSTACKUP 0, 0, implicit-def dead $sp, implicit $sp
    ADJCALLSTACKDOWN 0, 0, implicit-def dead $sp, implicit $sp
    %13:gpr64all = COPY $xzr
    $x0 = COPY %9
    $x1 = COPY %13
    BL @gettimeofday, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit $x1, implicit-def $sp, implicit-def $w0
    ADJCALLSTACKUP 0, 0, implicit-def dead $sp, implicit $sp
    %15:gpr32all = COPY $wzr
    %10:gpr32all = COPY %15
    %20:gpr32 = MOVi32imm 256
    %21:gpr64common = LOADgot target-flags(aarch64-got) @cc
    %23:gpr64common = LOADgot target-flags(aarch64-got) @bb
    %25:gpr64common = LOADgot target-flags(aarch64-got) @aa
    %30:ppr_3b = PTRUE_B 31
    %32:gpr64common = MOVi64imm 8
    %33:ppr_3b = PTRUE_D 31
    %35:gpr64common = MOVi64imm 16
    %37:gpr64common = MOVi64imm 24
    %52:gpr64common = LOADgot target-flags(aarch64-got) @flat_2d_array
    %54:gpr64common = MOVi64imm 1
    %55:gpr64common = MOVi64imm 9
    %56:gpr64common = MOVi64imm 17
    %57:gpr64common = MOVi64imm 25
    %58:gpr64common = MOVi64imm 32
    %60:gpr64common = MOVi64imm 40
    %62:gpr64common = MOVi64imm 48
    %64:gpr64common = MOVi64imm 56
    %78:gpr64common = MOVi64imm 33
    %79:gpr64common = MOVi64imm 41
    %80:gpr64common = MOVi64imm 49
    %81:gpr64common = MOVi64imm 57
    %82:gpr64common = MOVi64imm 64
    %84:gpr64common = MOVi64imm 72
    %86:gpr64common = MOVi64imm 80
    %88:gpr64common = MOVi64imm 88
    %102:gpr64common = MOVi64imm 65
    %103:gpr64common = MOVi64imm 73
    %104:gpr64common = MOVi64imm 81
    %105:gpr64common = MOVi64imm 89
    %106:gpr64common = MOVi64imm 96
    %108:gpr64common = MOVi64imm 104
    %110:gpr64common = MOVi64imm 112
    %112:gpr64common = MOVi64imm 120
    %126:gpr64common = MOVi64imm 97
    %127:gpr64common = MOVi64imm 105
    %128:gpr64common = MOVi64imm 113
    %129:gpr64common = MOVi64imm 121
    %130:gpr64common = MOVi64imm 128
    %132:gpr64common = MOVi64imm 136
    %134:gpr64common = MOVi64imm 144
    %136:gpr64common = MOVi64imm 152
    %150:gpr64common = MOVi64imm 129
    %151:gpr64common = MOVi64imm 137
    %152:gpr64common = MOVi64imm 145
    %153:gpr64common = MOVi64imm 153
    %154:gpr64common = MOVi64imm 160
    %156:gpr64common = MOVi64imm 168
    %158:gpr64common = MOVi64imm 176
    %160:gpr64common = MOVi64imm 184
    %174:gpr64common = MOVi64imm 161
    %175:gpr64common = MOVi64imm 169
    %176:gpr64common = MOVi64imm 177
    %177:gpr64common = MOVi64imm 185
    %178:gpr64common = MOVi64imm 192
    %180:gpr64common = MOVi64imm 200
    %182:gpr64common = MOVi64imm 208
    %184:gpr64common = MOVi64imm 216
    %198:gpr64common = MOVi64imm 193
    %199:gpr64common = MOVi64imm 201
    %200:gpr64common = MOVi64imm 209
    %201:gpr64common = MOVi64imm 217
    %202:gpr64common = MOVi64imm 224
    %204:gpr64common = MOVi64imm 232
    %206:gpr64common = MOVi64imm 240
    %208:gpr64common = MOVi64imm 248
    %222:gpr64common = MOVi64imm 225
    %223:gpr64common = MOVi64imm 233
    %224:gpr64common = MOVi64imm 241
    %225:gpr64common = MOVi64imm 249
    %228:gpr64common = LOADgot target-flags(aarch64-got) @a
    %229:gpr64common = LOADgot target-flags(aarch64-got) @b
    %230:gpr64common = LOADgot target-flags(aarch64-got) @c
    %231:gpr64common = LOADgot target-flags(aarch64-got) @d
    %232:gpr64common = LOADgot target-flags(aarch64-got) @e
    %236:fpr64 = FMOVD0
    %239:gpr32 = MOVi32imm 39000
  
  bb.1.for.cond2.preheader:
    successors: %bb.3(0x80000000)
  
    %0:gpr32sp = PHI %10, %bb.0, %8, %bb.4
    %18:gpr64 = MOVi64imm -1
    %17:gpr64all = COPY %18
    %19:gpr64all = COPY $xzr
    %16:gpr64all = COPY %19
    %1:gpr64all = SUBREG_TO_REG 0, %20, %subreg.sub_32
    B %bb.3
  
  bb.2.for.cond.cleanup:
    %241:gpr64sp = nuw ADDXri %9, 16, 0
    ADJCALLSTACKDOWN 0, 0, implicit-def dead $sp, implicit $sp
    %242:gpr64all = COPY $xzr
    $x0 = COPY %241
    $x1 = COPY %242
    BL @gettimeofday, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit $x1, implicit-def $sp, implicit-def $w0
    ADJCALLSTACKUP 0, 0, implicit-def dead $sp, implicit $sp
    %244:gpr64common = MOVaddr target-flags(aarch64-page) @__func__.s125, target-flags(aarch64-pageoff, aarch64-nc) @__func__.s125
    $x0 = COPY %244
    TCRETURNdi @calc_checksum, 0, csr_aarch64_aapcs, implicit $sp, implicit $x0
  
  bb.3.for.cond6.preheader:
    successors: %bb.3(0x7c000000), %bb.4(0x04000000)
  
    %2:gpr64common = PHI %16, %bb.1, %6, %bb.3
    %3:gpr64 = PHI %17, %bb.1, %5, %bb.3
    %4:gpr64sp = PHI %1, %bb.1, %7, %bb.3
    %22:gpr64common = ADDXrr %21, %2
    %24:gpr64common = ADDXrr %23, %2
    %26:gpr64common = ADDXrr %25, %2
    %27:gpr64 = UBFMXri %3, 32, 31
    %28:gpr64common = SBFMXri %3, 0, 31
    %29:gpr64sp = nsw ADDXri killed %28, 256, 0
    %5:gpr64all = COPY %29
    PRFMui 0, %26, 256
    %31:zpr = LD1B %30, %25, %2 :: (load unknown-size from %ir.uglygep403, align 64, !tbaa !6)
    %34:zpr = LD1D %33, %26, %32 :: (load unknown-size from %ir.uglygep402, align 64, !tbaa !6)
    %36:zpr = LD1D %33, %26, %35 :: (load unknown-size from %ir.uglygep400, align 64, !tbaa !6)
    %38:zpr = LD1D %33, %26, %37 :: (load unknown-size from %ir.uglygep398, align 64, !tbaa !6)
    PRFMui 0, %24, 256
    %39:zpr = LD1B %30, %23, %2 :: (load unknown-size from %ir.uglygep575, align 64, !tbaa !6)
    %40:zpr = LD1D %33, %24, %32 :: (load unknown-size from %ir.uglygep479, align 64, !tbaa !6)
    %41:zpr = LD1D %33, %24, %35 :: (load unknown-size from %ir.uglygep477, align 64, !tbaa !6)
    %42:zpr = LD1D %33, %24, %37 :: (load unknown-size from %ir.uglygep475, align 64, !tbaa !6)
    PRFMui 0, %22, 256
    %43:zpr = LD1B %30, %21, %2 :: (load unknown-size from %ir.uglygep574, align 64, !tbaa !6)
    %44:zpr = LD1D %33, %22, %32 :: (load unknown-size from %ir.uglygep557, align 64, !tbaa !6)
    %45:zpr = LD1D %33, %22, %35 :: (load unknown-size from %ir.uglygep555, align 64, !tbaa !6)
    %46:zpr = LD1D %33, %22, %37 :: (load unknown-size from %ir.uglygep553, align 64, !tbaa !6)
    %48:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %31, killed %43, killed %39
    %49:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %34, killed %44, killed %40
    %50:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %36, killed %45, killed %41
    %51:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %38, killed %46, killed %42
    %53:gpr64common = ADDXrs %52, killed %27, 157
    ST1D killed %48, %33, %53, %54 :: (store unknown-size into %ir.uglygep159, align 8, !tbaa !6)
    ST1D killed %49, %33, %53, %55 :: (store unknown-size into %ir.12, align 8, !tbaa !6)
    ST1D killed %50, %33, %53, %56 :: (store unknown-size into %ir.13, align 8, !tbaa !6)
    ST1D killed %51, %33, %53, %57 :: (store unknown-size into %ir.14, align 8, !tbaa !6)
    PRFMui 0, %26, 288
    %59:zpr = LD1D %33, %26, %58 :: (load unknown-size from %ir.uglygep396, align 64, !tbaa !6)
    %61:zpr = LD1D %33, %26, %60 :: (load unknown-size from %ir.uglygep394, align 64, !tbaa !6)
    %63:zpr = LD1D %33, %26, %62 :: (load unknown-size from %ir.uglygep392, align 64, !tbaa !6)
    %65:zpr = LD1D %33, %26, %64 :: (load unknown-size from %ir.uglygep390, align 64, !tbaa !6)
    PRFMui 0, %24, 288
    %66:zpr = LD1D %33, %24, %58 :: (load unknown-size from %ir.uglygep473, align 64, !tbaa !6)
    %67:zpr = LD1D %33, %24, %60 :: (load unknown-size from %ir.uglygep471, align 64, !tbaa !6)
    %68:zpr = LD1D %33, %24, %62 :: (load unknown-size from %ir.uglygep469, align 64, !tbaa !6)
    %69:zpr = LD1D %33, %24, %64 :: (load unknown-size from %ir.uglygep467, align 64, !tbaa !6)
    PRFMui 0, %22, 288
    %70:zpr = LD1D %33, %22, %58 :: (load unknown-size from %ir.uglygep551, align 64, !tbaa !6)
    %71:zpr = LD1D %33, %22, %60 :: (load unknown-size from %ir.uglygep549, align 64, !tbaa !6)
    %72:zpr = LD1D %33, %22, %62 :: (load unknown-size from %ir.uglygep547, align 64, !tbaa !6)
    %73:zpr = LD1D %33, %22, %64 :: (load unknown-size from %ir.uglygep545, align 64, !tbaa !6)
    %74:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %59, killed %70, killed %66
    %75:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %61, killed %71, killed %67
    %76:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %63, killed %72, killed %68
    %77:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %65, killed %73, killed %69
    ST1D killed %74, %33, %53, %78 :: (store unknown-size into %ir.uglygep185, align 8, !tbaa !6)
    ST1D killed %75, %33, %53, %79 :: (store unknown-size into %ir.23, align 8, !tbaa !6)
    ST1D killed %76, %33, %53, %80 :: (store unknown-size into %ir.24, align 8, !tbaa !6)
    ST1D killed %77, %33, %53, %81 :: (store unknown-size into %ir.25, align 8, !tbaa !6)
    PRFMui 0, %26, 320
    %83:zpr = LD1D %33, %26, %82 :: (load unknown-size from %ir.uglygep388, align 64, !tbaa !6)
    %85:zpr = LD1D %33, %26, %84 :: (load unknown-size from %ir.uglygep386, align 64, !tbaa !6)
    %87:zpr = LD1D %33, %26, %86 :: (load unknown-size from %ir.uglygep384, align 64, !tbaa !6)
    %89:zpr = LD1D %33, %26, %88 :: (load unknown-size from %ir.uglygep382, align 64, !tbaa !6)
    PRFMui 0, %24, 320
    %90:zpr = LD1D %33, %24, %82 :: (load unknown-size from %ir.uglygep465, align 64, !tbaa !6)
    %91:zpr = LD1D %33, %24, %84 :: (load unknown-size from %ir.uglygep463, align 64, !tbaa !6)
    %92:zpr = LD1D %33, %24, %86 :: (load unknown-size from %ir.uglygep461, align 64, !tbaa !6)
    %93:zpr = LD1D %33, %24, %88 :: (load unknown-size from %ir.uglygep459, align 64, !tbaa !6)
    PRFMui 0, %22, 320
    %94:zpr = LD1D %33, %22, %82 :: (load unknown-size from %ir.uglygep543, align 64, !tbaa !6)
    %95:zpr = LD1D %33, %22, %84 :: (load unknown-size from %ir.uglygep541, align 64, !tbaa !6)
    %96:zpr = LD1D %33, %22, %86 :: (load unknown-size from %ir.uglygep539, align 64, !tbaa !6)
    %97:zpr = LD1D %33, %22, %88 :: (load unknown-size from %ir.uglygep537, align 64, !tbaa !6)
    %98:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %83, killed %94, killed %90
    %99:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %85, killed %95, killed %91
    %100:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %87, killed %96, killed %92
    %101:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %89, killed %97, killed %93
    ST1D killed %98, %33, %53, %102 :: (store unknown-size into %ir.uglygep211, align 8, !tbaa !6)
    ST1D killed %99, %33, %53, %103 :: (store unknown-size into %ir.34, align 8, !tbaa !6)
    ST1D killed %100, %33, %53, %104 :: (store unknown-size into %ir.35, align 8, !tbaa !6)
    ST1D killed %101, %33, %53, %105 :: (store unknown-size into %ir.36, align 8, !tbaa !6)
    PRFMui 0, %26, 352
    %107:zpr = LD1D %33, %26, %106 :: (load unknown-size from %ir.uglygep380, align 64, !tbaa !6)
    %109:zpr = LD1D %33, %26, %108 :: (load unknown-size from %ir.uglygep378, align 64, !tbaa !6)
    %111:zpr = LD1D %33, %26, %110 :: (load unknown-size from %ir.uglygep376, align 64, !tbaa !6)
    %113:zpr = LD1D %33, %26, %112 :: (load unknown-size from %ir.uglygep374, align 64, !tbaa !6)
    PRFMui 0, %24, 352
    %114:zpr = LD1D %33, %24, %106 :: (load unknown-size from %ir.uglygep457, align 64, !tbaa !6)
    %115:zpr = LD1D %33, %24, %108 :: (load unknown-size from %ir.uglygep455, align 64, !tbaa !6)
    %116:zpr = LD1D %33, %24, %110 :: (load unknown-size from %ir.uglygep453, align 64, !tbaa !6)
    %117:zpr = LD1D %33, %24, %112 :: (load unknown-size from %ir.uglygep451, align 64, !tbaa !6)
    PRFMui 0, %22, 352
    %118:zpr = LD1D %33, %22, %106 :: (load unknown-size from %ir.uglygep535, align 64, !tbaa !6)
    %119:zpr = LD1D %33, %22, %108 :: (load unknown-size from %ir.uglygep533, align 64, !tbaa !6)
    %120:zpr = LD1D %33, %22, %110 :: (load unknown-size from %ir.uglygep531, align 64, !tbaa !6)
    %121:zpr = LD1D %33, %22, %112 :: (load unknown-size from %ir.uglygep529, align 64, !tbaa !6)
    %122:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %107, killed %118, killed %114
    %123:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %109, killed %119, killed %115
    %124:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %111, killed %120, killed %116
    %125:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %113, killed %121, killed %117
    ST1D killed %122, %33, %53, %126 :: (store unknown-size into %ir.uglygep237, align 8, !tbaa !6)
    ST1D killed %123, %33, %53, %127 :: (store unknown-size into %ir.45, align 8, !tbaa !6)
    ST1D killed %124, %33, %53, %128 :: (store unknown-size into %ir.46, align 8, !tbaa !6)
    ST1D killed %125, %33, %53, %129 :: (store unknown-size into %ir.47, align 8, !tbaa !6)
    PRFMui 0, %26, 384
    %131:zpr = LD1D %33, %26, %130 :: (load unknown-size from %ir.uglygep372, align 64, !tbaa !6)
    %133:zpr = LD1D %33, %26, %132 :: (load unknown-size from %ir.uglygep370, align 64, !tbaa !6)
    %135:zpr = LD1D %33, %26, %134 :: (load unknown-size from %ir.uglygep368, align 64, !tbaa !6)
    %137:zpr = LD1D %33, %26, %136 :: (load unknown-size from %ir.uglygep366, align 64, !tbaa !6)
    PRFMui 0, %24, 384
    %138:zpr = LD1D %33, %24, %130 :: (load unknown-size from %ir.uglygep449, align 64, !tbaa !6)
    %139:zpr = LD1D %33, %24, %132 :: (load unknown-size from %ir.uglygep447, align 64, !tbaa !6)
    %140:zpr = LD1D %33, %24, %134 :: (load unknown-size from %ir.uglygep445, align 64, !tbaa !6)
    %141:zpr = LD1D %33, %24, %136 :: (load unknown-size from %ir.uglygep443, align 64, !tbaa !6)
    PRFMui 0, %22, 384
    %142:zpr = LD1D %33, %22, %130 :: (load unknown-size from %ir.uglygep527, align 64, !tbaa !6)
    %143:zpr = LD1D %33, %22, %132 :: (load unknown-size from %ir.uglygep525, align 64, !tbaa !6)
    %144:zpr = LD1D %33, %22, %134 :: (load unknown-size from %ir.uglygep523, align 64, !tbaa !6)
    %145:zpr = LD1D %33, %22, %136 :: (load unknown-size from %ir.uglygep521, align 64, !tbaa !6)
    %146:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %131, killed %142, killed %138
    %147:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %133, killed %143, killed %139
    %148:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %135, killed %144, killed %140
    %149:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %137, killed %145, killed %141
    ST1D killed %146, %33, %53, %150 :: (store unknown-size into %ir.uglygep263, align 8, !tbaa !6)
    ST1D killed %147, %33, %53, %151 :: (store unknown-size into %ir.56, align 8, !tbaa !6)
    ST1D killed %148, %33, %53, %152 :: (store unknown-size into %ir.57, align 8, !tbaa !6)
    ST1D killed %149, %33, %53, %153 :: (store unknown-size into %ir.58, align 8, !tbaa !6)
    PRFMui 0, %26, 416
    %155:zpr = LD1D %33, %26, %154 :: (load unknown-size from %ir.uglygep364, align 64, !tbaa !6)
    %157:zpr = LD1D %33, %26, %156 :: (load unknown-size from %ir.uglygep362, align 64, !tbaa !6)
    %159:zpr = LD1D %33, %26, %158 :: (load unknown-size from %ir.uglygep360, align 64, !tbaa !6)
    %161:zpr = LD1D %33, %26, %160 :: (load unknown-size from %ir.uglygep358, align 64, !tbaa !6)
    PRFMui 0, %24, 416
    %162:zpr = LD1D %33, %24, %154 :: (load unknown-size from %ir.uglygep441, align 64, !tbaa !6)
    %163:zpr = LD1D %33, %24, %156 :: (load unknown-size from %ir.uglygep439, align 64, !tbaa !6)
    %164:zpr = LD1D %33, %24, %158 :: (load unknown-size from %ir.uglygep437, align 64, !tbaa !6)
    %165:zpr = LD1D %33, %24, %160 :: (load unknown-size from %ir.uglygep435, align 64, !tbaa !6)
    PRFMui 0, %22, 416
    %166:zpr = LD1D %33, %22, %154 :: (load unknown-size from %ir.uglygep519, align 64, !tbaa !6)
    %167:zpr = LD1D %33, %22, %156 :: (load unknown-size from %ir.uglygep517, align 64, !tbaa !6)
    %168:zpr = LD1D %33, %22, %158 :: (load unknown-size from %ir.uglygep515, align 64, !tbaa !6)
    %169:zpr = LD1D %33, %22, %160 :: (load unknown-size from %ir.uglygep513, align 64, !tbaa !6)
    %170:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %155, killed %166, killed %162
    %171:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %157, killed %167, killed %163
    %172:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %159, killed %168, killed %164
    %173:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %161, killed %169, killed %165
    ST1D killed %170, %33, %53, %174 :: (store unknown-size into %ir.uglygep289, align 8, !tbaa !6)
    ST1D killed %171, %33, %53, %175 :: (store unknown-size into %ir.67, align 8, !tbaa !6)
    ST1D killed %172, %33, %53, %176 :: (store unknown-size into %ir.68, align 8, !tbaa !6)
    ST1D killed %173, %33, %53, %177 :: (store unknown-size into %ir.69, align 8, !tbaa !6)
    PRFMui 0, %26, 448
    %179:zpr = LD1D %33, %26, %178 :: (load unknown-size from %ir.uglygep356, align 64, !tbaa !6)
    %181:zpr = LD1D %33, %26, %180 :: (load unknown-size from %ir.uglygep354, align 64, !tbaa !6)
    %183:zpr = LD1D %33, %26, %182 :: (load unknown-size from %ir.uglygep352, align 64, !tbaa !6)
    %185:zpr = LD1D %33, %26, %184 :: (load unknown-size from %ir.uglygep350, align 64, !tbaa !6)
    PRFMui 0, %24, 448
    %186:zpr = LD1D %33, %24, %178 :: (load unknown-size from %ir.uglygep433, align 64, !tbaa !6)
    %187:zpr = LD1D %33, %24, %180 :: (load unknown-size from %ir.uglygep431, align 64, !tbaa !6)
    %188:zpr = LD1D %33, %24, %182 :: (load unknown-size from %ir.uglygep429, align 64, !tbaa !6)
    %189:zpr = LD1D %33, %24, %184 :: (load unknown-size from %ir.uglygep427, align 64, !tbaa !6)
    PRFMui 0, %22, 448
    %190:zpr = LD1D %33, %22, %178 :: (load unknown-size from %ir.uglygep511, align 64, !tbaa !6)
    %191:zpr = LD1D %33, %22, %180 :: (load unknown-size from %ir.uglygep509, align 64, !tbaa !6)
    %192:zpr = LD1D %33, %22, %182 :: (load unknown-size from %ir.uglygep507, align 64, !tbaa !6)
    %193:zpr = LD1D %33, %22, %184 :: (load unknown-size from %ir.uglygep505, align 64, !tbaa !6)
    %194:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %179, killed %190, killed %186
    %195:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %181, killed %191, killed %187
    %196:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %183, killed %192, killed %188
    %197:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %185, killed %193, killed %189
    ST1D killed %194, %33, %53, %198 :: (store unknown-size into %ir.uglygep315, align 8, !tbaa !6)
    ST1D killed %195, %33, %53, %199 :: (store unknown-size into %ir.78, align 8, !tbaa !6)
    ST1D killed %196, %33, %53, %200 :: (store unknown-size into %ir.79, align 8, !tbaa !6)
    ST1D killed %197, %33, %53, %201 :: (store unknown-size into %ir.80, align 8, !tbaa !6)
    PRFMui 0, %26, 480
    %203:zpr = LD1D %33, %26, %202 :: (load unknown-size from %ir.uglygep348, align 64, !tbaa !6)
    %205:zpr = LD1D %33, %26, %204 :: (load unknown-size from %ir.uglygep346, align 64, !tbaa !6)
    %207:zpr = LD1D %33, %26, %206 :: (load unknown-size from %ir.uglygep344, align 64, !tbaa !6)
    %209:zpr = LD1D %33, %26, %208 :: (load unknown-size from %ir.uglygep342, align 64, !tbaa !6)
    PRFMui 0, %24, 480
    %210:zpr = LD1D %33, %24, %202 :: (load unknown-size from %ir.uglygep425, align 64, !tbaa !6)
    %211:zpr = LD1D %33, %24, %204 :: (load unknown-size from %ir.uglygep423, align 64, !tbaa !6)
    %212:zpr = LD1D %33, %24, %206 :: (load unknown-size from %ir.uglygep421, align 64, !tbaa !6)
    %213:zpr = LD1D %33, %24, %208 :: (load unknown-size from %ir.uglygep419, align 64, !tbaa !6)
    PRFMui 0, %22, 480
    %214:zpr = LD1D %33, %22, %202 :: (load unknown-size from %ir.uglygep503, align 64, !tbaa !6)
    %215:zpr = LD1D %33, %22, %204 :: (load unknown-size from %ir.uglygep501, align 64, !tbaa !6)
    %216:zpr = LD1D %33, %22, %206 :: (load unknown-size from %ir.uglygep499, align 64, !tbaa !6)
    %217:zpr = LD1D %33, %22, %208 :: (load unknown-size from %ir.uglygep497, align 64, !tbaa !6)
    %218:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %203, killed %214, killed %210
    %219:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %205, killed %215, killed %211
    %220:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %207, killed %216, killed %212
    %221:zpr = nnan ninf nsz arcp contract afn reassoc FMLA_ZPZZZ_UNDEF_D %33, killed %209, killed %217, killed %213
    ST1D killed %218, %33, %53, %222 :: (store unknown-size into %ir.uglygep341, align 8, !tbaa !6)
    ST1D killed %219, %33, %53, %223 :: (store unknown-size into %ir.89, align 8, !tbaa !6)
    ST1D killed %220, %33, %53, %224 :: (store unknown-size into %ir.90, align 8, !tbaa !6)
    ST1D killed %221, %33, %53, %225 :: (store unknown-size into %ir.91, align 8, !tbaa !6)
    %226:gpr64sp = nuw nsw ADDXri %2, 2048, 0
    %6:gpr64all = COPY %226
    %227:gpr64 = SUBSXri %4, 1, 0, implicit-def $nzcv
    %7:gpr64all = COPY %227
    Bcc 1, %bb.3, implicit $nzcv
    B %bb.4
  
  bb.4.for.cond.cleanup4:
    successors: %bb.2(0x04000000), %bb.1(0x7c000000)
  
    ADJCALLSTACKDOWN 0, 0, implicit-def dead $sp, implicit $sp
    $x0 = COPY %228
    $x1 = COPY %229
    $x2 = COPY %230
    $x3 = COPY %231
    $x4 = COPY %232
    $x5 = COPY %25
    $x6 = COPY %23
    $x7 = COPY %21
    $d0 = COPY %236
    BL @dummy, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit $x1, implicit $x2, implicit $x3, implicit $x4, implicit $x5, implicit $x6, implicit $x7, implicit $d0, implicit-def $sp, implicit-def $w0
    ADJCALLSTACKUP 0, 0, implicit-def dead $sp, implicit $sp
    %238:gpr32common = nuw nsw ADDWri %0, 1, 0
    %8:gpr32all = COPY %238
    dead $wzr = SUBSWrr %238, %239, implicit-def $nzcv
    Bcc 0, %bb.2, implicit $nzcv
    B %bb.1

...
