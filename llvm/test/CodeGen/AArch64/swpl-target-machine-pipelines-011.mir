#RUN: llc %s -mcpu=a64fx -fswp -O1 -swpl-debug-dump-resource-filter="BIC_PPzPP|FCMGT_PPzZ0_D" -start-before=aarch64-swpipeliner -o /dev/null 2>&1 | FileCheck %s

#CHECK:DBG(AArch64SwplTargetMachine::getPipelines): MI: %167:ppr_3b = FCMGT_PPzZ0_D
#CHECK-NEXT:  ResourceID: SVE_CMP_INST+1
#CHECK-NEXT:  latency: 4
#CHECK-NEXT:  seqDecode: false
#CHECK-NEXT:  stage/resource(): 0/FLA, 4/FLA_C

#CHECK:DBG(AArch64SwplTargetMachine::getPipelines): MI: %180:ppr_3b = BIC_PPzPP
#CHECK-NEXT:  ResourceID: PREDICATE_OP+1
#CHECK-NEXT:  latency: 3
#CHECK-NEXT:  seqDecode: false
#CHECK-NEXT:  stage/resource(): 0/PRX

--- |
  ; ModuleID = '/TSVC_2/src_sep_optnone/s279.c'
  source_filename = "/TSVC_2/src_sep_optnone/s279.c"
  target datalayout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128"
  target triple = "aarch64-unknown-linux-gnu"
  
  %struct.args_t = type { %struct.timeval, %struct.timeval, ptr }
  %struct.timeval = type { i64, i64 }
  
  @__func__.s279 = private unnamed_addr constant [5 x i8] c"s279\00", align 1
  @a = external global [32000 x double], align 64
  @b = external global [32000 x double], align 64
  @d = external global [32000 x double], align 64
  @e = external global [32000 x double], align 64
  @c = external global [32000 x double], align 64
  @aa = external global [256 x [256 x double]], align 64
  @bb = external global [256 x [256 x double]], align 64
  @cc = external global [256 x [256 x double]], align 64
  
  ; Function Attrs: nounwind uwtable vscale_range(4,4)
  define dso_local double @s279(ptr nocapture noundef %func_args) local_unnamed_addr #0 {
  entry:
    %call = tail call i32 @initialise_arrays(ptr noundef nonnull @__func__.s279) #6
    %call1 = tail call i32 @gettimeofday(ptr noundef %func_args, ptr noundef null) #6
    br label %vector.ph
  
  vector.ph:                                        ; preds = %for.cond.cleanup4, %entry
    %nl.073 = phi i32 [ 0, %entry ], [ %inc47, %for.cond.cleanup4 ]
    %0 = call i64 @llvm.start.loop.iterations.i64(i64 800)
    br label %vector.body
  
  vector.body:                                      ; preds = %vector.body, %vector.ph
    %lsr.iv = phi i64 [ %lsr.iv.next, %vector.body ], [ 0, %vector.ph ]
    %1 = phi i64 [ %0, %vector.ph ], [ %52, %vector.body ]
    %uglygep121 = getelementptr i8, ptr @a, i64 %lsr.iv
    %wide.load = load <vscale x 2 x double>, ptr %uglygep121, align 64, !tbaa !6
    %2 = fcmp ogt <vscale x 2 x double> %wide.load, zeroinitializer
    %uglygep122 = getelementptr i8, ptr @b, i64 %lsr.iv
    %wide.load78 = load <vscale x 2 x double>, ptr %uglygep122, align 64, !tbaa !6
    %3 = fneg <vscale x 2 x double> %wide.load78
    %uglygep123 = getelementptr i8, ptr @d, i64 %lsr.iv
    %wide.load79 = load <vscale x 2 x double>, ptr %uglygep123, align 64, !tbaa !6
    %4 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %wide.load79, <vscale x 2 x double> %wide.load79, <vscale x 2 x double> %3)
    %5 = xor <vscale x 2 x i1> %2, shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer)
    tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %4, ptr %uglygep122, i32 8, <vscale x 2 x i1> %5), !tbaa !6
    %6 = fcmp ugt <vscale x 2 x double> %4, %wide.load
    %uglygep124 = getelementptr i8, ptr @c, i64 %lsr.iv
    %wide.load80 = load <vscale x 2 x double>, ptr %uglygep124, align 64, !tbaa !6
    %uglygep125 = getelementptr i8, ptr @e, i64 %lsr.iv
    %wide.load81 = load <vscale x 2 x double>, ptr %uglygep125, align 64, !tbaa !6
    %7 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %wide.load79, <vscale x 2 x double> %wide.load81, <vscale x 2 x double> %wide.load80)
    %8 = select <vscale x 2 x i1> %5, <vscale x 2 x i1> %6, <vscale x 2 x i1> zeroinitializer
    tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %7, ptr %uglygep124, i32 8, <vscale x 2 x i1> %8), !tbaa !6
    %wide.load83 = load <vscale x 2 x double>, ptr %uglygep124, align 64, !tbaa !6
    %9 = fneg <vscale x 2 x double> %wide.load83
    %10 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %wide.load81, <vscale x 2 x double> %wide.load81, <vscale x 2 x double> %9)
    tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %10, ptr %uglygep124, i32 8, <vscale x 2 x i1> %2), !tbaa !6
    %wide.load85 = load <vscale x 2 x double>, ptr %uglygep122, align 64, !tbaa !6
    %wide.load86 = load <vscale x 2 x double>, ptr %uglygep123, align 64, !tbaa !6
    %predphi = select <vscale x 2 x i1> %2, <vscale x 2 x double> %wide.load86, <vscale x 2 x double> %wide.load79
    %predphi87 = select <vscale x 2 x i1> %8, <vscale x 2 x double> %wide.load79, <vscale x 2 x double> %predphi
    %predphi88 = select <vscale x 2 x i1> %2, <vscale x 2 x double> %10, <vscale x 2 x double> %wide.load80
    %predphi89 = select <vscale x 2 x i1> %8, <vscale x 2 x double> %7, <vscale x 2 x double> %predphi88
    %predphi90 = select <vscale x 2 x i1> %2, <vscale x 2 x double> %wide.load85, <vscale x 2 x double> %4
    %predphi91 = select <vscale x 2 x i1> %8, <vscale x 2 x double> %4, <vscale x 2 x double> %predphi90
    %11 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %predphi89, <vscale x 2 x double> %predphi87, <vscale x 2 x double> %predphi91)
    store <vscale x 2 x double> %11, ptr %uglygep121, align 64, !tbaa !6
    %uglygep134 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep135 = getelementptr i8, ptr %uglygep134, i64 64
    %wide.load.1 = load <vscale x 2 x double>, ptr %uglygep135, align 64, !tbaa !6
    %12 = fcmp ogt <vscale x 2 x double> %wide.load.1, zeroinitializer
    %uglygep132 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep133 = getelementptr i8, ptr %uglygep132, i64 64
    %wide.load78.1 = load <vscale x 2 x double>, ptr %uglygep133, align 64, !tbaa !6
    %13 = fneg <vscale x 2 x double> %wide.load78.1
    %uglygep130 = getelementptr i8, ptr @d, i64 %lsr.iv
    %uglygep131 = getelementptr i8, ptr %uglygep130, i64 64
    %wide.load79.1 = load <vscale x 2 x double>, ptr %uglygep131, align 64, !tbaa !6
    %14 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %wide.load79.1, <vscale x 2 x double> %wide.load79.1, <vscale x 2 x double> %13)
    %15 = xor <vscale x 2 x i1> %12, shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer)
    tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %14, ptr %uglygep133, i32 8, <vscale x 2 x i1> %15), !tbaa !6
    %16 = fcmp ugt <vscale x 2 x double> %14, %wide.load.1
    %uglygep128 = getelementptr i8, ptr @c, i64 %lsr.iv
    %uglygep129 = getelementptr i8, ptr %uglygep128, i64 64
    %wide.load80.1 = load <vscale x 2 x double>, ptr %uglygep129, align 64, !tbaa !6
    %uglygep126 = getelementptr i8, ptr @e, i64 %lsr.iv
    %uglygep127 = getelementptr i8, ptr %uglygep126, i64 64
    %wide.load81.1 = load <vscale x 2 x double>, ptr %uglygep127, align 64, !tbaa !6
    %17 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %wide.load79.1, <vscale x 2 x double> %wide.load81.1, <vscale x 2 x double> %wide.load80.1)
    %18 = select <vscale x 2 x i1> %15, <vscale x 2 x i1> %16, <vscale x 2 x i1> zeroinitializer
    tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %17, ptr %uglygep129, i32 8, <vscale x 2 x i1> %18), !tbaa !6
    %wide.load83.1 = load <vscale x 2 x double>, ptr %uglygep129, align 64, !tbaa !6
    %19 = fneg <vscale x 2 x double> %wide.load83.1
    %20 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %wide.load81.1, <vscale x 2 x double> %wide.load81.1, <vscale x 2 x double> %19)
    tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %20, ptr %uglygep129, i32 8, <vscale x 2 x i1> %12), !tbaa !6
    %wide.load85.1 = load <vscale x 2 x double>, ptr %uglygep133, align 64, !tbaa !6
    %wide.load86.1 = load <vscale x 2 x double>, ptr %uglygep131, align 64, !tbaa !6
    %predphi.1 = select <vscale x 2 x i1> %12, <vscale x 2 x double> %wide.load86.1, <vscale x 2 x double> %wide.load79.1
    %predphi87.1 = select <vscale x 2 x i1> %18, <vscale x 2 x double> %wide.load79.1, <vscale x 2 x double> %predphi.1
    %predphi88.1 = select <vscale x 2 x i1> %12, <vscale x 2 x double> %20, <vscale x 2 x double> %wide.load80.1
    %predphi89.1 = select <vscale x 2 x i1> %18, <vscale x 2 x double> %17, <vscale x 2 x double> %predphi88.1
    %predphi90.1 = select <vscale x 2 x i1> %12, <vscale x 2 x double> %wide.load85.1, <vscale x 2 x double> %14
    %predphi91.1 = select <vscale x 2 x i1> %18, <vscale x 2 x double> %14, <vscale x 2 x double> %predphi90.1
    %21 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %predphi89.1, <vscale x 2 x double> %predphi87.1, <vscale x 2 x double> %predphi91.1)
    store <vscale x 2 x double> %21, ptr %uglygep135, align 64, !tbaa !6
    %uglygep119 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep120 = getelementptr i8, ptr %uglygep119, i64 128
    %wide.load.2 = load <vscale x 2 x double>, ptr %uglygep120, align 64, !tbaa !6
    %22 = fcmp ogt <vscale x 2 x double> %wide.load.2, zeroinitializer
    %uglygep117 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep118 = getelementptr i8, ptr %uglygep117, i64 128
    %wide.load78.2 = load <vscale x 2 x double>, ptr %uglygep118, align 64, !tbaa !6
    %23 = fneg <vscale x 2 x double> %wide.load78.2
    %uglygep115 = getelementptr i8, ptr @d, i64 %lsr.iv
    %uglygep116 = getelementptr i8, ptr %uglygep115, i64 128
    %wide.load79.2 = load <vscale x 2 x double>, ptr %uglygep116, align 64, !tbaa !6
    %24 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %wide.load79.2, <vscale x 2 x double> %wide.load79.2, <vscale x 2 x double> %23)
    %25 = xor <vscale x 2 x i1> %22, shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer)
    tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %24, ptr %uglygep118, i32 8, <vscale x 2 x i1> %25), !tbaa !6
    %26 = fcmp ugt <vscale x 2 x double> %24, %wide.load.2
    %uglygep113 = getelementptr i8, ptr @c, i64 %lsr.iv
    %uglygep114 = getelementptr i8, ptr %uglygep113, i64 128
    %wide.load80.2 = load <vscale x 2 x double>, ptr %uglygep114, align 64, !tbaa !6
    %uglygep111 = getelementptr i8, ptr @e, i64 %lsr.iv
    %uglygep112 = getelementptr i8, ptr %uglygep111, i64 128
    %wide.load81.2 = load <vscale x 2 x double>, ptr %uglygep112, align 64, !tbaa !6
    %27 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %wide.load79.2, <vscale x 2 x double> %wide.load81.2, <vscale x 2 x double> %wide.load80.2)
    %28 = select <vscale x 2 x i1> %25, <vscale x 2 x i1> %26, <vscale x 2 x i1> zeroinitializer
    tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %27, ptr %uglygep114, i32 8, <vscale x 2 x i1> %28), !tbaa !6
    %wide.load83.2 = load <vscale x 2 x double>, ptr %uglygep114, align 64, !tbaa !6
    %29 = fneg <vscale x 2 x double> %wide.load83.2
    %30 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %wide.load81.2, <vscale x 2 x double> %wide.load81.2, <vscale x 2 x double> %29)
    tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %30, ptr %uglygep114, i32 8, <vscale x 2 x i1> %22), !tbaa !6
    %wide.load85.2 = load <vscale x 2 x double>, ptr %uglygep118, align 64, !tbaa !6
    %wide.load86.2 = load <vscale x 2 x double>, ptr %uglygep116, align 64, !tbaa !6
    %predphi.2 = select <vscale x 2 x i1> %22, <vscale x 2 x double> %wide.load86.2, <vscale x 2 x double> %wide.load79.2
    %predphi87.2 = select <vscale x 2 x i1> %28, <vscale x 2 x double> %wide.load79.2, <vscale x 2 x double> %predphi.2
    %predphi88.2 = select <vscale x 2 x i1> %22, <vscale x 2 x double> %30, <vscale x 2 x double> %wide.load80.2
    %predphi89.2 = select <vscale x 2 x i1> %28, <vscale x 2 x double> %27, <vscale x 2 x double> %predphi88.2
    %predphi90.2 = select <vscale x 2 x i1> %22, <vscale x 2 x double> %wide.load85.2, <vscale x 2 x double> %24
    %predphi91.2 = select <vscale x 2 x i1> %28, <vscale x 2 x double> %24, <vscale x 2 x double> %predphi90.2
    %31 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %predphi89.2, <vscale x 2 x double> %predphi87.2, <vscale x 2 x double> %predphi91.2)
    store <vscale x 2 x double> %31, ptr %uglygep120, align 64, !tbaa !6
    %uglygep109 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep110 = getelementptr i8, ptr %uglygep109, i64 192
    %wide.load.3 = load <vscale x 2 x double>, ptr %uglygep110, align 64, !tbaa !6
    %32 = fcmp ogt <vscale x 2 x double> %wide.load.3, zeroinitializer
    %uglygep107 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep108 = getelementptr i8, ptr %uglygep107, i64 192
    %wide.load78.3 = load <vscale x 2 x double>, ptr %uglygep108, align 64, !tbaa !6
    %33 = fneg <vscale x 2 x double> %wide.load78.3
    %uglygep105 = getelementptr i8, ptr @d, i64 %lsr.iv
    %uglygep106 = getelementptr i8, ptr %uglygep105, i64 192
    %wide.load79.3 = load <vscale x 2 x double>, ptr %uglygep106, align 64, !tbaa !6
    %34 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %wide.load79.3, <vscale x 2 x double> %wide.load79.3, <vscale x 2 x double> %33)
    %35 = xor <vscale x 2 x i1> %32, shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer)
    tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %34, ptr %uglygep108, i32 8, <vscale x 2 x i1> %35), !tbaa !6
    %36 = fcmp ugt <vscale x 2 x double> %34, %wide.load.3
    %uglygep103 = getelementptr i8, ptr @c, i64 %lsr.iv
    %uglygep104 = getelementptr i8, ptr %uglygep103, i64 192
    %wide.load80.3 = load <vscale x 2 x double>, ptr %uglygep104, align 64, !tbaa !6
    %uglygep101 = getelementptr i8, ptr @e, i64 %lsr.iv
    %uglygep102 = getelementptr i8, ptr %uglygep101, i64 192
    %wide.load81.3 = load <vscale x 2 x double>, ptr %uglygep102, align 64, !tbaa !6
    %37 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %wide.load79.3, <vscale x 2 x double> %wide.load81.3, <vscale x 2 x double> %wide.load80.3)
    %38 = select <vscale x 2 x i1> %35, <vscale x 2 x i1> %36, <vscale x 2 x i1> zeroinitializer
    tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %37, ptr %uglygep104, i32 8, <vscale x 2 x i1> %38), !tbaa !6
    %wide.load83.3 = load <vscale x 2 x double>, ptr %uglygep104, align 64, !tbaa !6
    %39 = fneg <vscale x 2 x double> %wide.load83.3
    %40 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %wide.load81.3, <vscale x 2 x double> %wide.load81.3, <vscale x 2 x double> %39)
    tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %40, ptr %uglygep104, i32 8, <vscale x 2 x i1> %32), !tbaa !6
    %wide.load85.3 = load <vscale x 2 x double>, ptr %uglygep108, align 64, !tbaa !6
    %wide.load86.3 = load <vscale x 2 x double>, ptr %uglygep106, align 64, !tbaa !6
    %predphi.3 = select <vscale x 2 x i1> %32, <vscale x 2 x double> %wide.load86.3, <vscale x 2 x double> %wide.load79.3
    %predphi87.3 = select <vscale x 2 x i1> %38, <vscale x 2 x double> %wide.load79.3, <vscale x 2 x double> %predphi.3
    %predphi88.3 = select <vscale x 2 x i1> %32, <vscale x 2 x double> %40, <vscale x 2 x double> %wide.load80.3
    %predphi89.3 = select <vscale x 2 x i1> %38, <vscale x 2 x double> %37, <vscale x 2 x double> %predphi88.3
    %predphi90.3 = select <vscale x 2 x i1> %32, <vscale x 2 x double> %wide.load85.3, <vscale x 2 x double> %34
    %predphi91.3 = select <vscale x 2 x i1> %38, <vscale x 2 x double> %34, <vscale x 2 x double> %predphi90.3
    %41 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %predphi89.3, <vscale x 2 x double> %predphi87.3, <vscale x 2 x double> %predphi91.3)
    store <vscale x 2 x double> %41, ptr %uglygep110, align 64, !tbaa !6
    %uglygep99 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep100 = getelementptr i8, ptr %uglygep99, i64 256
    %wide.load.4 = load <vscale x 2 x double>, ptr %uglygep100, align 64, !tbaa !6
    %42 = fcmp ogt <vscale x 2 x double> %wide.load.4, zeroinitializer
    %uglygep97 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep98 = getelementptr i8, ptr %uglygep97, i64 256
    %wide.load78.4 = load <vscale x 2 x double>, ptr %uglygep98, align 64, !tbaa !6
    %43 = fneg <vscale x 2 x double> %wide.load78.4
    %uglygep95 = getelementptr i8, ptr @d, i64 %lsr.iv
    %uglygep96 = getelementptr i8, ptr %uglygep95, i64 256
    %wide.load79.4 = load <vscale x 2 x double>, ptr %uglygep96, align 64, !tbaa !6
    %44 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %wide.load79.4, <vscale x 2 x double> %wide.load79.4, <vscale x 2 x double> %43)
    %45 = xor <vscale x 2 x i1> %42, shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer)
    tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %44, ptr %uglygep98, i32 8, <vscale x 2 x i1> %45), !tbaa !6
    %46 = fcmp ugt <vscale x 2 x double> %44, %wide.load.4
    %uglygep93 = getelementptr i8, ptr @c, i64 %lsr.iv
    %uglygep94 = getelementptr i8, ptr %uglygep93, i64 256
    %wide.load80.4 = load <vscale x 2 x double>, ptr %uglygep94, align 64, !tbaa !6
    %uglygep = getelementptr i8, ptr @e, i64 %lsr.iv
    %uglygep92 = getelementptr i8, ptr %uglygep, i64 256
    %wide.load81.4 = load <vscale x 2 x double>, ptr %uglygep92, align 64, !tbaa !6
    %47 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %wide.load79.4, <vscale x 2 x double> %wide.load81.4, <vscale x 2 x double> %wide.load80.4)
    %48 = select <vscale x 2 x i1> %45, <vscale x 2 x i1> %46, <vscale x 2 x i1> zeroinitializer
    tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %47, ptr %uglygep94, i32 8, <vscale x 2 x i1> %48), !tbaa !6
    %wide.load83.4 = load <vscale x 2 x double>, ptr %uglygep94, align 64, !tbaa !6
    %49 = fneg <vscale x 2 x double> %wide.load83.4
    %50 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %wide.load81.4, <vscale x 2 x double> %wide.load81.4, <vscale x 2 x double> %49)
    tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %50, ptr %uglygep94, i32 8, <vscale x 2 x i1> %42), !tbaa !6
    %wide.load85.4 = load <vscale x 2 x double>, ptr %uglygep98, align 64, !tbaa !6
    %wide.load86.4 = load <vscale x 2 x double>, ptr %uglygep96, align 64, !tbaa !6
    %predphi.4 = select <vscale x 2 x i1> %42, <vscale x 2 x double> %wide.load86.4, <vscale x 2 x double> %wide.load79.4
    %predphi87.4 = select <vscale x 2 x i1> %48, <vscale x 2 x double> %wide.load79.4, <vscale x 2 x double> %predphi.4
    %predphi88.4 = select <vscale x 2 x i1> %42, <vscale x 2 x double> %50, <vscale x 2 x double> %wide.load80.4
    %predphi89.4 = select <vscale x 2 x i1> %48, <vscale x 2 x double> %47, <vscale x 2 x double> %predphi88.4
    %predphi90.4 = select <vscale x 2 x i1> %42, <vscale x 2 x double> %wide.load85.4, <vscale x 2 x double> %44
    %predphi91.4 = select <vscale x 2 x i1> %48, <vscale x 2 x double> %44, <vscale x 2 x double> %predphi90.4
    %51 = tail call <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double> %predphi89.4, <vscale x 2 x double> %predphi87.4, <vscale x 2 x double> %predphi91.4)
    store <vscale x 2 x double> %51, ptr %uglygep100, align 64, !tbaa !6
    %lsr.iv.next = add nuw nsw i64 %lsr.iv, 320
    %52 = call i64 @llvm.loop.decrement.reg.i64(i64 %1, i64 1)
    %53 = icmp ne i64 %52, 0
    br i1 %53, label %vector.body, label %for.cond.cleanup4, !llvm.loop !10
  
  for.cond.cleanup:                                 ; preds = %for.cond.cleanup4
    %t2 = getelementptr inbounds %struct.args_t, ptr %func_args, i64 0, i32 1
    %call49 = tail call i32 @gettimeofday(ptr noundef nonnull %t2, ptr noundef null) #6
    %call50 = tail call double @calc_checksum(ptr noundef nonnull @__func__.s279) #6
    ret double %call50
  
  for.cond.cleanup4:                                ; preds = %vector.body
    %call45 = tail call i32 @dummy(ptr noundef nonnull @a, ptr noundef nonnull @b, ptr noundef nonnull @c, ptr noundef nonnull @d, ptr noundef nonnull @e, ptr noundef nonnull @aa, ptr noundef nonnull @bb, ptr noundef nonnull @cc, double noundef 0.000000e+00) #6
    %inc47 = add nuw nsw i32 %nl.073, 1
    %exitcond75.not = icmp eq i32 %inc47, 50000
    br i1 %exitcond75.not, label %for.cond.cleanup, label %vector.ph, !llvm.loop !13
  }
  
  declare i32 @initialise_arrays(ptr noundef) local_unnamed_addr #1
  
  ; Function Attrs: nofree nounwind
  declare noundef i32 @gettimeofday(ptr nocapture noundef, ptr nocapture noundef) local_unnamed_addr #2
  
  declare i32 @dummy(ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, double noundef) local_unnamed_addr #1
  
  declare double @calc_checksum(ptr noundef) local_unnamed_addr #1
  
  ; Function Attrs: nocallback nofree nosync nounwind readnone speculatable willreturn
  declare <vscale x 2 x double> @llvm.fmuladd.nxv2f64(<vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>) #3
  
  ; Function Attrs: argmemonly nocallback nofree nosync nounwind willreturn writeonly
  declare void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double>, ptr, i32 immarg, <vscale x 2 x i1>) #4
  
  ; Function Attrs: nocallback noduplicate nofree nosync nounwind willreturn
  declare i64 @llvm.start.loop.iterations.i64(i64) #5
  
  ; Function Attrs: nocallback noduplicate nofree nosync nounwind willreturn
  declare i64 @llvm.loop.decrement.reg.i64(i64, i64) #5
  
  attributes #0 = { nounwind uwtable vscale_range(4,4) "frame-pointer"="non-leaf" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="a64fx" "target-features"="+aes,+crc,+crypto,+fp-armv8,+fullfp16,+lse,+neon,+outline-atomics,+ras,+rdm,+sha2,+sve,+v8.2a" }
  attributes #1 = { "frame-pointer"="non-leaf" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="a64fx" "target-features"="+aes,+crc,+crypto,+fp-armv8,+fullfp16,+lse,+neon,+outline-atomics,+ras,+rdm,+sha2,+sve,+v8.2a" }
  attributes #2 = { nofree nounwind "frame-pointer"="non-leaf" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="a64fx" "target-features"="+aes,+crc,+crypto,+fp-armv8,+fullfp16,+lse,+neon,+outline-atomics,+ras,+rdm,+sha2,+sve,+v8.2a" }
  attributes #3 = { nocallback nofree nosync nounwind readnone speculatable willreturn }
  attributes #4 = { argmemonly nocallback nofree nosync nounwind willreturn writeonly }
  attributes #5 = { nocallback noduplicate nofree nosync nounwind willreturn }
  attributes #6 = { nounwind }
  
  !llvm.module.flags = !{!0, !1, !2, !3, !4}
  
  !0 = !{i32 1, !"wchar_size", i32 4}
  !1 = !{i32 7, !"PIC Level", i32 2}
  !2 = !{i32 7, !"PIE Level", i32 2}
  !3 = !{i32 7, !"uwtable", i32 2}
  !4 = !{i32 7, !"frame-pointer", i32 1}
  !6 = !{!7, !7, i64 0}
  !7 = !{!"double", !8, i64 0}
  !8 = !{!"omnipotent char", !9, i64 0}
  !9 = !{!"Simple C/C++ TBAA"}
  !10 = distinct !{!10, !11, !12}
  !11 = !{!"llvm.loop.mustprogress"}
  !12 = !{!"llvm.loop.isvectorized", i32 1}
  !13 = distinct !{!13, !11}

...
---
name:            s279
alignment:       8
exposesReturnsTwice: false
legalized:       false
regBankSelected: false
selected:        false
failedISel:      false
tracksRegLiveness: true
hasWinCFI:       false
callsEHReturn:   false
callsUnwindInit: false
hasEHCatchret:   false
hasEHScopes:     false
hasEHFunclets:   false
failsVerification: false
tracksDebugUserValues: false
registers:
  - { id: 0, class: gpr32sp, preferred-register: '' }
  - { id: 1, class: gpr64all, preferred-register: '' }
  - { id: 2, class: gpr64common, preferred-register: '' }
  - { id: 3, class: gpr64sp, preferred-register: '' }
  - { id: 4, class: gpr64all, preferred-register: '' }
  - { id: 5, class: gpr64all, preferred-register: '' }
  - { id: 6, class: gpr32all, preferred-register: '' }
  - { id: 7, class: gpr64common, preferred-register: '' }
  - { id: 8, class: gpr32all, preferred-register: '' }
  - { id: 9, class: gpr64common, preferred-register: '' }
  - { id: 10, class: gpr32all, preferred-register: '' }
  - { id: 11, class: gpr64all, preferred-register: '' }
  - { id: 12, class: gpr32all, preferred-register: '' }
  - { id: 13, class: gpr32all, preferred-register: '' }
  - { id: 14, class: gpr64all, preferred-register: '' }
  - { id: 15, class: gpr64all, preferred-register: '' }
  - { id: 16, class: gpr32, preferred-register: '' }
  - { id: 17, class: gpr64common, preferred-register: '' }
  - { id: 18, class: gpr64common, preferred-register: '' }
  - { id: 19, class: ppr_3b, preferred-register: '' }
  - { id: 20, class: zpr, preferred-register: '' }
  - { id: 21, class: ppr_3b, preferred-register: '' }
  - { id: 22, class: ppr_3b, preferred-register: '' }
  - { id: 23, class: ppr_3b, preferred-register: '' }
  - { id: 24, class: ppr_3b, preferred-register: '' }
  - { id: 25, class: gpr64common, preferred-register: '' }
  - { id: 26, class: gpr64common, preferred-register: '' }
  - { id: 27, class: zpr, preferred-register: '' }
  - { id: 28, class: gpr64common, preferred-register: '' }
  - { id: 29, class: gpr64common, preferred-register: '' }
  - { id: 30, class: zpr, preferred-register: '' }
  - { id: 31, class: zpr, preferred-register: '' }
  - { id: 32, class: ppr, preferred-register: '' }
  - { id: 33, class: gpr64common, preferred-register: '' }
  - { id: 34, class: gpr64common, preferred-register: '' }
  - { id: 35, class: zpr, preferred-register: '' }
  - { id: 36, class: gpr64common, preferred-register: '' }
  - { id: 37, class: gpr64common, preferred-register: '' }
  - { id: 38, class: zpr, preferred-register: '' }
  - { id: 39, class: zpr, preferred-register: '' }
  - { id: 40, class: ppr_3b, preferred-register: '' }
  - { id: 41, class: zpr, preferred-register: '' }
  - { id: 42, class: zpr, preferred-register: '' }
  - { id: 43, class: zpr, preferred-register: '' }
  - { id: 44, class: zpr, preferred-register: '' }
  - { id: 45, class: zpr, preferred-register: '' }
  - { id: 46, class: zpr, preferred-register: '' }
  - { id: 47, class: zpr, preferred-register: '' }
  - { id: 48, class: zpr, preferred-register: '' }
  - { id: 49, class: zpr, preferred-register: '' }
  - { id: 50, class: zpr, preferred-register: '' }
  - { id: 51, class: zpr, preferred-register: '' }
  - { id: 52, class: gpr64common, preferred-register: '' }
  - { id: 53, class: zpr, preferred-register: '' }
  - { id: 54, class: ppr_3b, preferred-register: '' }
  - { id: 55, class: ppr_3b, preferred-register: '' }
  - { id: 56, class: zpr, preferred-register: '' }
  - { id: 57, class: zpr, preferred-register: '' }
  - { id: 58, class: zpr, preferred-register: '' }
  - { id: 59, class: ppr, preferred-register: '' }
  - { id: 60, class: zpr, preferred-register: '' }
  - { id: 61, class: zpr, preferred-register: '' }
  - { id: 62, class: zpr, preferred-register: '' }
  - { id: 63, class: ppr_3b, preferred-register: '' }
  - { id: 64, class: zpr, preferred-register: '' }
  - { id: 65, class: zpr, preferred-register: '' }
  - { id: 66, class: zpr, preferred-register: '' }
  - { id: 67, class: zpr, preferred-register: '' }
  - { id: 68, class: zpr, preferred-register: '' }
  - { id: 69, class: zpr, preferred-register: '' }
  - { id: 70, class: zpr, preferred-register: '' }
  - { id: 71, class: zpr, preferred-register: '' }
  - { id: 72, class: zpr, preferred-register: '' }
  - { id: 73, class: zpr, preferred-register: '' }
  - { id: 74, class: zpr, preferred-register: '' }
  - { id: 75, class: gpr64common, preferred-register: '' }
  - { id: 76, class: zpr, preferred-register: '' }
  - { id: 77, class: ppr_3b, preferred-register: '' }
  - { id: 78, class: ppr_3b, preferred-register: '' }
  - { id: 79, class: zpr, preferred-register: '' }
  - { id: 80, class: zpr, preferred-register: '' }
  - { id: 81, class: zpr, preferred-register: '' }
  - { id: 82, class: ppr, preferred-register: '' }
  - { id: 83, class: zpr, preferred-register: '' }
  - { id: 84, class: zpr, preferred-register: '' }
  - { id: 85, class: zpr, preferred-register: '' }
  - { id: 86, class: ppr_3b, preferred-register: '' }
  - { id: 87, class: zpr, preferred-register: '' }
  - { id: 88, class: zpr, preferred-register: '' }
  - { id: 89, class: zpr, preferred-register: '' }
  - { id: 90, class: zpr, preferred-register: '' }
  - { id: 91, class: zpr, preferred-register: '' }
  - { id: 92, class: zpr, preferred-register: '' }
  - { id: 93, class: zpr, preferred-register: '' }
  - { id: 94, class: zpr, preferred-register: '' }
  - { id: 95, class: zpr, preferred-register: '' }
  - { id: 96, class: zpr, preferred-register: '' }
  - { id: 97, class: zpr, preferred-register: '' }
  - { id: 98, class: gpr64common, preferred-register: '' }
  - { id: 99, class: zpr, preferred-register: '' }
  - { id: 100, class: ppr_3b, preferred-register: '' }
  - { id: 101, class: ppr_3b, preferred-register: '' }
  - { id: 102, class: zpr, preferred-register: '' }
  - { id: 103, class: zpr, preferred-register: '' }
  - { id: 104, class: zpr, preferred-register: '' }
  - { id: 105, class: ppr, preferred-register: '' }
  - { id: 106, class: zpr, preferred-register: '' }
  - { id: 107, class: zpr, preferred-register: '' }
  - { id: 108, class: zpr, preferred-register: '' }
  - { id: 109, class: ppr_3b, preferred-register: '' }
  - { id: 110, class: zpr, preferred-register: '' }
  - { id: 111, class: zpr, preferred-register: '' }
  - { id: 112, class: zpr, preferred-register: '' }
  - { id: 113, class: zpr, preferred-register: '' }
  - { id: 114, class: zpr, preferred-register: '' }
  - { id: 115, class: zpr, preferred-register: '' }
  - { id: 116, class: zpr, preferred-register: '' }
  - { id: 117, class: zpr, preferred-register: '' }
  - { id: 118, class: zpr, preferred-register: '' }
  - { id: 119, class: zpr, preferred-register: '' }
  - { id: 120, class: zpr, preferred-register: '' }
  - { id: 121, class: gpr64common, preferred-register: '' }
  - { id: 122, class: zpr, preferred-register: '' }
  - { id: 123, class: ppr_3b, preferred-register: '' }
  - { id: 124, class: ppr_3b, preferred-register: '' }
  - { id: 125, class: zpr, preferred-register: '' }
  - { id: 126, class: zpr, preferred-register: '' }
  - { id: 127, class: zpr, preferred-register: '' }
  - { id: 128, class: ppr, preferred-register: '' }
  - { id: 129, class: zpr, preferred-register: '' }
  - { id: 130, class: zpr, preferred-register: '' }
  - { id: 131, class: zpr, preferred-register: '' }
  - { id: 132, class: ppr_3b, preferred-register: '' }
  - { id: 133, class: zpr, preferred-register: '' }
  - { id: 134, class: zpr, preferred-register: '' }
  - { id: 135, class: zpr, preferred-register: '' }
  - { id: 136, class: zpr, preferred-register: '' }
  - { id: 137, class: zpr, preferred-register: '' }
  - { id: 138, class: zpr, preferred-register: '' }
  - { id: 139, class: zpr, preferred-register: '' }
  - { id: 140, class: zpr, preferred-register: '' }
  - { id: 141, class: zpr, preferred-register: '' }
  - { id: 142, class: zpr, preferred-register: '' }
  - { id: 143, class: zpr, preferred-register: '' }
  - { id: 144, class: gpr64sp, preferred-register: '' }
  - { id: 145, class: gpr64, preferred-register: '' }
  - { id: 146, class: gpr64common, preferred-register: '' }
  - { id: 147, class: gpr64common, preferred-register: '' }
  - { id: 148, class: gpr64common, preferred-register: '' }
  - { id: 149, class: gpr64common, preferred-register: '' }
  - { id: 150, class: gpr64common, preferred-register: '' }
  - { id: 151, class: gpr64common, preferred-register: '' }
  - { id: 152, class: gpr64common, preferred-register: '' }
  - { id: 153, class: gpr64common, preferred-register: '' }
  - { id: 154, class: fpr64, preferred-register: '' }
  - { id: 155, class: gpr32all, preferred-register: '' }
  - { id: 156, class: gpr32common, preferred-register: '' }
  - { id: 157, class: gpr32, preferred-register: '' }
  - { id: 158, class: gpr32, preferred-register: '' }
  - { id: 159, class: gpr64sp, preferred-register: '' }
  - { id: 160, class: gpr64all, preferred-register: '' }
  - { id: 161, class: gpr32all, preferred-register: '' }
  - { id: 162, class: gpr64common, preferred-register: '' }
liveins:
  - { reg: '$x0', virtual-reg: '%7' }
frameInfo:
  isFrameAddressTaken: false
  isReturnAddressTaken: false
  hasStackMap:     false
  hasPatchPoint:   false
  stackSize:       0
  offsetAdjustment: 0
  maxAlignment:    1
  adjustsStack:    true
  hasCalls:        true
  stackProtector:  ''
  functionContext: ''
  maxCallFrameSize: 0
  cvBytesOfCalleeSavedRegisters: 0
  hasOpaqueSPAdjustment: false
  hasVAStart:      false
  hasMustTailInVarArgFunc: false
  hasTailCall:     true
  localFrameSize:  0
  savePoint:       ''
  restorePoint:    ''
fixedStack:      []
stack:           []
callSites:       []
debugValueSubstitutions: []
constants:       []
machineFunctionInfo: {}
body:             |
  bb.0.entry:
    successors: %bb.1(0x80000000)
    liveins: $x0
  
    %7:gpr64common = COPY $x0
    ADJCALLSTACKDOWN 0, 0, implicit-def dead $sp, implicit $sp
    %9:gpr64common = MOVaddr target-flags(aarch64-page) @__func__.s279, target-flags(aarch64-pageoff, aarch64-nc) @__func__.s279
    $x0 = COPY %9
    BL @initialise_arrays, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit-def $sp, implicit-def $w0
    ADJCALLSTACKUP 0, 0, implicit-def dead $sp, implicit $sp
    ADJCALLSTACKDOWN 0, 0, implicit-def dead $sp, implicit $sp
    %11:gpr64all = COPY $xzr
    $x0 = COPY %7
    $x1 = COPY %11
    BL @gettimeofday, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit $x1, implicit-def $sp, implicit-def $w0
    ADJCALLSTACKUP 0, 0, implicit-def dead $sp, implicit $sp
    %13:gpr32all = COPY $wzr
    %8:gpr32all = COPY %13
    %16:gpr32 = MOVi32imm 800
    %17:gpr64common = LOADgot target-flags(aarch64-got) @a
    %19:ppr_3b = PTRUE_B 31
    %21:ppr_3b = PTRUE_D 31
    %25:gpr64common = LOADgot target-flags(aarch64-got) @b
    %28:gpr64common = LOADgot target-flags(aarch64-got) @d
    %33:gpr64common = LOADgot target-flags(aarch64-got) @c
    %36:gpr64common = LOADgot target-flags(aarch64-got) @e
    %52:gpr64common = MOVi64imm 8
    %75:gpr64common = MOVi64imm 16
    %98:gpr64common = MOVi64imm 24
    %121:gpr64common = MOVi64imm 32
    %151:gpr64common = LOADgot target-flags(aarch64-got) @aa
    %152:gpr64common = LOADgot target-flags(aarch64-got) @bb
    %153:gpr64common = LOADgot target-flags(aarch64-got) @cc
    %154:fpr64 = FMOVD0
    %157:gpr32 = MOVi32imm 50000
  
  bb.1.vector.ph:
    successors: %bb.2(0x80000000)
  
    %0:gpr32sp = PHI %8, %bb.0, %6, %bb.4
    %15:gpr64all = COPY $xzr
    %14:gpr64all = COPY %15
    %1:gpr64all = SUBREG_TO_REG 0, %16, %subreg.sub_32
  
  bb.2.vector.body:
    successors: %bb.2(0x7c000000), %bb.4(0x04000000)
  
    %2:gpr64common = PHI %14, %bb.1, %4, %bb.2
    %3:gpr64sp = PHI %1, %bb.1, %5, %bb.2
    %18:gpr64common = ADDXrr %17, %2
    %20:zpr = LD1B %19, %17, %2 :: (load unknown-size from %ir.uglygep121, align 64, !tbaa !6)
    %22:ppr_3b = FCMGT_PPzZ0_D %21, %20
    %24:ppr_3b = EOR_PPzPP %21, %22, %21
    %26:gpr64common = ADDXrr %25, %2
    %27:zpr = LD1B %19, %25, %2 :: (load unknown-size from %ir.uglygep122, align 64, !tbaa !6)
    %29:gpr64common = ADDXrr %28, %2
    %30:zpr = LD1B %19, %28, %2 :: (load unknown-size from %ir.uglygep123, align 64, !tbaa !6)
    %31:zpr = FNMLS_ZPZZZ_D_UNDEF %21, killed %27, %30, %30
    ST1D_IMM %31, %24, %26, 0 :: (store unknown-size into %ir.uglygep122, align 8, !tbaa !6)
    %32:ppr = FCMGE_PPzZZ_D %21, %20, %31
    %34:gpr64common = ADDXrr %33, %2
    %35:zpr = LD1B %19, %33, %2 :: (load unknown-size from %ir.uglygep124, align 64, !tbaa !6)
    %37:gpr64common = ADDXrr %36, %2
    %38:zpr = LD1B %19, %36, %2 :: (load unknown-size from %ir.uglygep125, align 64, !tbaa !6)
    %39:zpr = FMLA_ZPZZZ_D_UNDEF %21, %35, %30, %38
    %40:ppr_3b = BIC_PPzPP %24, %24, killed %32
    ST1D_IMM %39, %40, %34, 0 :: (store unknown-size into %ir.uglygep124, align 8, !tbaa !6)
    %41:zpr = LD1B %19, %33, %2 :: (load unknown-size from %ir.uglygep124, align 64, !tbaa !6)
    %42:zpr = FNMLS_ZPZZZ_D_UNDEF %21, killed %41, %38, %38
    ST1D_IMM %42, %22, %34, 0 :: (store unknown-size into %ir.uglygep124, align 8, !tbaa !6)
    %43:zpr = LD1B %19, %25, %2 :: (load unknown-size from %ir.uglygep122, align 64, !tbaa !6)
    %44:zpr = LD1B %19, %28, %2 :: (load unknown-size from %ir.uglygep123, align 64, !tbaa !6)
    %45:zpr = SEL_ZPZZ_D %22, killed %44, %30
    %46:zpr = SEL_ZPZZ_D %40, %30, killed %45
    %47:zpr = SEL_ZPZZ_D %22, %42, %35
    %48:zpr = SEL_ZPZZ_D %40, %39, killed %47
    %49:zpr = SEL_ZPZZ_D %22, killed %43, %31
    %50:zpr = SEL_ZPZZ_D %40, %31, killed %49
    %51:zpr = FMLA_ZPZZZ_D_UNDEF %21, killed %50, killed %48, killed %46
    ST1B killed %51, %19, %17, %2 :: (store unknown-size into %ir.uglygep121, align 64, !tbaa !6)
    %53:zpr = LD1D %21, %18, %52 :: (load unknown-size from %ir.uglygep135, align 64, !tbaa !6)
    %54:ppr_3b = FCMGT_PPzZ0_D %21, %53
    %55:ppr_3b = EOR_PPzPP %21, %54, %21
    %56:zpr = LD1D %21, %26, %52 :: (load unknown-size from %ir.uglygep133, align 64, !tbaa !6)
    %57:zpr = LD1D %21, %29, %52 :: (load unknown-size from %ir.uglygep131, align 64, !tbaa !6)
    %58:zpr = FNMLS_ZPZZZ_D_UNDEF %21, killed %56, %57, %57
    ST1D %58, %55, %26, %52 :: (store unknown-size into %ir.uglygep133, align 8, !tbaa !6)
    %59:ppr = FCMGE_PPzZZ_D %21, %53, %58
    %60:zpr = LD1D %21, %34, %52 :: (load unknown-size from %ir.uglygep129, align 64, !tbaa !6)
    %61:zpr = LD1D %21, %37, %52 :: (load unknown-size from %ir.uglygep127, align 64, !tbaa !6)
    %62:zpr = FMLA_ZPZZZ_D_UNDEF %21, %60, %57, %61
    %63:ppr_3b = BIC_PPzPP %55, %55, killed %59
    ST1D %62, %63, %34, %52 :: (store unknown-size into %ir.uglygep129, align 8, !tbaa !6)
    %64:zpr = LD1D %21, %34, %52 :: (load unknown-size from %ir.uglygep129, align 64, !tbaa !6)
    %65:zpr = FNMLS_ZPZZZ_D_UNDEF %21, killed %64, %61, %61
    ST1D %65, %54, %34, %52 :: (store unknown-size into %ir.uglygep129, align 8, !tbaa !6)
    %66:zpr = LD1D %21, %26, %52 :: (load unknown-size from %ir.uglygep133, align 64, !tbaa !6)
    %67:zpr = LD1D %21, %29, %52 :: (load unknown-size from %ir.uglygep131, align 64, !tbaa !6)
    %68:zpr = SEL_ZPZZ_D %54, killed %67, %57
    %69:zpr = SEL_ZPZZ_D %63, %57, killed %68
    %70:zpr = SEL_ZPZZ_D %54, %65, %60
    %71:zpr = SEL_ZPZZ_D %63, %62, killed %70
    %72:zpr = SEL_ZPZZ_D %54, killed %66, %58
    %73:zpr = SEL_ZPZZ_D %63, %58, killed %72
    %74:zpr = FMLA_ZPZZZ_D_UNDEF %21, killed %73, killed %71, killed %69
    ST1D killed %74, %21, %18, %52 :: (store unknown-size into %ir.uglygep135, align 64, !tbaa !6)
    %76:zpr = LD1D %21, %18, %75 :: (load unknown-size from %ir.uglygep120, align 64, !tbaa !6)
    %77:ppr_3b = FCMGT_PPzZ0_D %21, %76
    %78:ppr_3b = EOR_PPzPP %21, %77, %21
    %79:zpr = LD1D %21, %26, %75 :: (load unknown-size from %ir.uglygep118, align 64, !tbaa !6)
    %80:zpr = LD1D %21, %29, %75 :: (load unknown-size from %ir.uglygep116, align 64, !tbaa !6)
    %81:zpr = FNMLS_ZPZZZ_D_UNDEF %21, killed %79, %80, %80
    ST1D %81, %78, %26, %75 :: (store unknown-size into %ir.uglygep118, align 8, !tbaa !6)
    %82:ppr = FCMGE_PPzZZ_D %21, %76, %81
    %83:zpr = LD1D %21, %34, %75 :: (load unknown-size from %ir.uglygep114, align 64, !tbaa !6)
    %84:zpr = LD1D %21, %37, %75 :: (load unknown-size from %ir.uglygep112, align 64, !tbaa !6)
    %85:zpr = FMLA_ZPZZZ_D_UNDEF %21, %83, %80, %84
    %86:ppr_3b = BIC_PPzPP %78, %78, killed %82
    ST1D %85, %86, %34, %75 :: (store unknown-size into %ir.uglygep114, align 8, !tbaa !6)
    %87:zpr = LD1D %21, %34, %75 :: (load unknown-size from %ir.uglygep114, align 64, !tbaa !6)
    %88:zpr = FNMLS_ZPZZZ_D_UNDEF %21, killed %87, %84, %84
    ST1D %88, %77, %34, %75 :: (store unknown-size into %ir.uglygep114, align 8, !tbaa !6)
    %89:zpr = LD1D %21, %26, %75 :: (load unknown-size from %ir.uglygep118, align 64, !tbaa !6)
    %90:zpr = LD1D %21, %29, %75 :: (load unknown-size from %ir.uglygep116, align 64, !tbaa !6)
    %91:zpr = SEL_ZPZZ_D %77, killed %90, %80
    %92:zpr = SEL_ZPZZ_D %86, %80, killed %91
    %93:zpr = SEL_ZPZZ_D %77, %88, %83
    %94:zpr = SEL_ZPZZ_D %86, %85, killed %93
    %95:zpr = SEL_ZPZZ_D %77, killed %89, %81
    %96:zpr = SEL_ZPZZ_D %86, %81, killed %95
    %97:zpr = FMLA_ZPZZZ_D_UNDEF %21, killed %96, killed %94, killed %92
    ST1D killed %97, %21, %18, %75 :: (store unknown-size into %ir.uglygep120, align 64, !tbaa !6)
    %99:zpr = LD1D %21, %18, %98 :: (load unknown-size from %ir.uglygep110, align 64, !tbaa !6)
    %100:ppr_3b = FCMGT_PPzZ0_D %21, %99
    %101:ppr_3b = EOR_PPzPP %21, %100, %21
    %102:zpr = LD1D %21, %26, %98 :: (load unknown-size from %ir.uglygep108, align 64, !tbaa !6)
    %103:zpr = LD1D %21, %29, %98 :: (load unknown-size from %ir.uglygep106, align 64, !tbaa !6)
    %104:zpr = FNMLS_ZPZZZ_D_UNDEF %21, killed %102, %103, %103
    ST1D %104, %101, %26, %98 :: (store unknown-size into %ir.uglygep108, align 8, !tbaa !6)
    %105:ppr = FCMGE_PPzZZ_D %21, %99, %104
    %106:zpr = LD1D %21, %34, %98 :: (load unknown-size from %ir.uglygep104, align 64, !tbaa !6)
    %107:zpr = LD1D %21, %37, %98 :: (load unknown-size from %ir.uglygep102, align 64, !tbaa !6)
    %108:zpr = FMLA_ZPZZZ_D_UNDEF %21, %106, %103, %107
    %109:ppr_3b = BIC_PPzPP %101, %101, killed %105
    ST1D %108, %109, %34, %98 :: (store unknown-size into %ir.uglygep104, align 8, !tbaa !6)
    %110:zpr = LD1D %21, %34, %98 :: (load unknown-size from %ir.uglygep104, align 64, !tbaa !6)
    %111:zpr = FNMLS_ZPZZZ_D_UNDEF %21, killed %110, %107, %107
    ST1D %111, %100, %34, %98 :: (store unknown-size into %ir.uglygep104, align 8, !tbaa !6)
    %112:zpr = LD1D %21, %26, %98 :: (load unknown-size from %ir.uglygep108, align 64, !tbaa !6)
    %113:zpr = LD1D %21, %29, %98 :: (load unknown-size from %ir.uglygep106, align 64, !tbaa !6)
    %114:zpr = SEL_ZPZZ_D %100, killed %113, %103
    %115:zpr = SEL_ZPZZ_D %109, %103, killed %114
    %116:zpr = SEL_ZPZZ_D %100, %111, %106
    %117:zpr = SEL_ZPZZ_D %109, %108, killed %116
    %118:zpr = SEL_ZPZZ_D %100, killed %112, %104
    %119:zpr = SEL_ZPZZ_D %109, %104, killed %118
    %120:zpr = FMLA_ZPZZZ_D_UNDEF %21, killed %119, killed %117, killed %115
    ST1D killed %120, %21, %18, %98 :: (store unknown-size into %ir.uglygep110, align 64, !tbaa !6)
    %122:zpr = LD1D %21, %18, %121 :: (load unknown-size from %ir.uglygep100, align 64, !tbaa !6)
    %123:ppr_3b = FCMGT_PPzZ0_D %21, %122
    %124:ppr_3b = EOR_PPzPP %21, %123, %21
    %125:zpr = LD1D %21, %26, %121 :: (load unknown-size from %ir.uglygep98, align 64, !tbaa !6)
    %126:zpr = LD1D %21, %29, %121 :: (load unknown-size from %ir.uglygep96, align 64, !tbaa !6)
    %127:zpr = FNMLS_ZPZZZ_D_UNDEF %21, killed %125, %126, %126
    ST1D %127, %124, %26, %121 :: (store unknown-size into %ir.uglygep98, align 8, !tbaa !6)
    %128:ppr = FCMGE_PPzZZ_D %21, %122, %127
    %129:zpr = LD1D %21, %34, %121 :: (load unknown-size from %ir.uglygep94, align 64, !tbaa !6)
    %130:zpr = LD1D %21, %37, %121 :: (load unknown-size from %ir.uglygep92, align 64, !tbaa !6)
    %131:zpr = FMLA_ZPZZZ_D_UNDEF %21, %129, %126, %130
    %132:ppr_3b = BIC_PPzPP %124, %124, killed %128
    ST1D %131, %132, %34, %121 :: (store unknown-size into %ir.uglygep94, align 8, !tbaa !6)
    %133:zpr = LD1D %21, %34, %121 :: (load unknown-size from %ir.uglygep94, align 64, !tbaa !6)
    %134:zpr = FNMLS_ZPZZZ_D_UNDEF %21, killed %133, %130, %130
    ST1D %134, %123, %34, %121 :: (store unknown-size into %ir.uglygep94, align 8, !tbaa !6)
    %135:zpr = LD1D %21, %26, %121 :: (load unknown-size from %ir.uglygep98, align 64, !tbaa !6)
    %136:zpr = LD1D %21, %29, %121 :: (load unknown-size from %ir.uglygep96, align 64, !tbaa !6)
    %137:zpr = SEL_ZPZZ_D %123, killed %136, %126
    %138:zpr = SEL_ZPZZ_D %132, %126, killed %137
    %139:zpr = SEL_ZPZZ_D %123, %134, %129
    %140:zpr = SEL_ZPZZ_D %132, %131, killed %139
    %141:zpr = SEL_ZPZZ_D %123, killed %135, %127
    %142:zpr = SEL_ZPZZ_D %132, %127, killed %141
    %143:zpr = FMLA_ZPZZZ_D_UNDEF %21, killed %142, killed %140, killed %138
    ST1D killed %143, %21, %18, %121 :: (store unknown-size into %ir.uglygep100, align 64, !tbaa !6)
    %144:gpr64sp = nuw nsw ADDXri %2, 320, 0
    %4:gpr64all = COPY %144
    %145:gpr64 = SUBSXri %3, 1, 0, implicit-def $nzcv
    %5:gpr64all = COPY %145
    Bcc 1, %bb.2, implicit $nzcv
    B %bb.4
  
  bb.3.for.cond.cleanup:
    %159:gpr64sp = nuw ADDXri %7, 16, 0
    ADJCALLSTACKDOWN 0, 0, implicit-def dead $sp, implicit $sp
    %160:gpr64all = COPY $xzr
    $x0 = COPY %159
    $x1 = COPY %160
    BL @gettimeofday, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit $x1, implicit-def $sp, implicit-def $w0
    ADJCALLSTACKUP 0, 0, implicit-def dead $sp, implicit $sp
    %162:gpr64common = MOVaddr target-flags(aarch64-page) @__func__.s279, target-flags(aarch64-pageoff, aarch64-nc) @__func__.s279
    $x0 = COPY %162
    TCRETURNdi @calc_checksum, 0, csr_aarch64_aapcs, implicit $sp, implicit $x0
  
  bb.4.for.cond.cleanup4:
    successors: %bb.3(0x04000000), %bb.1(0x7c000000)
  
    ADJCALLSTACKDOWN 0, 0, implicit-def dead $sp, implicit $sp
    $x0 = COPY %17
    $x1 = COPY %25
    $x2 = COPY %33
    $x3 = COPY %28
    $x4 = COPY %36
    $x5 = COPY %151
    $x6 = COPY %152
    $x7 = COPY %153
    $d0 = COPY %154
    BL @dummy, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit $x1, implicit $x2, implicit $x3, implicit $x4, implicit $x5, implicit $x6, implicit $x7, implicit $d0, implicit-def $sp, implicit-def $w0
    ADJCALLSTACKUP 0, 0, implicit-def dead $sp, implicit $sp
    %156:gpr32common = nuw nsw ADDWri %0, 1, 0
    %6:gpr32all = COPY %156
    dead $wzr = SUBSWrr %156, %157, implicit-def $nzcv
    Bcc 0, %bb.3, implicit $nzcv
    B %bb.1

...
