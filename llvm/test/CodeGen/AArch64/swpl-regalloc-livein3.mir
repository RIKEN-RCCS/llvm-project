# RUN: llc %s -o /dev/null -O1 -mcpu=a64fx -fswp -swpl-enable-reg-alloc -swpl-enable-proepi-copy -start-before=aarch64-swpipeliner -swpl-debug-dump-mir=8 2>&1 | FileCheck %s

# CHECK:bb.8.vector.body:
# CHECK:  $q12_q13 = COPY %37:qq
# CHECK:  SWPLIVEOUT implicit $x0, implicit $x1, implicit $x2, implicit $x3, implicit $x4, implicit $x5, implicit $q12_q13, implicit $x7, implicit $x8, implicit $x9, implicit $x11
# CHECK:bb.2.vector.body:
# CHECK:  SWPLIVEIN implicit-def $x0, implicit-def $x1, implicit-def $x2, implicit-def $x3, implicit-def $x4, implicit-def $x5, implicit-def $q12_q13, implicit-def $x7, implicit-def $x8, implicit-def $x9, implicit-def $x11
# CHECK:  SWPLIVEOUT implicit $x0, implicit $x1, implicit $x2, implicit $x3, implicit $x4, implicit $x5, implicit $q12_q13, implicit $x7, implicit $x8, implicit $x9, implicit $x11, implicit $x6, implicit $q0_q1, implicit $x12
# CHECK:bb.9.vector.body:
# CHECK:  SWPLIVEIN implicit-def $x0, implicit-def $x1, implicit-def $x6, implicit-def $q0_q1, implicit-def $x7, implicit-def $x8, implicit-def $x11, implicit-def $x11, implicit-def $x12
# CHECK:  %441:qq = COPY $q0_q1

# Test when multiple registers (qq) are livein/liveout to the kernel.

--- |
  ; ModuleID = 'a10.c'
  source_filename = "a10.c"
  target datalayout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128"
  target triple = "aarch64-unknown-linux-gnu"
  
  @b = dso_local global [32000 x float] zeroinitializer, align 64
  @c = dso_local global [32000 x float] zeroinitializer, align 64
  @d = dso_local global [32000 x float] zeroinitializer, align 64
  @a = dso_local global [32000 x float] zeroinitializer, align 64
  @e = dso_local global [32000 x float] zeroinitializer, align 64
  @aa = dso_local global [256 x [256 x float]] zeroinitializer, align 64
  @bb = dso_local global [256 x [256 x float]] zeroinitializer, align 64
  @cc = dso_local global [256 x [256 x float]] zeroinitializer, align 64
  
  ; Function Attrs: nounwind uwtable vscale_range(1,16)
  define dso_local void @a10(ptr nocapture noundef readnone %func_args) local_unnamed_addr #0 {
  entry:
    br label %vector.ph
  
  vector.ph:                                        ; preds = %for.cond.cleanup3, %entry
    %nl.039 = phi i32 [ 0, %entry ], [ %inc24, %for.cond.cleanup3 ]
    %0 = call i64 @llvm.start.loop.iterations.i64(i64 400)
    br label %vector.body
  
  vector.body:                                      ; preds = %vector.body, %vector.ph
    %lsr.iv137 = phi ptr [ %uglygep138, %vector.body ], [ getelementptr (i8, ptr @a, i64 160), %vector.ph ]
    %lsr.iv = phi i64 [ %lsr.iv.next, %vector.body ], [ 0, %vector.ph ]
    %1 = phi i64 [ %0, %vector.ph ], [ %62, %vector.body ]
    %uglygep149 = getelementptr i8, ptr getelementptr (i8, ptr @b, i64 144), i64 %lsr.iv
    %uglygep150 = getelementptr i8, ptr %uglygep149, i64 -144
    %wide.load = load <4 x float>, ptr %uglygep150, align 32, !tbaa !6
    %uglygep151 = getelementptr i8, ptr %uglygep149, i64 -128
    %wide.load48 = load <4 x float>, ptr %uglygep151, align 16, !tbaa !6
    %uglygep167 = getelementptr i8, ptr getelementptr (i8, ptr @c, i64 144), i64 %lsr.iv
    %uglygep168 = getelementptr i8, ptr %uglygep167, i64 -144
    %wide.load49 = load <4 x float>, ptr %uglygep168, align 32, !tbaa !6
    %uglygep169 = getelementptr i8, ptr %uglygep167, i64 -128
    %wide.load50 = load <4 x float>, ptr %uglygep169, align 16, !tbaa !6
    %uglygep187 = getelementptr i8, ptr getelementptr (i8, ptr @d, i64 80), i64 %lsr.iv
    %uglygep188 = getelementptr i8, ptr %uglygep187, i64 -80
    %wide.load51 = load <4 x float>, ptr %uglygep188, align 32, !tbaa !6
    %uglygep189 = getelementptr i8, ptr %uglygep187, i64 -64
    %wide.load52 = load <4 x float>, ptr %uglygep189, align 16, !tbaa !6
    %2 = fmul fast <4 x float> %wide.load51, %wide.load49
    %3 = fmul fast <4 x float> %wide.load52, %wide.load50
    %4 = fadd fast <4 x float> %2, %wide.load
    %5 = fadd fast <4 x float> %3, %wide.load48
    %uglygep123 = getelementptr i8, ptr @e, i64 %lsr.iv
    %wide.load53 = load <4 x float>, ptr %uglygep123, align 32, !tbaa !6
    %uglygep133 = getelementptr i8, ptr %uglygep123, i64 16
    %wide.load54 = load <4 x float>, ptr %uglygep133, align 16, !tbaa !6
    %6 = fmul fast <4 x float> %wide.load53, %wide.load51
    %7 = fmul fast <4 x float> %wide.load54, %wide.load52
    %8 = fadd fast <4 x float> %6, %wide.load
    %9 = fadd fast <4 x float> %7, %wide.load48
    %uglygep139 = getelementptr i8, ptr %lsr.iv137, i64 -160
    %uglygep143 = getelementptr i8, ptr %lsr.iv137, i64 -128
    %10 = shufflevector <4 x float> %4, <4 x float> %8, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
    %11 = shufflevector <4 x float> %4, <4 x float> %8, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
    call void @llvm.aarch64.neon.st2.v4f32.p0(<4 x float> %10, <4 x float> %11, ptr %uglygep139)
    %12 = shufflevector <4 x float> %5, <4 x float> %9, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
    %13 = shufflevector <4 x float> %5, <4 x float> %9, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
    call void @llvm.aarch64.neon.st2.v4f32.p0(<4 x float> %12, <4 x float> %13, ptr %uglygep143)
    %uglygep154 = getelementptr i8, ptr getelementptr (i8, ptr @b, i64 144), i64 %lsr.iv
    %uglygep155 = getelementptr i8, ptr %uglygep154, i64 -112
    %wide.load.1 = load <4 x float>, ptr %uglygep155, align 32, !tbaa !6
    %uglygep152 = getelementptr i8, ptr getelementptr (i8, ptr @b, i64 144), i64 %lsr.iv
    %uglygep153 = getelementptr i8, ptr %uglygep152, i64 -96
    %wide.load48.1 = load <4 x float>, ptr %uglygep153, align 16, !tbaa !6
    %uglygep172 = getelementptr i8, ptr getelementptr (i8, ptr @c, i64 144), i64 %lsr.iv
    %uglygep173 = getelementptr i8, ptr %uglygep172, i64 -112
    %wide.load49.1 = load <4 x float>, ptr %uglygep173, align 32, !tbaa !6
    %uglygep170 = getelementptr i8, ptr getelementptr (i8, ptr @c, i64 144), i64 %lsr.iv
    %uglygep171 = getelementptr i8, ptr %uglygep170, i64 -96
    %wide.load50.1 = load <4 x float>, ptr %uglygep171, align 16, !tbaa !6
    %uglygep196 = getelementptr i8, ptr getelementptr (i8, ptr @d, i64 80), i64 %lsr.iv
    %uglygep197 = getelementptr i8, ptr %uglygep196, i64 -48
    %wide.load51.1 = load <4 x float>, ptr %uglygep197, align 32, !tbaa !6
    %uglygep194 = getelementptr i8, ptr getelementptr (i8, ptr @d, i64 80), i64 %lsr.iv
    %uglygep195 = getelementptr i8, ptr %uglygep194, i64 -32
    %wide.load52.1 = load <4 x float>, ptr %uglygep195, align 16, !tbaa !6
    %14 = fmul fast <4 x float> %wide.load51.1, %wide.load49.1
    %15 = fmul fast <4 x float> %wide.load52.1, %wide.load50.1
    %16 = fadd fast <4 x float> %14, %wide.load.1
    %17 = fadd fast <4 x float> %15, %wide.load48.1
    %uglygep134 = getelementptr i8, ptr @e, i64 %lsr.iv
    %uglygep135 = getelementptr i8, ptr %uglygep134, i64 32
    %wide.load53.1 = load <4 x float>, ptr %uglygep135, align 32, !tbaa !6
    %uglygep136 = getelementptr i8, ptr %uglygep134, i64 48
    %wide.load54.1 = load <4 x float>, ptr %uglygep136, align 16, !tbaa !6
    %18 = fmul fast <4 x float> %wide.load53.1, %wide.load51.1
    %19 = fmul fast <4 x float> %wide.load54.1, %wide.load52.1
    %20 = fadd fast <4 x float> %18, %wide.load.1
    %21 = fadd fast <4 x float> %19, %wide.load48.1
    %uglygep140 = getelementptr i8, ptr %lsr.iv137, i64 -96
    %uglygep141 = getelementptr i8, ptr %lsr.iv137, i64 -64
    %22 = shufflevector <4 x float> %16, <4 x float> %20, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
    %23 = shufflevector <4 x float> %16, <4 x float> %20, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
    call void @llvm.aarch64.neon.st2.v4f32.p0(<4 x float> %22, <4 x float> %23, ptr %uglygep140)
    %24 = shufflevector <4 x float> %17, <4 x float> %21, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
    %25 = shufflevector <4 x float> %17, <4 x float> %21, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
    call void @llvm.aarch64.neon.st2.v4f32.p0(<4 x float> %24, <4 x float> %25, ptr %uglygep141)
    %uglygep158 = getelementptr i8, ptr getelementptr (i8, ptr @b, i64 144), i64 %lsr.iv
    %uglygep159 = getelementptr i8, ptr %uglygep158, i64 -80
    %wide.load.2 = load <4 x float>, ptr %uglygep159, align 32, !tbaa !6
    %uglygep156 = getelementptr i8, ptr getelementptr (i8, ptr @b, i64 144), i64 %lsr.iv
    %uglygep157 = getelementptr i8, ptr %uglygep156, i64 -64
    %wide.load48.2 = load <4 x float>, ptr %uglygep157, align 16, !tbaa !6
    %uglygep176 = getelementptr i8, ptr getelementptr (i8, ptr @c, i64 144), i64 %lsr.iv
    %uglygep177 = getelementptr i8, ptr %uglygep176, i64 -80
    %wide.load49.2 = load <4 x float>, ptr %uglygep177, align 32, !tbaa !6
    %uglygep174 = getelementptr i8, ptr getelementptr (i8, ptr @c, i64 144), i64 %lsr.iv
    %uglygep175 = getelementptr i8, ptr %uglygep174, i64 -64
    %wide.load50.2 = load <4 x float>, ptr %uglygep175, align 16, !tbaa !6
    %uglygep199 = getelementptr i8, ptr getelementptr (i8, ptr @d, i64 80), i64 %lsr.iv
    %uglygep200 = getelementptr i8, ptr %uglygep199, i64 -16
    %wide.load51.2 = load <4 x float>, ptr %uglygep200, align 32, !tbaa !6
    %uglygep184 = getelementptr i8, ptr getelementptr (i8, ptr @d, i64 80), i64 %lsr.iv
    %wide.load52.2 = load <4 x float>, ptr %uglygep184, align 16, !tbaa !6
    %26 = fmul fast <4 x float> %wide.load51.2, %wide.load49.2
    %27 = fmul fast <4 x float> %wide.load52.2, %wide.load50.2
    %28 = fadd fast <4 x float> %26, %wide.load.2
    %29 = fadd fast <4 x float> %27, %wide.load48.2
    %uglygep130 = getelementptr i8, ptr @e, i64 %lsr.iv
    %uglygep131 = getelementptr i8, ptr %uglygep130, i64 64
    %wide.load53.2 = load <4 x float>, ptr %uglygep131, align 32, !tbaa !6
    %uglygep132 = getelementptr i8, ptr %uglygep130, i64 80
    %wide.load54.2 = load <4 x float>, ptr %uglygep132, align 16, !tbaa !6
    %30 = fmul fast <4 x float> %wide.load53.2, %wide.load51.2
    %31 = fmul fast <4 x float> %wide.load54.2, %wide.load52.2
    %32 = fadd fast <4 x float> %30, %wide.load.2
    %33 = fadd fast <4 x float> %31, %wide.load48.2
    %uglygep144 = getelementptr i8, ptr %lsr.iv137, i64 -32
    %34 = shufflevector <4 x float> %28, <4 x float> %32, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
    %35 = shufflevector <4 x float> %28, <4 x float> %32, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
    call void @llvm.aarch64.neon.st2.v4f32.p0(<4 x float> %34, <4 x float> %35, ptr %uglygep144)
    %36 = shufflevector <4 x float> %29, <4 x float> %33, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
    %37 = shufflevector <4 x float> %29, <4 x float> %33, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
    call void @llvm.aarch64.neon.st2.v4f32.p0(<4 x float> %36, <4 x float> %37, ptr %lsr.iv137)
    %uglygep162 = getelementptr i8, ptr getelementptr (i8, ptr @b, i64 144), i64 %lsr.iv
    %uglygep163 = getelementptr i8, ptr %uglygep162, i64 -48
    %wide.load.3 = load <4 x float>, ptr %uglygep163, align 32, !tbaa !6
    %uglygep160 = getelementptr i8, ptr getelementptr (i8, ptr @b, i64 144), i64 %lsr.iv
    %uglygep161 = getelementptr i8, ptr %uglygep160, i64 -32
    %wide.load48.3 = load <4 x float>, ptr %uglygep161, align 16, !tbaa !6
    %uglygep180 = getelementptr i8, ptr getelementptr (i8, ptr @c, i64 144), i64 %lsr.iv
    %uglygep181 = getelementptr i8, ptr %uglygep180, i64 -48
    %wide.load49.3 = load <4 x float>, ptr %uglygep181, align 32, !tbaa !6
    %uglygep178 = getelementptr i8, ptr getelementptr (i8, ptr @c, i64 144), i64 %lsr.iv
    %uglygep179 = getelementptr i8, ptr %uglygep178, i64 -32
    %wide.load50.3 = load <4 x float>, ptr %uglygep179, align 16, !tbaa !6
    %uglygep192 = getelementptr i8, ptr getelementptr (i8, ptr @d, i64 80), i64 %lsr.iv
    %uglygep193 = getelementptr i8, ptr %uglygep192, i64 16
    %wide.load51.3 = load <4 x float>, ptr %uglygep193, align 32, !tbaa !6
    %uglygep198 = getelementptr i8, ptr %uglygep192, i64 32
    %wide.load52.3 = load <4 x float>, ptr %uglygep198, align 16, !tbaa !6
    %38 = fmul fast <4 x float> %wide.load51.3, %wide.load49.3
    %39 = fmul fast <4 x float> %wide.load52.3, %wide.load50.3
    %40 = fadd fast <4 x float> %38, %wide.load.3
    %41 = fadd fast <4 x float> %39, %wide.load48.3
    %uglygep127 = getelementptr i8, ptr @e, i64 %lsr.iv
    %uglygep128 = getelementptr i8, ptr %uglygep127, i64 96
    %wide.load53.3 = load <4 x float>, ptr %uglygep128, align 32, !tbaa !6
    %uglygep129 = getelementptr i8, ptr %uglygep127, i64 112
    %wide.load54.3 = load <4 x float>, ptr %uglygep129, align 16, !tbaa !6
    %42 = fmul fast <4 x float> %wide.load53.3, %wide.load51.3
    %43 = fmul fast <4 x float> %wide.load54.3, %wide.load52.3
    %44 = fadd fast <4 x float> %42, %wide.load.3
    %45 = fadd fast <4 x float> %43, %wide.load48.3
    %uglygep145 = getelementptr i8, ptr %lsr.iv137, i64 32
    %uglygep146 = getelementptr i8, ptr %lsr.iv137, i64 64
    %46 = shufflevector <4 x float> %40, <4 x float> %44, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
    %47 = shufflevector <4 x float> %40, <4 x float> %44, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
    call void @llvm.aarch64.neon.st2.v4f32.p0(<4 x float> %46, <4 x float> %47, ptr %uglygep145)
    %48 = shufflevector <4 x float> %41, <4 x float> %45, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
    %49 = shufflevector <4 x float> %41, <4 x float> %45, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
    call void @llvm.aarch64.neon.st2.v4f32.p0(<4 x float> %48, <4 x float> %49, ptr %uglygep146)
    %uglygep164 = getelementptr i8, ptr getelementptr (i8, ptr @b, i64 144), i64 %lsr.iv
    %uglygep165 = getelementptr i8, ptr %uglygep164, i64 -16
    %wide.load.4 = load <4 x float>, ptr %uglygep165, align 32, !tbaa !6
    %uglygep148 = getelementptr i8, ptr getelementptr (i8, ptr @b, i64 144), i64 %lsr.iv
    %wide.load48.4 = load <4 x float>, ptr %uglygep148, align 16, !tbaa !6
    %uglygep182 = getelementptr i8, ptr getelementptr (i8, ptr @c, i64 144), i64 %lsr.iv
    %uglygep183 = getelementptr i8, ptr %uglygep182, i64 -16
    %wide.load49.4 = load <4 x float>, ptr %uglygep183, align 32, !tbaa !6
    %uglygep166 = getelementptr i8, ptr getelementptr (i8, ptr @c, i64 144), i64 %lsr.iv
    %wide.load50.4 = load <4 x float>, ptr %uglygep166, align 16, !tbaa !6
    %uglygep190 = getelementptr i8, ptr getelementptr (i8, ptr @d, i64 80), i64 %lsr.iv
    %uglygep191 = getelementptr i8, ptr %uglygep190, i64 48
    %wide.load51.4 = load <4 x float>, ptr %uglygep191, align 32, !tbaa !6
    %uglygep185 = getelementptr i8, ptr getelementptr (i8, ptr @d, i64 80), i64 %lsr.iv
    %uglygep186 = getelementptr i8, ptr %uglygep185, i64 64
    %wide.load52.4 = load <4 x float>, ptr %uglygep186, align 16, !tbaa !6
    %50 = fmul fast <4 x float> %wide.load51.4, %wide.load49.4
    %51 = fmul fast <4 x float> %wide.load52.4, %wide.load50.4
    %52 = fadd fast <4 x float> %50, %wide.load.4
    %53 = fadd fast <4 x float> %51, %wide.load48.4
    %uglygep124 = getelementptr i8, ptr @e, i64 %lsr.iv
    %uglygep125 = getelementptr i8, ptr %uglygep124, i64 128
    %wide.load53.4 = load <4 x float>, ptr %uglygep125, align 32, !tbaa !6
    %uglygep126 = getelementptr i8, ptr %uglygep124, i64 144
    %wide.load54.4 = load <4 x float>, ptr %uglygep126, align 16, !tbaa !6
    %54 = fmul fast <4 x float> %wide.load53.4, %wide.load51.4
    %55 = fmul fast <4 x float> %wide.load54.4, %wide.load52.4
    %56 = fadd fast <4 x float> %54, %wide.load.4
    %57 = fadd fast <4 x float> %55, %wide.load48.4
    %uglygep147 = getelementptr i8, ptr %lsr.iv137, i64 96
    %uglygep142 = getelementptr i8, ptr %lsr.iv137, i64 128
    %58 = shufflevector <4 x float> %52, <4 x float> %56, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
    %59 = shufflevector <4 x float> %52, <4 x float> %56, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
    call void @llvm.aarch64.neon.st2.v4f32.p0(<4 x float> %58, <4 x float> %59, ptr %uglygep147)
    %60 = shufflevector <4 x float> %53, <4 x float> %57, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
    %61 = shufflevector <4 x float> %53, <4 x float> %57, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
    call void @llvm.aarch64.neon.st2.v4f32.p0(<4 x float> %60, <4 x float> %61, ptr %uglygep142)
    %lsr.iv.next = add nuw nsw i64 %lsr.iv, 160
    %uglygep138 = getelementptr i8, ptr %lsr.iv137, i64 320
    %62 = call i64 @llvm.loop.decrement.reg.i64(i64 %1, i64 1)
    %63 = icmp ne i64 %62, 0
    br i1 %63, label %vector.body, label %for.cond.cleanup3, !llvm.loop !10
  
  for.cond.cleanup:                                 ; preds = %for.cond.cleanup3
    ret void
  
  for.cond.cleanup3:                                ; preds = %vector.body
    %call = tail call i32 @dummy(ptr noundef nonnull @a, ptr noundef nonnull @b, ptr noundef nonnull @c, ptr noundef nonnull @d, ptr noundef nonnull @e, ptr noundef nonnull @aa, ptr noundef nonnull @bb, ptr noundef nonnull @cc, float noundef 0.000000e+00) #4
    %inc24 = add nuw nsw i32 %nl.039, 1
    %exitcond46.not = icmp eq i32 %inc24, 20
    br i1 %exitcond46.not, label %for.cond.cleanup, label %vector.ph, !llvm.loop !13
  }
  
  declare i32 @dummy(ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, float noundef) local_unnamed_addr #1
  
  ; Function Attrs: argmemonly nocallback nofree nosync nounwind willreturn
  declare void @llvm.aarch64.neon.st2.v4f32.p0(<4 x float>, <4 x float>, ptr nocapture) #2
  
  ; Function Attrs: nocallback noduplicate nofree nosync nounwind willreturn
  declare i64 @llvm.start.loop.iterations.i64(i64) #3
  
  ; Function Attrs: nocallback noduplicate nofree nosync nounwind willreturn
  declare i64 @llvm.loop.decrement.reg.i64(i64, i64) #3
  
  attributes #0 = { nounwind uwtable vscale_range(1,16) "approx-func-fp-math"="true" "frame-pointer"="non-leaf" "min-legal-vector-width"="0" "no-infs-fp-math"="true" "no-nans-fp-math"="true" "no-signed-zeros-fp-math"="true" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="a64fx" "target-features"="+aes,+crc,+crypto,+fp-armv8,+fullfp16,+lse,+neon,+outline-atomics,+ras,+rdm,+sha2,+sve,+v8.2a" "unsafe-fp-math"="true" }
  attributes #1 = { "approx-func-fp-math"="true" "frame-pointer"="non-leaf" "no-infs-fp-math"="true" "no-nans-fp-math"="true" "no-signed-zeros-fp-math"="true" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="a64fx" "target-features"="+aes,+crc,+crypto,+fp-armv8,+fullfp16,+lse,+neon,+outline-atomics,+ras,+rdm,+sha2,+sve,+v8.2a" "unsafe-fp-math"="true" }
  attributes #2 = { argmemonly nocallback nofree nosync nounwind willreturn }
  attributes #3 = { nocallback noduplicate nofree nosync nounwind willreturn }
  attributes #4 = { nounwind }
  
  !llvm.module.flags = !{!0, !1, !2, !3, !4}
  !llvm.ident = !{!5}
  
  !0 = !{i32 1, !"wchar_size", i32 4}
  !1 = !{i32 7, !"PIC Level", i32 2}
  !2 = !{i32 7, !"PIE Level", i32 2}
  !3 = !{i32 7, !"uwtable", i32 2}
  !4 = !{i32 7, !"frame-pointer", i32 1}
  !5 = !{!"clang version 15.0.4"}
  !6 = !{!7, !7, i64 0}
  !7 = !{!"float", !8, i64 0}
  !8 = !{!"omnipotent char", !9, i64 0}
  !9 = !{!"Simple C/C++ TBAA"}
  !10 = distinct !{!10, !11, !12}
  !11 = !{!"llvm.loop.mustprogress"}
  !12 = !{!"llvm.loop.isvectorized", i32 1}
  !13 = distinct !{!13, !11}

...
---
name:            a10
alignment:       8
exposesReturnsTwice: false
legalized:       false
regBankSelected: false
selected:        false
failedISel:      false
tracksRegLiveness: true
hasWinCFI:       false
callsEHReturn:   false
callsUnwindInit: false
hasEHCatchret:   false
hasEHScopes:     false
hasEHFunclets:   false
failsVerification: false
tracksDebugUserValues: false
registers:
  - { id: 0, class: gpr32sp, preferred-register: '' }
  - { id: 1, class: gpr64all, preferred-register: '' }
  - { id: 2, class: gpr64sp, preferred-register: '' }
  - { id: 3, class: gpr64common, preferred-register: '' }
  - { id: 4, class: gpr64sp, preferred-register: '' }
  - { id: 5, class: gpr64all, preferred-register: '' }
  - { id: 6, class: gpr64all, preferred-register: '' }
  - { id: 7, class: gpr64all, preferred-register: '' }
  - { id: 8, class: gpr32all, preferred-register: '' }
  - { id: 9, class: gpr64, preferred-register: '' }
  - { id: 10, class: gpr32all, preferred-register: '' }
  - { id: 11, class: gpr32all, preferred-register: '' }
  - { id: 12, class: gpr64all, preferred-register: '' }
  - { id: 13, class: gpr64all, preferred-register: '' }
  - { id: 14, class: gpr64all, preferred-register: '' }
  - { id: 15, class: gpr64common, preferred-register: '' }
  - { id: 16, class: gpr32, preferred-register: '' }
  - { id: 17, class: gpr64common, preferred-register: '' }
  - { id: 18, class: gpr64common, preferred-register: '' }
  - { id: 19, class: fpr128, preferred-register: '' }
  - { id: 20, class: fpr128, preferred-register: '' }
  - { id: 21, class: gpr64common, preferred-register: '' }
  - { id: 22, class: gpr64common, preferred-register: '' }
  - { id: 23, class: fpr128, preferred-register: '' }
  - { id: 24, class: fpr128, preferred-register: '' }
  - { id: 25, class: gpr64common, preferred-register: '' }
  - { id: 26, class: gpr64common, preferred-register: '' }
  - { id: 27, class: fpr128, preferred-register: '' }
  - { id: 28, class: fpr128, preferred-register: '' }
  - { id: 29, class: fpr128, preferred-register: '' }
  - { id: 30, class: fpr128, preferred-register: '' }
  - { id: 31, class: fpr128, preferred-register: '' }
  - { id: 32, class: gpr64common, preferred-register: '' }
  - { id: 33, class: gpr64common, preferred-register: '' }
  - { id: 34, class: fpr128, preferred-register: '' }
  - { id: 35, class: fpr128, preferred-register: '' }
  - { id: 36, class: fpr128, preferred-register: '' }
  - { id: 37, class: qq, preferred-register: '' }
  - { id: 38, class: fpr128, preferred-register: '' }
  - { id: 39, class: fpr128, preferred-register: '' }
  - { id: 40, class: fpr128, preferred-register: '' }
  - { id: 41, class: fpr128, preferred-register: '' }
  - { id: 42, class: qq, preferred-register: '' }
  - { id: 43, class: gpr64common, preferred-register: '' }
  - { id: 44, class: gpr64common, preferred-register: '' }
  - { id: 45, class: fpr128, preferred-register: '' }
  - { id: 46, class: fpr128, preferred-register: '' }
  - { id: 47, class: fpr128, preferred-register: '' }
  - { id: 48, class: fpr128, preferred-register: '' }
  - { id: 49, class: fpr128, preferred-register: '' }
  - { id: 50, class: fpr128, preferred-register: '' }
  - { id: 51, class: fpr128, preferred-register: '' }
  - { id: 52, class: fpr128, preferred-register: '' }
  - { id: 53, class: fpr128, preferred-register: '' }
  - { id: 54, class: fpr128, preferred-register: '' }
  - { id: 55, class: fpr128, preferred-register: '' }
  - { id: 56, class: fpr128, preferred-register: '' }
  - { id: 57, class: qq, preferred-register: '' }
  - { id: 58, class: fpr128, preferred-register: '' }
  - { id: 59, class: fpr128, preferred-register: '' }
  - { id: 60, class: fpr128, preferred-register: '' }
  - { id: 61, class: fpr128, preferred-register: '' }
  - { id: 62, class: qq, preferred-register: '' }
  - { id: 63, class: gpr64common, preferred-register: '' }
  - { id: 64, class: gpr64common, preferred-register: '' }
  - { id: 65, class: fpr128, preferred-register: '' }
  - { id: 66, class: fpr128, preferred-register: '' }
  - { id: 67, class: fpr128, preferred-register: '' }
  - { id: 68, class: fpr128, preferred-register: '' }
  - { id: 69, class: fpr128, preferred-register: '' }
  - { id: 70, class: fpr128, preferred-register: '' }
  - { id: 71, class: fpr128, preferred-register: '' }
  - { id: 72, class: fpr128, preferred-register: '' }
  - { id: 73, class: fpr128, preferred-register: '' }
  - { id: 74, class: fpr128, preferred-register: '' }
  - { id: 75, class: fpr128, preferred-register: '' }
  - { id: 76, class: fpr128, preferred-register: '' }
  - { id: 77, class: qq, preferred-register: '' }
  - { id: 78, class: fpr128, preferred-register: '' }
  - { id: 79, class: fpr128, preferred-register: '' }
  - { id: 80, class: fpr128, preferred-register: '' }
  - { id: 81, class: fpr128, preferred-register: '' }
  - { id: 82, class: qq, preferred-register: '' }
  - { id: 83, class: gpr64common, preferred-register: '' }
  - { id: 84, class: gpr64sp, preferred-register: '' }
  - { id: 85, class: fpr128, preferred-register: '' }
  - { id: 86, class: fpr128, preferred-register: '' }
  - { id: 87, class: fpr128, preferred-register: '' }
  - { id: 88, class: fpr128, preferred-register: '' }
  - { id: 89, class: fpr128, preferred-register: '' }
  - { id: 90, class: fpr128, preferred-register: '' }
  - { id: 91, class: fpr128, preferred-register: '' }
  - { id: 92, class: fpr128, preferred-register: '' }
  - { id: 93, class: fpr128, preferred-register: '' }
  - { id: 94, class: fpr128, preferred-register: '' }
  - { id: 95, class: fpr128, preferred-register: '' }
  - { id: 96, class: fpr128, preferred-register: '' }
  - { id: 97, class: qq, preferred-register: '' }
  - { id: 98, class: fpr128, preferred-register: '' }
  - { id: 99, class: fpr128, preferred-register: '' }
  - { id: 100, class: fpr128, preferred-register: '' }
  - { id: 101, class: fpr128, preferred-register: '' }
  - { id: 102, class: qq, preferred-register: '' }
  - { id: 103, class: gpr64sp, preferred-register: '' }
  - { id: 104, class: fpr128, preferred-register: '' }
  - { id: 105, class: fpr128, preferred-register: '' }
  - { id: 106, class: fpr128, preferred-register: '' }
  - { id: 107, class: fpr128, preferred-register: '' }
  - { id: 108, class: fpr128, preferred-register: '' }
  - { id: 109, class: fpr128, preferred-register: '' }
  - { id: 110, class: fpr128, preferred-register: '' }
  - { id: 111, class: fpr128, preferred-register: '' }
  - { id: 112, class: fpr128, preferred-register: '' }
  - { id: 113, class: fpr128, preferred-register: '' }
  - { id: 114, class: fpr128, preferred-register: '' }
  - { id: 115, class: fpr128, preferred-register: '' }
  - { id: 116, class: qq, preferred-register: '' }
  - { id: 117, class: fpr128, preferred-register: '' }
  - { id: 118, class: fpr128, preferred-register: '' }
  - { id: 119, class: fpr128, preferred-register: '' }
  - { id: 120, class: fpr128, preferred-register: '' }
  - { id: 121, class: qq, preferred-register: '' }
  - { id: 122, class: gpr64sp, preferred-register: '' }
  - { id: 123, class: gpr64sp, preferred-register: '' }
  - { id: 124, class: gpr64sp, preferred-register: '' }
  - { id: 125, class: gpr64sp, preferred-register: '' }
  - { id: 126, class: gpr64, preferred-register: '' }
  - { id: 127, class: gpr64common, preferred-register: '' }
  - { id: 128, class: gpr64common, preferred-register: '' }
  - { id: 129, class: gpr64common, preferred-register: '' }
  - { id: 130, class: gpr64common, preferred-register: '' }
  - { id: 131, class: gpr64common, preferred-register: '' }
  - { id: 132, class: gpr64common, preferred-register: '' }
  - { id: 133, class: gpr64common, preferred-register: '' }
  - { id: 134, class: gpr64common, preferred-register: '' }
  - { id: 135, class: fpr32, preferred-register: '' }
  - { id: 136, class: gpr32all, preferred-register: '' }
  - { id: 137, class: gpr32sp, preferred-register: '' }
  - { id: 138, class: gpr32, preferred-register: '' }
  - { id: 139, class: qq, preferred-register: '' }
liveins:         []
frameInfo:
  isFrameAddressTaken: false
  isReturnAddressTaken: false
  hasStackMap:     false
  hasPatchPoint:   false
  stackSize:       0
  offsetAdjustment: 0
  maxAlignment:    1
  adjustsStack:    true
  hasCalls:        true
  stackProtector:  ''
  functionContext: ''
  maxCallFrameSize: 0
  cvBytesOfCalleeSavedRegisters: 0
  hasOpaqueSPAdjustment: false
  hasVAStart:      false
  hasMustTailInVarArgFunc: false
  hasTailCall:     false
  localFrameSize:  0
  savePoint:       ''
  restorePoint:    ''
fixedStack:      []
stack:           []
callSites:       []
debugValueSubstitutions: []
constants:       []
machineFunctionInfo: {}
body:             |
  bb.0.entry:
    successors: %bb.1(0x80000000)
  
    %11:gpr32all = COPY $wzr
    %10:gpr32all = COPY %11
    %15:gpr64common = MOVaddr target-flags(aarch64-page) @a + 160, target-flags(aarch64-pageoff, aarch64-nc) @a + 160
    %16:gpr32 = MOVi32imm 400
    %17:gpr64common = MOVaddr target-flags(aarch64-page) @b, target-flags(aarch64-pageoff, aarch64-nc) @b
    %21:gpr64common = MOVaddr target-flags(aarch64-page) @c, target-flags(aarch64-pageoff, aarch64-nc) @c
    %25:gpr64common = MOVaddr target-flags(aarch64-page) @d, target-flags(aarch64-pageoff, aarch64-nc) @d
    %32:gpr64common = MOVaddr target-flags(aarch64-page) @e, target-flags(aarch64-pageoff, aarch64-nc) @e
    %127:gpr64common = MOVaddr target-flags(aarch64-page) @a, target-flags(aarch64-pageoff, aarch64-nc) @a
    %132:gpr64common = MOVaddr target-flags(aarch64-page) @aa, target-flags(aarch64-pageoff, aarch64-nc) @aa
    %133:gpr64common = MOVaddr target-flags(aarch64-page) @bb, target-flags(aarch64-pageoff, aarch64-nc) @bb
    %134:gpr64common = MOVaddr target-flags(aarch64-page) @cc, target-flags(aarch64-pageoff, aarch64-nc) @cc
    %135:fpr32 = FMOVS0
  
  bb.1.vector.ph:
    successors: %bb.2(0x80000000)
  
    %0:gpr32sp = PHI %10, %bb.0, %8, %bb.4
    %14:gpr64all = COPY $xzr
    %13:gpr64all = COPY %14
    %12:gpr64all = COPY %15
    %1:gpr64all = SUBREG_TO_REG 0, %16, %subreg.sub_32
    %X:gpr64 = COPY $xzr
    %18:gpr64common = ADDXrr %17, %17
    %19:fpr128 = LDRQui %18, 0 :: (load (s128) from %ir.uglygep150, align 32, !tbaa !6)
    %22:gpr64common = ADDXrr %21, %21
    %23:fpr128 = LDRQui %22, 0 :: (load (s128) from %ir.uglygep168, align 32, !tbaa !6)
    %26:gpr64common = ADDXrr %25, %25
    %27:fpr128 = LDRQui %26, 0 :: (load (s128) from %ir.uglygep188, align 32, !tbaa !6)
    %31:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %19, %27, killed %23, implicit $fpcr
    %33:gpr64common = ADDXrr %32, %32
    %34:fpr128 = LDRQui %33, 0 :: (load (s128) from %ir.uglygep123, align 32, !tbaa !6)
    %36:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %19, killed %34, %27, implicit $fpcr
    %37:qq = REG_SEQUENCE %31, %subreg.qsub0, %36, %subreg.qsub1
  
  bb.2.vector.body:
    successors: %bb.2(0x7c000000), %bb.4(0x04000000)
  
    %2:gpr64sp = PHI %12, %bb.1, %6, %bb.2
    %3:gpr64common = PHI %13, %bb.1, %5, %bb.2
    %4:gpr64sp = PHI %1, %bb.1, %7, %bb.2
    %139:qq = PHI %37, %bb.1, %42, %bb.2
    %20:fpr128 = LDRQui %18, 1 :: (load (s128) from %ir.uglygep151, !tbaa !6)
    %24:fpr128 = LDRQui %22, 1 :: (load (s128) from %ir.uglygep169, !tbaa !6)
    %28:fpr128 = LDRQui %26, 1 :: (load (s128) from %ir.uglygep189, !tbaa !6)
    %38:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %20, %28, killed %24, implicit $fpcr
    %39:fpr128 = LDRQui %33, 1 :: (load (s128) from %ir.uglygep133, !tbaa !6)
    %41:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %20, killed %39, %28, implicit $fpcr
    %42:qq = REG_SEQUENCE killed %38, %subreg.qsub0, killed %41, %subreg.qsub1
    %43:gpr64common = SUBXri %2, 160, 0
    %44:gpr64common = SUBXri %2, 128, 0
    ST2Twov4s %37, killed %43 :: (store (s256) into %ir.uglygep139)
    ST2Twov4s %42, %44 :: (store (s256) into %ir.uglygep143)
    ST2Twov4s killed %139, killed %44
    %45:fpr128 = LDRQui %18, 2 :: (load (s128) from %ir.uglygep155, align 32, !tbaa !6)
    %46:fpr128 = LDRQui %18, 3 :: (load (s128) from %ir.uglygep153, !tbaa !6)
    %47:fpr128 = LDRQui %22, 2 :: (load (s128) from %ir.uglygep173, align 32, !tbaa !6)
    %48:fpr128 = LDRQui %22, 3 :: (load (s128) from %ir.uglygep171, !tbaa !6)
    %49:fpr128 = LDRQui %26, 2 :: (load (s128) from %ir.uglygep197, align 32, !tbaa !6)
    %50:fpr128 = LDRQui %26, 3 :: (load (s128) from %ir.uglygep195, !tbaa !6)
    %53:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %45, %49, killed %47, implicit $fpcr
    %54:fpr128 = LDRQui %33, 2 :: (load (s128) from %ir.uglygep135, align 32, !tbaa !6)
    %56:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %45, killed %54, %49, implicit $fpcr
    %57:qq = REG_SEQUENCE killed %53, %subreg.qsub0, killed %56, %subreg.qsub1
    %58:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %46, %50, killed %48, implicit $fpcr
    %59:fpr128 = LDRQui %33, 3 :: (load (s128) from %ir.uglygep136, !tbaa !6)
    %61:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %46, killed %59, %50, implicit $fpcr
    %62:qq = REG_SEQUENCE killed %58, %subreg.qsub0, killed %61, %subreg.qsub1
    %63:gpr64common = SUBXri %2, 96, 0
    %64:gpr64common = SUBXri %2, 64, 0
    ST2Twov4s killed %57, killed %63 :: (store (s256) into %ir.uglygep140)
    ST2Twov4s killed %62, killed %64 :: (store (s256) into %ir.uglygep141)
    %65:fpr128 = LDRQui %18, 4 :: (load (s128) from %ir.uglygep159, align 32, !tbaa !6)
    %66:fpr128 = LDRQui %18, 5 :: (load (s128) from %ir.uglygep157, !tbaa !6)
    %67:fpr128 = LDRQui %22, 4 :: (load (s128) from %ir.uglygep177, align 32, !tbaa !6)
    %68:fpr128 = LDRQui %22, 5 :: (load (s128) from %ir.uglygep175, !tbaa !6)
    %69:fpr128 = LDRQui %26, 4 :: (load (s128) from %ir.uglygep200, align 32, !tbaa !6)
    %70:fpr128 = LDRQui %26, 5 :: (load (s128) from %ir.uglygep184, !tbaa !6)
    %73:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %65, %69, killed %67, implicit $fpcr
    %74:fpr128 = LDRQui %33, 4 :: (load (s128) from %ir.uglygep131, align 32, !tbaa !6)
    %76:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %65, killed %74, %69, implicit $fpcr
    %77:qq = REG_SEQUENCE killed %73, %subreg.qsub0, killed %76, %subreg.qsub1
    %78:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %66, %70, killed %68, implicit $fpcr
    %79:fpr128 = LDRQui %33, 5 :: (load (s128) from %ir.uglygep132, !tbaa !6)
    %81:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %66, killed %79, %70, implicit $fpcr
    %82:qq = REG_SEQUENCE killed %78, %subreg.qsub0, killed %81, %subreg.qsub1
    %83:gpr64common = SUBXri %2, 32, 0
    ST2Twov4s killed %77, killed %83 :: (store (s256) into %ir.uglygep144)
    %84:gpr64sp = ST2Twov4s_POST %82, %2, $xzr
    %A1:gpr64sp = ST2Twov4s_POST killed %82, %2, %X
    %85:fpr128 = LDRQui %18, 6 :: (load (s128) from %ir.uglygep163, align 32, !tbaa !6)
    %86:fpr128 = LDRQui %18, 7 :: (load (s128) from %ir.uglygep161, !tbaa !6)
    %87:fpr128 = LDRQui %22, 6 :: (load (s128) from %ir.uglygep181, align 32, !tbaa !6)
    %88:fpr128 = LDRQui %22, 7 :: (load (s128) from %ir.uglygep179, !tbaa !6)
    %89:fpr128 = LDRQui %26, 6 :: (load (s128) from %ir.uglygep193, align 32, !tbaa !6)
    %90:fpr128 = LDRQui %26, 7 :: (load (s128) from %ir.uglygep198, !tbaa !6)
    %93:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %85, %89, killed %87, implicit $fpcr
    %94:fpr128 = LDRQui %33, 6 :: (load (s128) from %ir.uglygep128, align 32, !tbaa !6)
    %96:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %85, killed %94, %89, implicit $fpcr
    %97:qq = REG_SEQUENCE killed %93, %subreg.qsub0, killed %96, %subreg.qsub1
    %98:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %86, %90, killed %88, implicit $fpcr
    %99:fpr128 = LDRQui %33, 7 :: (load (s128) from %ir.uglygep129, !tbaa !6)
    %101:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %86, killed %99, %90, implicit $fpcr
    %102:qq = REG_SEQUENCE killed %98, %subreg.qsub0, killed %101, %subreg.qsub1
    %103:gpr64sp = ADDXri %2, 64, 0
    ST2Twov4s killed %97, killed %84 :: (store (s256) into %ir.uglygep145)
    ST2Twov4s killed %102, killed %103 :: (store (s256) into %ir.uglygep146)
    %104:fpr128 = LDRQui %18, 8 :: (load (s128) from %ir.uglygep165, align 32, !tbaa !6)
    %105:fpr128 = LDRQui %18, 9 :: (load (s128) from %ir.uglygep148, !tbaa !6)
    %106:fpr128 = LDRQui %22, 8 :: (load (s128) from %ir.uglygep183, align 32, !tbaa !6)
    %107:fpr128 = LDRQui %22, 9 :: (load (s128) from %ir.uglygep166, !tbaa !6)
    %108:fpr128 = LDRQui %26, 8 :: (load (s128) from %ir.uglygep191, align 32, !tbaa !6)
    %109:fpr128 = LDRQui %26, 9 :: (load (s128) from %ir.uglygep186, !tbaa !6)
    %112:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %104, %108, killed %106, implicit $fpcr
    %113:fpr128 = LDRQui %33, 8 :: (load (s128) from %ir.uglygep125, align 32, !tbaa !6)
    %115:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %104, killed %113, %108, implicit $fpcr
    %116:qq = REG_SEQUENCE killed %112, %subreg.qsub0, killed %115, %subreg.qsub1
    %117:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %105, %109, killed %107, implicit $fpcr
    %118:fpr128 = LDRQui %33, 9 :: (load (s128) from %ir.uglygep126, !tbaa !6)
    %120:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMLAv4f32 %105, killed %118, %109, implicit $fpcr
    %121:qq = REG_SEQUENCE killed %117, %subreg.qsub0, killed %120, %subreg.qsub1
    %122:gpr64sp = ADDXri %2, 96, 0
    %123:gpr64sp = ADDXri %2, 128, 0
    ST2Twov4s killed %116, killed %122 :: (store (s256) into %ir.uglygep147)
    ST2Twov4s killed %121, killed %123 :: (store (s256) into %ir.uglygep142)
    %124:gpr64sp = nuw nsw ADDXri %3, 160, 0
    %5:gpr64all = COPY %124
    %125:gpr64sp = ADDXri %2, 320, 0
    %6:gpr64all = COPY %125
    %126:gpr64 = SUBSXri %4, 1, 0, implicit-def $nzcv
    %7:gpr64all = COPY %126
    Bcc 1, %bb.2, implicit $nzcv
    B %bb.4
  
  bb.3.for.cond.cleanup:
    RET_ReallyLR
  
  bb.4.for.cond.cleanup3:
    successors: %bb.3(0x04000000), %bb.1(0x7c000000)
  
    ADJCALLSTACKDOWN 0, 0, implicit-def dead $sp, implicit $sp
    $x0 = COPY %127
    $x1 = COPY %17
    $x2 = COPY %21
    $x3 = COPY %25
    $x4 = COPY %32
    $x5 = COPY %132
    $x6 = COPY %133
    $x7 = COPY %134
    $s0 = COPY %135
    BL @dummy, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit $x1, implicit $x2, implicit $x3, implicit $x4, implicit $x5, implicit $x6, implicit $x7, implicit $s0, implicit-def $sp, implicit-def $w0
    ADJCALLSTACKUP 0, 0, implicit-def dead $sp, implicit $sp
    %137:gpr32sp = nuw nsw ADDWri %0, 1, 0
    %8:gpr32all = COPY %137
    dead $wzr = SUBSWri %137, 20, 0, implicit-def $nzcv
    Bcc 0, %bb.3, implicit $nzcv
    B %bb.1

...
