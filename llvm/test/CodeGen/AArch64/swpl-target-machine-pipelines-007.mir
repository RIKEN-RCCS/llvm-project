#RUN: llc %s -mcpu=a64fx -ffj-swp -O1 -swpl-debug-dump-resource-filter="FADDV_VPZ_S" -start-before=aarch64-swpipeliner -o /dev/null 2>&1 | FileCheck %s

#CHECK:DBG(AArch64SwplTargetMachine::getPipelines): MI: %173:zpr = FADDV_VPZ_S %33:ppr_3b, killed %169:zpr
#CHECK-NEXT:  ResourceID: SIMDFP_SVE_OP+16
#CHECK-NEXT:  latency: 34
#CHECK-NEXT:  seqDecode: true
#CHECK-NEXT:  stage/resource(): 0/FLA, 4/FLA, 10/FLA, 19/FLA, 25/FLA
#CHECK-NEXT:  stage/resource(): 0/FLA, 4/FLA, 10/FLA, 19/FLA, 25/FLB
#CHECK-NEXT:  stage/resource(): 0/FLA, 4/FLA, 10/FLB, 19/FLA, 25/FLA
#CHECK-NEXT:  stage/resource(): 0/FLA, 4/FLA, 10/FLB, 19/FLA, 25/FLB
#CHECK-NEXT:  stage/resource(): 0/FLB, 4/FLA, 10/FLA, 19/FLA, 25/FLA
#CHECK-NEXT:  stage/resource(): 0/FLB, 4/FLA, 10/FLA, 19/FLA, 25/FLB
#CHECK-NEXT:  stage/resource(): 0/FLB, 4/FLA, 10/FLB, 19/FLA, 25/FLA
#CHECK-NEXT:  stage/resource(): 0/FLB, 4/FLA, 10/FLB, 19/FLA, 25/FLB

--- |
  ; ModuleID = '/TSVC_2/src_sep_optnone/s352.c'
  source_filename = "/TSVC_2/src_sep_optnone/s352.c"
  target datalayout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128"
  target triple = "aarch64-unknown-linux-gnu"
  
  %struct.args_t = type { %struct.timeval, %struct.timeval, ptr }
  %struct.timeval = type { i64, i64 }
  
  @__func__.s352 = private unnamed_addr constant [5 x i8] c"s352\00", align 1
  @a = external global [32000 x float], align 64
  @b = external global [32000 x float], align 64
  @c = external global [32000 x float], align 64
  @d = external global [32000 x float], align 64
  @e = external global [32000 x float], align 64
  @aa = external global [256 x [256 x float]], align 64
  @bb = external global [256 x [256 x float]], align 64
  @cc = external global [256 x [256 x float]], align 64
  
  ; Function Attrs: nounwind uwtable vscale_range(1,1)
  define dso_local float @s352(ptr nocapture noundef %func_args) local_unnamed_addr #0 {
  entry:
    %call = tail call i32 @initialise_arrays(ptr noundef nonnull @__func__.s352) #5
    %call1 = tail call i32 @gettimeofday(ptr noundef %func_args, ptr noundef null) #5
    br label %for.cond2.preheader
  
  for.cond2.preheader:                              ; preds = %entry, %for.cond.cleanup4
    %nl.062 = phi i32 [ 0, %entry ], [ %inc, %for.cond.cleanup4 ]
    %0 = call i64 @llvm.start.loop.iterations.i64(i64 640)
    br label %for.body5
  
  for.cond.cleanup:                                 ; preds = %for.cond.cleanup4
    %t2 = getelementptr inbounds %struct.args_t, ptr %func_args, i64 0, i32 1
    %call44 = tail call i32 @gettimeofday(ptr noundef nonnull %t2, ptr noundef null) #5
    ret float %61
  
  for.cond.cleanup4:                                ; preds = %for.body5
    %call41 = tail call i32 @dummy(ptr noundef nonnull @a, ptr noundef nonnull @b, ptr noundef nonnull @c, ptr noundef nonnull @d, ptr noundef nonnull @e, ptr noundef nonnull @aa, ptr noundef nonnull @bb, ptr noundef nonnull @cc, float noundef %61) #5
    %inc = add nuw nsw i32 %nl.062, 1
    %exitcond.not = icmp eq i32 %inc, 800000
    br i1 %exitcond.not, label %for.cond.cleanup, label %for.cond2.preheader, !llvm.loop !6
  
  for.body5:                                        ; preds = %for.body5, %for.cond2.preheader
    %lsr.iv = phi i64 [ %lsr.iv.next, %for.body5 ], [ 0, %for.cond2.preheader ]
    %dot.160 = phi float [ 0.000000e+00, %for.cond2.preheader ], [ %61, %for.body5 ]
    %1 = phi i64 [ %0, %for.cond2.preheader ], [ %62, %for.body5 ]
    %uglygep186 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep225 = getelementptr i8, ptr @b, i64 %lsr.iv
    %2 = load <4 x float>, ptr %uglygep186, align 8, !tbaa !8
    %3 = load <4 x float>, ptr %uglygep225, align 8, !tbaa !8
    %4 = fmul fast <4 x float> %3, %2
    %uglygep205 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep206 = getelementptr i8, ptr %uglygep205, i64 16
    %5 = load float, ptr %uglygep206, align 8, !tbaa !8
    %uglygep170 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep171 = getelementptr i8, ptr %uglygep170, i64 16
    %6 = load float, ptr %uglygep171, align 8, !tbaa !8
    %mul38 = fmul fast float %6, %5
    %op.rdx = fadd fast float %mul38, %dot.160
    %7 = tail call fast float @llvm.vector.reduce.fadd.v4f32(float %op.rdx, <4 x float> %4)
    %uglygep187 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep188 = getelementptr i8, ptr %uglygep187, i64 20
    %uglygep232 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep233 = getelementptr i8, ptr %uglygep232, i64 20
    %8 = load <4 x float>, ptr %uglygep188, align 4, !tbaa !8
    %9 = load <4 x float>, ptr %uglygep233, align 4, !tbaa !8
    %10 = fmul fast <4 x float> %9, %8
    %uglygep207 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep208 = getelementptr i8, ptr %uglygep207, i64 36
    %11 = load float, ptr %uglygep208, align 4, !tbaa !8
    %uglygep172 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep173 = getelementptr i8, ptr %uglygep172, i64 36
    %12 = load float, ptr %uglygep173, align 4, !tbaa !8
    %mul38.1 = fmul fast float %12, %11
    %op.rdx.1 = fadd fast float %mul38.1, %7
    %13 = tail call fast float @llvm.vector.reduce.fadd.v4f32(float %op.rdx.1, <4 x float> %10)
    %uglygep189 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep190 = getelementptr i8, ptr %uglygep189, i64 40
    %uglygep234 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep235 = getelementptr i8, ptr %uglygep234, i64 40
    %14 = load <4 x float>, ptr %uglygep190, align 8, !tbaa !8
    %15 = load <4 x float>, ptr %uglygep235, align 8, !tbaa !8
    %16 = fmul fast <4 x float> %15, %14
    %uglygep209 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep210 = getelementptr i8, ptr %uglygep209, i64 56
    %17 = load float, ptr %uglygep210, align 8, !tbaa !8
    %uglygep174 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep175 = getelementptr i8, ptr %uglygep174, i64 56
    %18 = load float, ptr %uglygep175, align 8, !tbaa !8
    %mul38.2 = fmul fast float %18, %17
    %op.rdx.2 = fadd fast float %mul38.2, %13
    %19 = tail call fast float @llvm.vector.reduce.fadd.v4f32(float %op.rdx.2, <4 x float> %16)
    %uglygep191 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep192 = getelementptr i8, ptr %uglygep191, i64 60
    %uglygep236 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep237 = getelementptr i8, ptr %uglygep236, i64 60
    %20 = load <4 x float>, ptr %uglygep192, align 4, !tbaa !8
    %21 = load <4 x float>, ptr %uglygep237, align 4, !tbaa !8
    %22 = fmul fast <4 x float> %21, %20
    %uglygep211 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep212 = getelementptr i8, ptr %uglygep211, i64 76
    %23 = load float, ptr %uglygep212, align 4, !tbaa !8
    %uglygep176 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep177 = getelementptr i8, ptr %uglygep176, i64 76
    %24 = load float, ptr %uglygep177, align 4, !tbaa !8
    %mul38.3 = fmul fast float %24, %23
    %op.rdx.3 = fadd fast float %mul38.3, %19
    %25 = tail call fast float @llvm.vector.reduce.fadd.v4f32(float %op.rdx.3, <4 x float> %22)
    %uglygep193 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep194 = getelementptr i8, ptr %uglygep193, i64 80
    %uglygep238 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep239 = getelementptr i8, ptr %uglygep238, i64 80
    %26 = load <4 x float>, ptr %uglygep194, align 8, !tbaa !8
    %27 = load <4 x float>, ptr %uglygep239, align 8, !tbaa !8
    %28 = fmul fast <4 x float> %27, %26
    %uglygep213 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep214 = getelementptr i8, ptr %uglygep213, i64 96
    %29 = load float, ptr %uglygep214, align 8, !tbaa !8
    %uglygep178 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep179 = getelementptr i8, ptr %uglygep178, i64 96
    %30 = load float, ptr %uglygep179, align 8, !tbaa !8
    %mul38.4 = fmul fast float %30, %29
    %op.rdx.4 = fadd fast float %mul38.4, %25
    %31 = tail call fast float @llvm.vector.reduce.fadd.v4f32(float %op.rdx.4, <4 x float> %28)
    %uglygep184 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep185 = getelementptr i8, ptr %uglygep184, i64 100
    %uglygep223 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep224 = getelementptr i8, ptr %uglygep223, i64 100
    %32 = load <4 x float>, ptr %uglygep185, align 4, !tbaa !8
    %33 = load <4 x float>, ptr %uglygep224, align 4, !tbaa !8
    %34 = fmul fast <4 x float> %33, %32
    %uglygep215 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep216 = getelementptr i8, ptr %uglygep215, i64 116
    %35 = load float, ptr %uglygep216, align 4, !tbaa !8
    %uglygep180 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep181 = getelementptr i8, ptr %uglygep180, i64 116
    %36 = load float, ptr %uglygep181, align 4, !tbaa !8
    %mul38.5 = fmul fast float %36, %35
    %op.rdx.5 = fadd fast float %mul38.5, %31
    %37 = tail call fast float @llvm.vector.reduce.fadd.v4f32(float %op.rdx.5, <4 x float> %34)
    %uglygep195 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep196 = getelementptr i8, ptr %uglygep195, i64 120
    %uglygep240 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep241 = getelementptr i8, ptr %uglygep240, i64 120
    %38 = load <4 x float>, ptr %uglygep196, align 8, !tbaa !8
    %39 = load <4 x float>, ptr %uglygep241, align 8, !tbaa !8
    %40 = fmul fast <4 x float> %39, %38
    %uglygep217 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep218 = getelementptr i8, ptr %uglygep217, i64 136
    %41 = load float, ptr %uglygep218, align 8, !tbaa !8
    %uglygep168 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep169 = getelementptr i8, ptr %uglygep168, i64 136
    %42 = load float, ptr %uglygep169, align 8, !tbaa !8
    %mul38.6 = fmul fast float %42, %41
    %op.rdx.6 = fadd fast float %mul38.6, %37
    %43 = tail call fast float @llvm.vector.reduce.fadd.v4f32(float %op.rdx.6, <4 x float> %40)
    %uglygep197 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep198 = getelementptr i8, ptr %uglygep197, i64 140
    %uglygep230 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep231 = getelementptr i8, ptr %uglygep230, i64 140
    %44 = load <4 x float>, ptr %uglygep198, align 4, !tbaa !8
    %45 = load <4 x float>, ptr %uglygep231, align 4, !tbaa !8
    %46 = fmul fast <4 x float> %45, %44
    %uglygep219 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep220 = getelementptr i8, ptr %uglygep219, i64 156
    %47 = load float, ptr %uglygep220, align 4, !tbaa !8
    %uglygep166 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep167 = getelementptr i8, ptr %uglygep166, i64 156
    %48 = load float, ptr %uglygep167, align 4, !tbaa !8
    %mul38.7 = fmul fast float %48, %47
    %op.rdx.7 = fadd fast float %mul38.7, %43
    %49 = tail call fast float @llvm.vector.reduce.fadd.v4f32(float %op.rdx.7, <4 x float> %46)
    %uglygep199 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep200 = getelementptr i8, ptr %uglygep199, i64 160
    %uglygep228 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep229 = getelementptr i8, ptr %uglygep228, i64 160
    %50 = load <4 x float>, ptr %uglygep200, align 8, !tbaa !8
    %51 = load <4 x float>, ptr %uglygep229, align 8, !tbaa !8
    %52 = fmul fast <4 x float> %51, %50
    %uglygep221 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep222 = getelementptr i8, ptr %uglygep221, i64 176
    %53 = load float, ptr %uglygep222, align 8, !tbaa !8
    %uglygep164 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep165 = getelementptr i8, ptr %uglygep164, i64 176
    %54 = load float, ptr %uglygep165, align 8, !tbaa !8
    %mul38.8 = fmul fast float %54, %53
    %op.rdx.8 = fadd fast float %mul38.8, %49
    %55 = tail call fast float @llvm.vector.reduce.fadd.v4f32(float %op.rdx.8, <4 x float> %52)
    %uglygep201 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep202 = getelementptr i8, ptr %uglygep201, i64 180
    %uglygep226 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep227 = getelementptr i8, ptr %uglygep226, i64 180
    %56 = load <4 x float>, ptr %uglygep202, align 4, !tbaa !8
    %57 = load <4 x float>, ptr %uglygep227, align 4, !tbaa !8
    %58 = fmul fast <4 x float> %57, %56
    %uglygep203 = getelementptr i8, ptr @a, i64 %lsr.iv
    %uglygep204 = getelementptr i8, ptr %uglygep203, i64 196
    %59 = load float, ptr %uglygep204, align 4, !tbaa !8
    %uglygep162 = getelementptr i8, ptr @b, i64 %lsr.iv
    %uglygep163 = getelementptr i8, ptr %uglygep162, i64 196
    %60 = load float, ptr %uglygep163, align 4, !tbaa !8
    %mul38.9 = fmul fast float %60, %59
    %op.rdx.9 = fadd fast float %mul38.9, %55
    %61 = tail call fast float @llvm.vector.reduce.fadd.v4f32(float %op.rdx.9, <4 x float> %58)
    %lsr.iv.next = add nuw nsw i64 %lsr.iv, 200
    %62 = call i64 @llvm.loop.decrement.reg.i64(i64 %1, i64 1)
    %63 = icmp ne i64 %62, 0
    br i1 %63, label %for.body5, label %for.cond.cleanup4, !llvm.loop !12
  }
  
  declare i32 @initialise_arrays(ptr noundef) local_unnamed_addr #1
  
  ; Function Attrs: nofree nounwind
  declare noundef i32 @gettimeofday(ptr nocapture noundef, ptr nocapture noundef) local_unnamed_addr #2
  
  declare i32 @dummy(ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, ptr noundef, float noundef) local_unnamed_addr #1
  
  ; Function Attrs: nocallback nofree nosync nounwind readnone willreturn
  declare float @llvm.vector.reduce.fadd.v4f32(float, <4 x float>) #3
  
  ; Function Attrs: nocallback noduplicate nofree nosync nounwind willreturn
  declare i64 @llvm.start.loop.iterations.i64(i64) #4
  
  ; Function Attrs: nocallback noduplicate nofree nosync nounwind willreturn
  declare i64 @llvm.loop.decrement.reg.i64(i64, i64) #4
  
  attributes #0 = { nounwind uwtable vscale_range(1,1) "approx-func-fp-math"="true" "frame-pointer"="non-leaf" "min-legal-vector-width"="0" "no-infs-fp-math"="true" "no-nans-fp-math"="true" "no-signed-zeros-fp-math"="true" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="a64fx" "target-features"="+aes,+crc,+crypto,+fp-armv8,+fullfp16,+lse,+neon,+outline-atomics,+ras,+rdm,+sha2,+sve,+v8.2a" "unsafe-fp-math"="true" }
  attributes #1 = { "approx-func-fp-math"="true" "frame-pointer"="non-leaf" "no-infs-fp-math"="true" "no-nans-fp-math"="true" "no-signed-zeros-fp-math"="true" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="a64fx" "target-features"="+aes,+crc,+crypto,+fp-armv8,+fullfp16,+lse,+neon,+outline-atomics,+ras,+rdm,+sha2,+sve,+v8.2a" "unsafe-fp-math"="true" }
  attributes #2 = { nofree nounwind "approx-func-fp-math"="true" "frame-pointer"="non-leaf" "no-infs-fp-math"="true" "no-nans-fp-math"="true" "no-signed-zeros-fp-math"="true" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="a64fx" "target-features"="+aes,+crc,+crypto,+fp-armv8,+fullfp16,+lse,+neon,+outline-atomics,+ras,+rdm,+sha2,+sve,+v8.2a" "unsafe-fp-math"="true" }
  attributes #3 = { nocallback nofree nosync nounwind readnone willreturn }
  attributes #4 = { nocallback noduplicate nofree nosync nounwind willreturn }
  attributes #5 = { nounwind }
  
  !llvm.module.flags = !{!0, !1, !2, !3, !4}
  
  !0 = !{i32 1, !"wchar_size", i32 4}
  !1 = !{i32 7, !"PIC Level", i32 2}
  !2 = !{i32 7, !"PIE Level", i32 2}
  !3 = !{i32 7, !"uwtable", i32 2}
  !4 = !{i32 7, !"frame-pointer", i32 1}
  !6 = distinct !{!6, !7}
  !7 = !{!"llvm.loop.mustprogress"}
  !8 = !{!9, !9, i64 0}
  !9 = !{!"float", !10, i64 0}
  !10 = !{!"omnipotent char", !11, i64 0}
  !11 = !{!"Simple C/C++ TBAA"}
  !12 = distinct !{!12, !7}

...
---
name:            s352
alignment:       8
exposesReturnsTwice: false
legalized:       false
regBankSelected: false
selected:        false
failedISel:      false
tracksRegLiveness: true
hasWinCFI:       false
callsEHReturn:   false
callsUnwindInit: false
hasEHCatchret:   false
hasEHScopes:     false
hasEHFunclets:   false
failsVerification: false
tracksDebugUserValues: false
registers:
  - { id: 0, class: gpr32sp, preferred-register: '' }
  - { id: 1, class: gpr64all, preferred-register: '' }
  - { id: 2, class: gpr32all, preferred-register: '' }
  - { id: 3, class: gpr64common, preferred-register: '' }
  - { id: 4, class: fpr32, preferred-register: '' }
  - { id: 5, class: gpr64sp, preferred-register: '' }
  - { id: 6, class: fpr32, preferred-register: '' }
  - { id: 7, class: gpr64all, preferred-register: '' }
  - { id: 8, class: gpr64all, preferred-register: '' }
  - { id: 9, class: gpr64common, preferred-register: '' }
  - { id: 10, class: gpr32all, preferred-register: '' }
  - { id: 11, class: gpr64common, preferred-register: '' }
  - { id: 12, class: gpr32all, preferred-register: '' }
  - { id: 13, class: gpr64all, preferred-register: '' }
  - { id: 14, class: gpr32all, preferred-register: '' }
  - { id: 15, class: gpr32all, preferred-register: '' }
  - { id: 16, class: gpr64all, preferred-register: '' }
  - { id: 17, class: fpr32, preferred-register: '' }
  - { id: 18, class: gpr64all, preferred-register: '' }
  - { id: 19, class: gpr32, preferred-register: '' }
  - { id: 20, class: gpr64common, preferred-register: '' }
  - { id: 21, class: gpr64common, preferred-register: '' }
  - { id: 22, class: gpr64common, preferred-register: '' }
  - { id: 23, class: gpr64common, preferred-register: '' }
  - { id: 24, class: fpr128, preferred-register: '' }
  - { id: 25, class: fpr128, preferred-register: '' }
  - { id: 26, class: fpr128, preferred-register: '' }
  - { id: 27, class: zpr, preferred-register: '' }
  - { id: 28, class: zpr, preferred-register: '' }
  - { id: 29, class: fpr32, preferred-register: '' }
  - { id: 30, class: fpr32, preferred-register: '' }
  - { id: 31, class: fpr32, preferred-register: '' }
  - { id: 32, class: fpr32, preferred-register: '' }
  - { id: 33, class: ppr_3b, preferred-register: '' }
  - { id: 34, class: zpr, preferred-register: '' }
  - { id: 35, class: fpr32, preferred-register: '' }
  - { id: 36, class: fpr32, preferred-register: '' }
  - { id: 37, class: fpr128, preferred-register: '' }
  - { id: 38, class: fpr128, preferred-register: '' }
  - { id: 39, class: fpr128, preferred-register: '' }
  - { id: 40, class: zpr, preferred-register: '' }
  - { id: 41, class: zpr, preferred-register: '' }
  - { id: 42, class: fpr32, preferred-register: '' }
  - { id: 43, class: fpr32, preferred-register: '' }
  - { id: 44, class: fpr32, preferred-register: '' }
  - { id: 45, class: fpr32, preferred-register: '' }
  - { id: 46, class: zpr, preferred-register: '' }
  - { id: 47, class: fpr32, preferred-register: '' }
  - { id: 48, class: fpr32, preferred-register: '' }
  - { id: 49, class: fpr128, preferred-register: '' }
  - { id: 50, class: fpr128, preferred-register: '' }
  - { id: 51, class: fpr128, preferred-register: '' }
  - { id: 52, class: zpr, preferred-register: '' }
  - { id: 53, class: zpr, preferred-register: '' }
  - { id: 54, class: fpr32, preferred-register: '' }
  - { id: 55, class: fpr32, preferred-register: '' }
  - { id: 56, class: fpr32, preferred-register: '' }
  - { id: 57, class: fpr32, preferred-register: '' }
  - { id: 58, class: zpr, preferred-register: '' }
  - { id: 59, class: fpr32, preferred-register: '' }
  - { id: 60, class: fpr32, preferred-register: '' }
  - { id: 61, class: fpr128, preferred-register: '' }
  - { id: 62, class: fpr128, preferred-register: '' }
  - { id: 63, class: fpr128, preferred-register: '' }
  - { id: 64, class: zpr, preferred-register: '' }
  - { id: 65, class: zpr, preferred-register: '' }
  - { id: 66, class: fpr32, preferred-register: '' }
  - { id: 67, class: fpr32, preferred-register: '' }
  - { id: 68, class: fpr32, preferred-register: '' }
  - { id: 69, class: fpr32, preferred-register: '' }
  - { id: 70, class: zpr, preferred-register: '' }
  - { id: 71, class: fpr32, preferred-register: '' }
  - { id: 72, class: fpr32, preferred-register: '' }
  - { id: 73, class: fpr128, preferred-register: '' }
  - { id: 74, class: fpr128, preferred-register: '' }
  - { id: 75, class: fpr128, preferred-register: '' }
  - { id: 76, class: zpr, preferred-register: '' }
  - { id: 77, class: zpr, preferred-register: '' }
  - { id: 78, class: fpr32, preferred-register: '' }
  - { id: 79, class: fpr32, preferred-register: '' }
  - { id: 80, class: fpr32, preferred-register: '' }
  - { id: 81, class: fpr32, preferred-register: '' }
  - { id: 82, class: zpr, preferred-register: '' }
  - { id: 83, class: fpr32, preferred-register: '' }
  - { id: 84, class: fpr32, preferred-register: '' }
  - { id: 85, class: fpr128, preferred-register: '' }
  - { id: 86, class: fpr128, preferred-register: '' }
  - { id: 87, class: fpr128, preferred-register: '' }
  - { id: 88, class: zpr, preferred-register: '' }
  - { id: 89, class: zpr, preferred-register: '' }
  - { id: 90, class: fpr32, preferred-register: '' }
  - { id: 91, class: fpr32, preferred-register: '' }
  - { id: 92, class: fpr32, preferred-register: '' }
  - { id: 93, class: fpr32, preferred-register: '' }
  - { id: 94, class: zpr, preferred-register: '' }
  - { id: 95, class: fpr32, preferred-register: '' }
  - { id: 96, class: fpr32, preferred-register: '' }
  - { id: 97, class: fpr128, preferred-register: '' }
  - { id: 98, class: fpr128, preferred-register: '' }
  - { id: 99, class: fpr128, preferred-register: '' }
  - { id: 100, class: zpr, preferred-register: '' }
  - { id: 101, class: zpr, preferred-register: '' }
  - { id: 102, class: fpr32, preferred-register: '' }
  - { id: 103, class: fpr32, preferred-register: '' }
  - { id: 104, class: fpr32, preferred-register: '' }
  - { id: 105, class: fpr32, preferred-register: '' }
  - { id: 106, class: zpr, preferred-register: '' }
  - { id: 107, class: fpr32, preferred-register: '' }
  - { id: 108, class: fpr32, preferred-register: '' }
  - { id: 109, class: fpr128, preferred-register: '' }
  - { id: 110, class: fpr128, preferred-register: '' }
  - { id: 111, class: fpr128, preferred-register: '' }
  - { id: 112, class: zpr, preferred-register: '' }
  - { id: 113, class: zpr, preferred-register: '' }
  - { id: 114, class: fpr32, preferred-register: '' }
  - { id: 115, class: fpr32, preferred-register: '' }
  - { id: 116, class: fpr32, preferred-register: '' }
  - { id: 117, class: fpr32, preferred-register: '' }
  - { id: 118, class: zpr, preferred-register: '' }
  - { id: 119, class: fpr32, preferred-register: '' }
  - { id: 120, class: fpr32, preferred-register: '' }
  - { id: 121, class: fpr128, preferred-register: '' }
  - { id: 122, class: fpr128, preferred-register: '' }
  - { id: 123, class: fpr128, preferred-register: '' }
  - { id: 124, class: zpr, preferred-register: '' }
  - { id: 125, class: zpr, preferred-register: '' }
  - { id: 126, class: fpr32, preferred-register: '' }
  - { id: 127, class: fpr32, preferred-register: '' }
  - { id: 128, class: fpr32, preferred-register: '' }
  - { id: 129, class: fpr32, preferred-register: '' }
  - { id: 130, class: zpr, preferred-register: '' }
  - { id: 131, class: fpr32, preferred-register: '' }
  - { id: 132, class: fpr32, preferred-register: '' }
  - { id: 133, class: fpr128, preferred-register: '' }
  - { id: 134, class: fpr128, preferred-register: '' }
  - { id: 135, class: fpr128, preferred-register: '' }
  - { id: 136, class: zpr, preferred-register: '' }
  - { id: 137, class: zpr, preferred-register: '' }
  - { id: 138, class: fpr32, preferred-register: '' }
  - { id: 139, class: fpr32, preferred-register: '' }
  - { id: 140, class: fpr32, preferred-register: '' }
  - { id: 141, class: fpr32, preferred-register: '' }
  - { id: 142, class: zpr, preferred-register: '' }
  - { id: 143, class: fpr32, preferred-register: '' }
  - { id: 144, class: gpr64sp, preferred-register: '' }
  - { id: 145, class: gpr64, preferred-register: '' }
  - { id: 146, class: gpr64common, preferred-register: '' }
  - { id: 147, class: gpr64common, preferred-register: '' }
  - { id: 148, class: gpr64common, preferred-register: '' }
  - { id: 149, class: gpr64common, preferred-register: '' }
  - { id: 150, class: gpr64common, preferred-register: '' }
  - { id: 151, class: gpr64common, preferred-register: '' }
  - { id: 152, class: gpr64common, preferred-register: '' }
  - { id: 153, class: gpr64common, preferred-register: '' }
  - { id: 154, class: gpr32all, preferred-register: '' }
  - { id: 155, class: gpr32common, preferred-register: '' }
  - { id: 156, class: gpr32, preferred-register: '' }
  - { id: 157, class: gpr32, preferred-register: '' }
  - { id: 158, class: gpr64sp, preferred-register: '' }
  - { id: 159, class: gpr64all, preferred-register: '' }
  - { id: 160, class: gpr32all, preferred-register: '' }
liveins:
  - { reg: '$x0', virtual-reg: '%9' }
frameInfo:
  isFrameAddressTaken: false
  isReturnAddressTaken: false
  hasStackMap:     false
  hasPatchPoint:   false
  stackSize:       0
  offsetAdjustment: 0
  maxAlignment:    1
  adjustsStack:    true
  hasCalls:        true
  stackProtector:  ''
  functionContext: ''
  maxCallFrameSize: 0
  cvBytesOfCalleeSavedRegisters: 0
  hasOpaqueSPAdjustment: false
  hasVAStart:      false
  hasMustTailInVarArgFunc: false
  hasTailCall:     false
  localFrameSize:  0
  savePoint:       ''
  restorePoint:    ''
fixedStack:      []
stack:           []
callSites:       []
debugValueSubstitutions: []
constants:       []
machineFunctionInfo: {}
body:             |
  bb.0.entry:
    successors: %bb.1(0x80000000)
    liveins: $x0
  
    %9:gpr64common = COPY $x0
    ADJCALLSTACKDOWN 0, 0, implicit-def dead $sp, implicit $sp
    %11:gpr64common = MOVaddr target-flags(aarch64-page) @__func__.s352, target-flags(aarch64-pageoff, aarch64-nc) @__func__.s352
    $x0 = COPY %11
    BL @initialise_arrays, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit-def $sp, implicit-def $w0
    ADJCALLSTACKUP 0, 0, implicit-def dead $sp, implicit $sp
    ADJCALLSTACKDOWN 0, 0, implicit-def dead $sp, implicit $sp
    %13:gpr64all = COPY $xzr
    $x0 = COPY %9
    $x1 = COPY %13
    BL @gettimeofday, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit $x1, implicit-def $sp, implicit-def $w0
    ADJCALLSTACKUP 0, 0, implicit-def dead $sp, implicit $sp
    %15:gpr32all = COPY $wzr
    %10:gpr32all = COPY %15
    %19:gpr32 = MOVi32imm 640
    %20:gpr64common = LOADgot target-flags(aarch64-got) @a
    %22:gpr64common = LOADgot target-flags(aarch64-got) @b
    %28:zpr = IMPLICIT_DEF
    %33:ppr_3b = PTRUE_S 4
    %41:zpr = IMPLICIT_DEF
    %53:zpr = IMPLICIT_DEF
    %65:zpr = IMPLICIT_DEF
    %77:zpr = IMPLICIT_DEF
    %89:zpr = IMPLICIT_DEF
    %101:zpr = IMPLICIT_DEF
    %113:zpr = IMPLICIT_DEF
    %125:zpr = IMPLICIT_DEF
    %137:zpr = IMPLICIT_DEF
    %148:gpr64common = LOADgot target-flags(aarch64-got) @c
    %149:gpr64common = LOADgot target-flags(aarch64-got) @d
    %150:gpr64common = LOADgot target-flags(aarch64-got) @e
    %151:gpr64common = LOADgot target-flags(aarch64-got) @aa
    %152:gpr64common = LOADgot target-flags(aarch64-got) @bb
    %153:gpr64common = LOADgot target-flags(aarch64-got) @cc
    %156:gpr32 = MOVi32imm 800000
  
  bb.1.for.cond2.preheader:
    successors: %bb.4(0x80000000)
  
    %0:gpr32sp = PHI %10, %bb.0, %2, %bb.3
    %17:fpr32 = FMOVS0
    %18:gpr64all = COPY $xzr
    %16:gpr64all = COPY %18
    %1:gpr64all = SUBREG_TO_REG 0, %19, %subreg.sub_32
    B %bb.4
  
  bb.2.for.cond.cleanup:
    %158:gpr64sp = nuw ADDXri %9, 16, 0
    ADJCALLSTACKDOWN 0, 0, implicit-def dead $sp, implicit $sp
    %159:gpr64all = COPY $xzr
    $x0 = COPY %158
    $x1 = COPY %159
    BL @gettimeofday, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit $x1, implicit-def $sp, implicit-def $w0
    ADJCALLSTACKUP 0, 0, implicit-def dead $sp, implicit $sp
    $s0 = COPY %6
    RET_ReallyLR implicit $s0
  
  bb.3.for.cond.cleanup4:
    successors: %bb.2(0x04000000), %bb.1(0x7c000000)
  
    ADJCALLSTACKDOWN 0, 0, implicit-def dead $sp, implicit $sp
    $x0 = COPY %20
    $x1 = COPY %22
    $x2 = COPY %148
    $x3 = COPY %149
    $x4 = COPY %150
    $x5 = COPY %151
    $x6 = COPY %152
    $x7 = COPY %153
    $s0 = COPY %6
    BL @dummy, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit $x1, implicit $x2, implicit $x3, implicit $x4, implicit $x5, implicit $x6, implicit $x7, implicit $s0, implicit-def $sp, implicit-def $w0
    ADJCALLSTACKUP 0, 0, implicit-def dead $sp, implicit $sp
    %155:gpr32common = nuw nsw ADDWri %0, 1, 0
    %2:gpr32all = COPY %155
    dead $wzr = SUBSWrr %155, %156, implicit-def $nzcv
    Bcc 0, %bb.2, implicit $nzcv
    B %bb.1
  
  bb.4.for.body5:
    successors: %bb.4(0x7c000000), %bb.3(0x04000000)
  
    %3:gpr64common = PHI %16, %bb.1, %7, %bb.4
    %4:fpr32 = PHI %17, %bb.1, %6, %bb.4
    %5:gpr64sp = PHI %1, %bb.1, %8, %bb.4
    %21:gpr64common = ADDXrr %20, %3
    %23:gpr64common = ADDXrr %22, %3
    %24:fpr128 = LDRQui %21, 0 :: (load (s128) from %ir.uglygep186, align 8, !tbaa !8)
    %25:fpr128 = LDRQui %23, 0 :: (load (s128) from %ir.uglygep225, align 8, !tbaa !8)
    %26:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMULv4f32 killed %25, killed %24
    %27:zpr = INSERT_SUBREG %28, killed %26, %subreg.zsub
    %29:fpr32 = LDRSui %21, 4 :: (load (s32) from %ir.uglygep206, align 8, !tbaa !8)
    %30:fpr32 = LDRSui %23, 4 :: (load (s32) from %ir.uglygep171, align 8, !tbaa !8)
    %32:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMADDSrrr killed %30, killed %29, %4
    %34:zpr = FADDV_VPZ_S %33, killed %27
    %35:fpr32 = COPY %34.ssub
    %36:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FADDSrr killed %32, killed %35
    %37:fpr128 = LDURQi %21, 20 :: (load (s128) from %ir.uglygep188, align 4, !tbaa !8)
    %38:fpr128 = LDURQi %23, 20 :: (load (s128) from %ir.uglygep233, align 4, !tbaa !8)
    %39:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMULv4f32 killed %38, killed %37
    %40:zpr = INSERT_SUBREG %41, killed %39, %subreg.zsub
    %42:fpr32 = LDRSui %21, 9 :: (load (s32) from %ir.uglygep208, !tbaa !8)
    %43:fpr32 = LDRSui %23, 9 :: (load (s32) from %ir.uglygep173, !tbaa !8)
    %45:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMADDSrrr killed %43, killed %42, killed %36
    %46:zpr = FADDV_VPZ_S %33, killed %40
    %47:fpr32 = COPY %46.ssub
    %48:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FADDSrr killed %45, killed %47
    %49:fpr128 = LDURQi %21, 40 :: (load (s128) from %ir.uglygep190, align 8, !tbaa !8)
    %50:fpr128 = LDURQi %23, 40 :: (load (s128) from %ir.uglygep235, align 8, !tbaa !8)
    %51:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMULv4f32 killed %50, killed %49
    %52:zpr = INSERT_SUBREG %53, killed %51, %subreg.zsub
    %54:fpr32 = LDRSui %21, 14 :: (load (s32) from %ir.uglygep210, align 8, !tbaa !8)
    %55:fpr32 = LDRSui %23, 14 :: (load (s32) from %ir.uglygep175, align 8, !tbaa !8)
    %57:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMADDSrrr killed %55, killed %54, killed %48
    %58:zpr = FADDV_VPZ_S %33, killed %52
    %59:fpr32 = COPY %58.ssub
    %60:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FADDSrr killed %57, killed %59
    %61:fpr128 = LDURQi %21, 60 :: (load (s128) from %ir.uglygep192, align 4, !tbaa !8)
    %62:fpr128 = LDURQi %23, 60 :: (load (s128) from %ir.uglygep237, align 4, !tbaa !8)
    %63:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMULv4f32 killed %62, killed %61
    %64:zpr = INSERT_SUBREG %65, killed %63, %subreg.zsub
    %66:fpr32 = LDRSui %21, 19 :: (load (s32) from %ir.uglygep212, !tbaa !8)
    %67:fpr32 = LDRSui %23, 19 :: (load (s32) from %ir.uglygep177, !tbaa !8)
    %69:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMADDSrrr killed %67, killed %66, killed %60
    %70:zpr = FADDV_VPZ_S %33, killed %64
    %71:fpr32 = COPY %70.ssub
    %72:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FADDSrr killed %69, killed %71
    %73:fpr128 = LDRQui %21, 5 :: (load (s128) from %ir.uglygep194, align 8, !tbaa !8)
    %74:fpr128 = LDRQui %23, 5 :: (load (s128) from %ir.uglygep239, align 8, !tbaa !8)
    %75:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMULv4f32 killed %74, killed %73
    %76:zpr = INSERT_SUBREG %77, killed %75, %subreg.zsub
    %78:fpr32 = LDRSui %21, 24 :: (load (s32) from %ir.uglygep214, align 8, !tbaa !8)
    %79:fpr32 = LDRSui %23, 24 :: (load (s32) from %ir.uglygep179, align 8, !tbaa !8)
    %81:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMADDSrrr killed %79, killed %78, killed %72
    %82:zpr = FADDV_VPZ_S %33, killed %76
    %83:fpr32 = COPY %82.ssub
    %84:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FADDSrr killed %81, killed %83
    %85:fpr128 = LDURQi %21, 100 :: (load (s128) from %ir.uglygep185, align 4, !tbaa !8)
    %86:fpr128 = LDURQi %23, 100 :: (load (s128) from %ir.uglygep224, align 4, !tbaa !8)
    %87:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMULv4f32 killed %86, killed %85
    %88:zpr = INSERT_SUBREG %89, killed %87, %subreg.zsub
    %90:fpr32 = LDRSui %21, 29 :: (load (s32) from %ir.uglygep216, !tbaa !8)
    %91:fpr32 = LDRSui %23, 29 :: (load (s32) from %ir.uglygep181, !tbaa !8)
    %93:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMADDSrrr killed %91, killed %90, killed %84
    %94:zpr = FADDV_VPZ_S %33, killed %88
    %95:fpr32 = COPY %94.ssub
    %96:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FADDSrr killed %93, killed %95
    %97:fpr128 = LDURQi %21, 120 :: (load (s128) from %ir.uglygep196, align 8, !tbaa !8)
    %98:fpr128 = LDURQi %23, 120 :: (load (s128) from %ir.uglygep241, align 8, !tbaa !8)
    %99:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMULv4f32 killed %98, killed %97
    %100:zpr = INSERT_SUBREG %101, killed %99, %subreg.zsub
    %102:fpr32 = LDRSui %21, 34 :: (load (s32) from %ir.uglygep218, align 8, !tbaa !8)
    %103:fpr32 = LDRSui %23, 34 :: (load (s32) from %ir.uglygep169, align 8, !tbaa !8)
    %105:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMADDSrrr killed %103, killed %102, killed %96
    %106:zpr = FADDV_VPZ_S %33, killed %100
    %107:fpr32 = COPY %106.ssub
    %108:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FADDSrr killed %105, killed %107
    %109:fpr128 = LDURQi %21, 140 :: (load (s128) from %ir.uglygep198, align 4, !tbaa !8)
    %110:fpr128 = LDURQi %23, 140 :: (load (s128) from %ir.uglygep231, align 4, !tbaa !8)
    %111:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMULv4f32 killed %110, killed %109
    %112:zpr = INSERT_SUBREG %113, killed %111, %subreg.zsub
    %114:fpr32 = LDRSui %21, 39 :: (load (s32) from %ir.uglygep220, !tbaa !8)
    %115:fpr32 = LDRSui %23, 39 :: (load (s32) from %ir.uglygep167, !tbaa !8)
    %117:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMADDSrrr killed %115, killed %114, killed %108
    %118:zpr = FADDV_VPZ_S %33, killed %112
    %119:fpr32 = COPY %118.ssub
    %120:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FADDSrr killed %117, killed %119
    %121:fpr128 = LDRQui %21, 10 :: (load (s128) from %ir.uglygep200, align 8, !tbaa !8)
    %122:fpr128 = LDRQui %23, 10 :: (load (s128) from %ir.uglygep229, align 8, !tbaa !8)
    %123:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMULv4f32 killed %122, killed %121
    %124:zpr = INSERT_SUBREG %125, killed %123, %subreg.zsub
    %126:fpr32 = LDRSui %21, 44 :: (load (s32) from %ir.uglygep222, align 8, !tbaa !8)
    %127:fpr32 = LDRSui %23, 44 :: (load (s32) from %ir.uglygep165, align 8, !tbaa !8)
    %129:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMADDSrrr killed %127, killed %126, killed %120
    %130:zpr = FADDV_VPZ_S %33, killed %124
    %131:fpr32 = COPY %130.ssub
    %132:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FADDSrr killed %129, killed %131
    %133:fpr128 = LDURQi %21, 180 :: (load (s128) from %ir.uglygep202, align 4, !tbaa !8)
    %134:fpr128 = LDURQi %23, 180 :: (load (s128) from %ir.uglygep227, align 4, !tbaa !8)
    %135:fpr128 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMULv4f32 killed %134, killed %133
    %136:zpr = INSERT_SUBREG %137, killed %135, %subreg.zsub
    %138:fpr32 = LDRSui %21, 49 :: (load (s32) from %ir.uglygep204, !tbaa !8)
    %139:fpr32 = LDRSui %23, 49 :: (load (s32) from %ir.uglygep163, !tbaa !8)
    %141:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FMADDSrrr killed %139, killed %138, killed %132
    %142:zpr = FADDV_VPZ_S %33, killed %136
    %143:fpr32 = COPY %142.ssub
    %6:fpr32 = nnan ninf nsz arcp contract afn reassoc nofpexcept FADDSrr killed %141, killed %143
    %144:gpr64sp = nuw nsw ADDXri %3, 200, 0
    %7:gpr64all = COPY %144
    %145:gpr64 = SUBSXri %5, 1, 0, implicit-def $nzcv
    %8:gpr64all = COPY %145
    Bcc 1, %bb.4, implicit $nzcv
    B %bb.3

...
